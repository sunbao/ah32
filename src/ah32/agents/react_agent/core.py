"""ReAct Agent æ ¸å¿ƒæ¨¡å—"""



from __future__ import annotations



import asyncio

import json

import os

import logging

import re

import time

from typing import Dict, List, Any, Optional

from datetime import datetime



from ah32.memory.manager import Ah32MemorySystem

from ah32.core.prompts import get_react_system_prompt

from ah32.services.docx_extract import maybe_extract_active_docx, maybe_extract_active_doc_text_full

from ah32.plan.schema import PLAN_SCHEMA_ID


# é¢„ç¼–è¯‘å¸¸ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»¥æé«˜æ€§èƒ½

STRICT_PLAN_CODE_BLOCK_RE = re.compile(r"```(?:json)?\s*\n([\s\S]*?)```", re.IGNORECASE)


logger = logging.getLogger(__name__)



from ah32.knowledge.project_rag_index import derive_project_context, get_project_rag_index

from ah32.telemetry import RunContext, get_telemetry





class ToolCall:

    """å·¥å…·è°ƒç”¨è®°å½•"""



    def __init__(self, tool_name: str, arguments: Dict[str, Any], result: str = None):

        self.tool_name = tool_name

        self.arguments = arguments

        self.result = result

        self.timestamp = datetime.now().isoformat()



    def to_dict(self) -> Dict[str, Any]:

        return {

            "tool": self.tool_name,

            "arguments": self.arguments,

            "result": self.result,

            "timestamp": self.timestamp,

        }





def _should_stop_iteration(result: str, iteration: int, max_iterations: int) -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥åœæ­¢ReActè¿­ä»£"""
    # å¦‚æœå·¥å…·è°ƒç”¨è¿”å›æ˜ç¡®çš„ç»“æŸä¿¡å·
    if "åˆ†æå®Œæˆ" in result or "å¤„ç†å®Œæ¯•" in result:
        return True



    # å¦‚æœè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°

    if iteration >= max_iterations:

        return True



    # å…¶ä»–æƒ…å†µç»§ç»­è¿­ä»£
    return False


def _extract_json_payload(text: str) -> str:
    """Extract a JSON object string from a model response (fenced or raw)."""
    if not text:
        return ""
    m = STRICT_PLAN_CODE_BLOCK_RE.search(text)
    if m and m.group(1):
        return m.group(1).strip()
    start = text.find("{")
    end = text.rfind("}")
    if start >= 0 and end >= 0 and end > start:
        return text[start : end + 1].strip()
    return text.strip()




def _extract_run_context(frontend_context: Optional[dict], session_id: str, *, mode: str) -> RunContext:

    """Best-effort extract RunContext from frontend_context (never throws)."""

    base: Dict[str, Any] = {}

    host: str = ""

    try:

        if isinstance(frontend_context, dict):

            rc = frontend_context.get("run_context") or frontend_context.get("runContext")

            if isinstance(rc, dict):

                base.update(rc)

            host = str(frontend_context.get("host_app") or frontend_context.get("hostApp") or "").strip()

            active = frontend_context.get("activeDocument") or frontend_context.get("document") or {}

            if isinstance(active, dict):

                if not host:

                    host = str(active.get("hostApp") or active.get("host_app") or "").strip()

                base.setdefault("doc_id", str(active.get("id") or active.get("docId") or active.get("doc_id") or "").strip())

                base.setdefault("doc_key", str(active.get("docKey") or active.get("doc_key") or "").strip())

    except Exception:

        base = {}



    base.setdefault("session_id", str(session_id or "").strip())

    base.setdefault("mode", str(mode or "").strip())

    if host:

        base.setdefault("host_app", host)

    return RunContext.from_any(base)





def _is_user_info_query(message: str) -> bool:

    """æ£€æŸ¥æ˜¯å¦ä¸ºç”¨æˆ·ä¿¡æ¯æŸ¥è¯¢"""

    user_info_keywords = ["æˆ‘", "æˆ‘çš„", "ä¸ªäººä¿¡æ¯", "ç”¨æˆ·", "åå¥½", "è®¾ç½®", "é…ç½®"]

    return any(keyword in message for keyword in user_info_keywords)





def _format_result_for_user(tool_name: str, result: Any) -> str:

    """å°†å·¥å…·ç»“æœè½¬æ¢ä¸ºç”¨æˆ·å‹å¥½çš„è‡ªç„¶è¯­è¨€æè¿°



    Args:

        tool_name: å·¥å…·åç§°ï¼ˆä¸å‘ç”¨æˆ·æ˜¾ç¤ºï¼‰

        result: å·¥å…·æ‰§è¡Œç»“æœ



    Returns:

        str: ç”¨æˆ·å‹å¥½çš„è‡ªç„¶è¯­è¨€æè¿°

    """

    try:

        # å·¥å…·ç‰¹å®šçš„è½¬æ¢é€»è¾‘

        if tool_name == "list_open_documents":

            # æ–‡æ¡£åˆ—è¡¨ç»“æœè½¬æ¢

            if isinstance(result, list):

                doc_list = "\n".join([f"- {doc}" for doc in result])

                return f"æˆ‘æ³¨æ„åˆ°æ‚¨æ‰“å¼€äº†ä»¥ä¸‹æ–‡æ¡£ï¼š\n{doc_list}"

            return f"æˆ‘æ£€æŸ¥äº†æ‚¨æ‰“å¼€çš„æ–‡æ¡£ã€‚"



        elif tool_name in ["read_document"]:

            # æ–‡æ¡£è¯»å–ç»“æœè½¬æ¢

            if isinstance(result, dict):

                if "content" in result:

                    # æå–æ–‡æ¡£å…³é”®ä¿¡æ¯

                    content = result["content"]

                    if len(content) > 200:

                        # å¦‚æœå†…å®¹å¤ªé•¿ï¼Œæå–å…³é”®ä¿¡æ¯

                        return f"æˆ‘å·²åˆ†æè¯¥æ–‡æ¡£ï¼Œå†…å®¹åŒ…æ‹¬ï¼š\n{content[:200]}..."

                    else:

                        return f"æˆ‘å·²åˆ†æè¯¥æ–‡æ¡£ï¼Œå†…å®¹ä¸ºï¼š\n{content}"

            return "æˆ‘å·²åˆ†æè¯¥æ–‡æ¡£ã€‚"



        elif tool_name == "analyze_chapter":

            # ç« èŠ‚åˆ†æç»“æœè½¬æ¢

            if isinstance(result, dict):

                if "analysis" in result:

                    return f"è¯¥ç« èŠ‚çš„å…³é”®ä¿¡æ¯åŒ…æ‹¬ï¼š\n{result['analysis']}"

                elif "content" in result:

                    return f"æˆ‘å·²åˆ†æè¯¥ç« èŠ‚ï¼Œå†…å®¹ä¸ºï¼š\n{result['content'][:200]}..."

            return "æˆ‘å·²åˆ†æè¯¥ç« èŠ‚ã€‚"



        elif tool_name in ["extract_requirements", "extract_responses"]:

            # éœ€æ±‚/å“åº”æå–ç»“æœè½¬æ¢

            if isinstance(result, list):

                items = "\n".join([f"- {item}" for item in result[:5]])  # åªæ˜¾ç¤ºå‰5é¡¹

                return f"æˆ‘æå–åˆ°ä»¥ä¸‹è¦ç‚¹ï¼š\n{items}\n..."

            return "æˆ‘å·²æå–ç›¸å…³å†…å®¹ã€‚"



        elif tool_name in ["match_requirements", "map_chapters"]:

            # åŒ¹é…/æ˜ å°„ç»“æœè½¬æ¢

            if isinstance(result, dict):

                if "matches" in result:

                    return f"æˆ‘å‘ç°äº†ä»¥ä¸‹åŒ¹é…é¡¹ï¼š\n{result['matches']}"

                elif "mapping" in result:

                    return f"æˆ‘å·²å®Œæˆç« èŠ‚æ˜ å°„ã€‚"

            return "æˆ‘å·²å®Œæˆåˆ†æã€‚"



        elif tool_name in ["assess_quality", "assess_risks"]:

            # è´¨é‡/é£é™©è¯„ä¼°ç»“æœè½¬æ¢

            if isinstance(result, dict):

                if "assessment" in result:

                    return f"è¯„ä¼°ç»“æœï¼š\n{result['assessment']}"

                elif "risks" in result:

                    return f"é£é™©è¯„ä¼°ï¼š\n{result['risks']}"

            return "æˆ‘å·²å®Œæˆè¯„ä¼°ã€‚"



        elif tool_name == "answer_question":

            # é—®ç­”ç»“æœè½¬æ¢

            if isinstance(result, dict):

                if "answer" in result:

                    return result["answer"]

            return "æˆ‘å·²ç»æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"



        # é»˜è®¤å¤„ç†ï¼šç›´æ¥è¿”å›ç»“æœå­—ç¬¦ä¸²çš„ä¸€éƒ¨åˆ†

        result_str = str(result)

        if len(result_str) > 200:

            return f"æˆ‘å·²å¤„ç†è¯¥ä¿¡æ¯ï¼Œå†…å®¹ä¸ºï¼š\n{result_str[:200]}..."

        else:

            return f"æˆ‘å·²å¤„ç†è¯¥ä¿¡æ¯ï¼Œå†…å®¹ä¸ºï¼š\n{result_str}"



    except Exception as e:

        logger.error(f"æ ¼å¼åŒ–å·¥å…·ç»“æœå¤±è´¥: {e}")

        return "æˆ‘å·²å¤„ç†è¯¥ä¿¡æ¯ï¼Œä½†æ— æ³•æ˜¾ç¤ºè¯¦ç»†å†…å®¹ã€‚"





class ReActAgent:

    """ReAct Agent - æ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿå¾ªç¯"""



    def __init__(

        self,

        llm,

        tools: List[Any],

        memory_system: Ah32MemorySystem,

        vector_store: Any = None,

        skills_registry: Any = None,

    ):

        self.llm = llm

        self.tools = {tool.name: tool for tool in tools}

        self.memory_system = memory_system

        self.tool_calls: List[ToolCall] = []

        # Optional RAG knowledge base (e.g. LocalVectorStore). If provided, we retrieve

        # relevant snippets per user turn and inject them into the LLM context.

        self.vector_store = vector_store

        # Optional hot-loaded skills registry (prompt-only for now).

        self.skills_registry = skills_registry

        # Skill tools executor (for skill.json tools -> scripts/*.py)
        self._skill_tool_executor = None
        try:
            from ah32.skills.tool_executor import ToolExecutor as SkillToolExecutor

            self._skill_tool_executor = SkillToolExecutor(max_tool_calls_per_turn=3)
        except Exception as e:
            logger.warning(f"[skills] init skill tool executor failed: {e}", exc_info=True)

        # Skills selected for current turn (used to resolve skill tools)
        self._selected_skills: List[Any] = []



        # cache for tool results within a single agent session to avoid duplicate external calls

        # key: (tool_name, normalized_input) -> result string

        self._tool_cache: dict = {}

        # simple counts to detect repeated requests (for observability)

        self._tool_call_counts: dict = {}



        # Track reasoning content for incremental streaming

        self._last_reasoning_content: Optional[str] = None

        # Debug: last RAG retrieval summary for this turn.

        self._last_rag_debug: Optional[dict] = None



        # LLMåˆ†ç±»ç»“æœç¼“å­˜ - é¿å…é‡å¤è°ƒç”¨LLMè¿›è¡Œç›¸åŒæ¶ˆæ¯çš„åˆ†ç±»

        # key: message_hash -> classification_result

        self._classification_cache: Dict[str, Any] = {}



        # ç¼“å­˜ç»Ÿè®¡

        self._classification_cache_hits = 0

        self._classification_cache_misses = 0



        # ReActç³»ç»Ÿæç¤º

        self.system_prompt = self._create_react_prompt()



        # åˆå§‹åŒ–è®°å¿†ç®¡ç†

        self.global_memory = memory_system



        # åˆå§‹åŒ–è¯­ä¹‰åŒ¹é…ï¼ˆAh32MemorySystem æ”¯æŒè¯­ä¹‰åŒ¹é…ï¼‰

        self.semantic_matcher = memory_system



    def _accumulate_token_usage(self, obj: object) -> None:

        """Best-effort token usage accounting across providers.



        Used by MacroBench budgeting/observability. Missing usage is acceptable.

        """

        try:

            acc = getattr(self, "_token_usage_acc", None)

            if not isinstance(acc, dict):

                acc = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}

                self._token_usage_acc = acc



            md = getattr(obj, "response_metadata", None) or {}

            usage = None

            if isinstance(md, dict):

                usage = md.get("token_usage") or md.get("usage") or md.get("usage_metadata")

            if usage is None:

                usage = getattr(obj, "usage_metadata", None)

            if not usage or not isinstance(usage, dict):

                return



            def _pick(*keys: str) -> int:

                for k in keys:

                    try:

                        v = usage.get(k)

                        if v is not None:

                            return int(v or 0)

                    except Exception:

                        continue

                return 0



            pt = _pick("prompt_tokens", "input_tokens")

            ct = _pick("completion_tokens", "output_tokens")

            tt = _pick("total_tokens") or (pt + ct if (pt or ct) else 0)



            acc["prompt_tokens"] = int(acc.get("prompt_tokens") or 0) + max(0, pt)

            acc["completion_tokens"] = int(acc.get("completion_tokens") or 0) + max(0, ct)

            acc["total_tokens"] = int(acc.get("total_tokens") or 0) + max(0, tt)

        except Exception:

            return



    def _build_rag_context(self, user_message: str, at_reference_paths: Optional[List[str]] = None) -> str:

        """Best-effort RAG retrieval against the knowledge base (if configured)."""

        if not self.vector_store:

            return ""



        def _iter_docs_and_scores(raw):

            if not raw:

                return []

            out = []

            for item in raw:

                if isinstance(item, tuple) and len(item) == 2:

                    out.append(item)

                else:

                    out.append((item, None))

            return out



        # Chroma score is typically a distance (lower is more similar). In practice the

        # absolute scale depends on the embedding model; be permissive and keep a minimum

        # number of top results so RAG is actually usable.

        max_distance = 2.0

        min_keep = 3

        per_source_k = 2

        general_k = 5



        # Query rewrite/decomposition (lightweight, heuristic):

        # Long user messages often mix multiple sub-questions; splitting improves recall without changing business logic.

        queries: List[str] = []

        query_rewrites: List[str] = []

        try:

            t = (user_message or "").strip()

            queries = [t] if t else []

            if t and len(t) > 120:

                parts = re.split(r"[ã€‚ï¼ï¼Ÿ!?ï¼›;\n]+", t)

                parts = [p.strip() for p in parts if len(p.strip()) >= 6]

                picked: List[str] = []

                for p in parts:

                    if p not in picked:

                        picked.append(p)

                    if len(picked) >= 3:

                        break

                if picked:

                    query_rewrites = picked[:3]

                    # Keep the original query first, then a couple of shorter sub-queries.

                    queries = [t] + picked[:2]

        except Exception:

            queries = [str(user_message or "").strip()] if str(user_message or "").strip() else []

            query_rewrites = []



        raw_results = []

        t_retrieve0 = time.perf_counter()

        try:

            if at_reference_paths:

                for path in at_reference_paths[:5]:

                    raw_results.extend(

                        _iter_docs_and_scores(

                            self.vector_store.similarity_search(

                                user_message, k=per_source_k, filter={"source": path}

                            )

                        )

                    )

                # Fallback: if per-source retrieval yields nothing (path mismatch), do a general search.

                if not raw_results:

                    for q in (queries or [user_message])[:3]:

                        raw_results.extend(

                            _iter_docs_and_scores(

                                self.vector_store.similarity_search(

                                    q, k=general_k, filter=getattr(self, "_rag_source_filter", None)

                                )

                            )

                        )

            else:

                for q in (queries or [user_message])[:3]:

                    raw_results.extend(

                        _iter_docs_and_scores(

                            self.vector_store.similarity_search(

                                q, k=general_k, filter=getattr(self, "_rag_source_filter", None)

                            )

                        )

                    )

        except Exception as e:

            retrieve_ms = int((time.perf_counter() - t_retrieve0) * 1000)

            try:

                self._last_rag_debug = {

                    "triggered": True,

                    "raw": 0,

                    "kept": 0,

                    "queries": (queries or [])[:3],

                    "query_rewrites": (query_rewrites or [])[:3],

                    "retrieve_ms": retrieve_ms,

                    "error": str(e)[:200],

                }

            except Exception:

                self._last_rag_debug = None

            logger.debug(f"[RAG] retrieval failed: {e}", exc_info=True)

            return ""

        retrieve_ms = int((time.perf_counter() - t_retrieve0) * 1000)



        # Filter + de-dup by (source, chunk_index, content hash)

        seen = set()

        kept = []

        for doc, score in raw_results:

            try:

                meta = getattr(doc, "metadata", {}) or {}

                key = (

                    meta.get("source", "unknown"),

                    meta.get("chunk_index"),

                    hash(getattr(doc, "page_content", "")),

                )

            except Exception:

                key = (id(doc),)

            if key in seen:

                continue

            seen.add(key)



            # Keep top results even if distance is large; we also keep all results under threshold.

            if score is None or score <= max_distance or len(kept) < min_keep:

                kept.append((doc, score))



        if not kept:

            # Keep a lightweight debug summary even when RAG retrieval returns 0 hits.
            try:
                self._last_rag_debug = {
                    "triggered": True,
                    "raw": len(raw_results),
                    "kept": 0,
                    "queries": (queries or [])[:3],
                    "query_rewrites": (query_rewrites or [])[:3],
                    "retrieve_ms": retrieve_ms,
                    "sources": [],
                }
            except Exception:
                self._last_rag_debug = None

            # Skill-scoped hint: only emit explicit "RAG 0-hit" signal when a selected
            # skill declares it needs the hint (avoid polluting generic chats).
            selected = getattr(self, "_selected_skills", None) or []
            wants_hint = False
            try:
                for s in selected:
                    if bool(getattr(s, "rag_missing_hint", False)):
                        wants_hint = True
                        break
            except Exception:
                wants_hint = False

            if wants_hint:
                return (
                    "RAGæ£€ç´¢ç»“æœï¼šæœ¬è½®æœªå‘½ä¸­ä»»ä½•ç‰‡æ®µï¼ˆ0æ¡ï¼‰ã€‚\n"
                    "æç¤ºï¼šå¦‚æœç”¨æˆ·éœ€è¦â€œåˆåŒ/æ¡ˆä¾‹/æ¨¡æ¿/æ”¿ç­–/æ³•è§„â€ç­‰èµ„æ–™æ”¯æ’‘ç»“è®ºï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥å½“å‰çŸ¥è¯†åº“æœªæ‰¾åˆ°ï¼Œ"
                    "å¹¶å¼•å¯¼ç”¨æˆ·ä¸Šä¼ /å¯¼å…¥èµ„æ–™å…¥åº“ï¼Œæˆ–æä¾›ç½‘é¡µURLä»¥ä¾¿åç»­æŠ“å–å…¥åº“åå†æ£€ç´¢ï¼›"
                    "è‹¥URLæŠ“å–å¤±è´¥/ä¸å¯è®¿é—®ï¼Œè¯·ç”¨æˆ·ç›´æ¥å¤åˆ¶ç²˜è´´åŸæ–‡ï¼ˆçº¯æ–‡å­—ï¼‰å†ç»§ç»­ã€‚"
                )

            return ""



        # Prefer lower distance (more similar), and avoid stuffing too many chunks from the same source.

        try:

            kept.sort(key=lambda x: (x[1] if isinstance(x[1], (int, float)) else 999999))

            per_source_limit = 2

            max_total = 8

            by_source = {}

            filtered = []

            for doc, score in kept:

                meta = getattr(doc, "metadata", {}) or {}

                source = meta.get("source", "unknown")

                by_source[source] = by_source.get(source, 0) + 1

                if by_source[source] > per_source_limit:

                    continue

                filtered.append((doc, score))

                if len(filtered) >= max_total:

                    break

            kept = filtered or kept

        except Exception as e:

            logger.debug(f"[RAG] sort/filter kept docs failed: {e}", exc_info=True)



        try:

            preview_sources = []

            for doc, score in kept[:5]:

                meta = getattr(doc, "metadata", {}) or {}

                preview_sources.append((meta.get("source", "unknown"), score))

            logger.debug(f"[RAG] kept={len(kept)} sources={preview_sources}")

        except Exception as e:

            logger.debug(f"[RAG] build preview sources failed: {e}", exc_info=True)



        # Keep a lightweight debug summary for optional UI/logging/telemetry.

        try:

            scores = [float(score) for _doc, score in kept if isinstance(score, (int, float))]

            scores_sorted = sorted(scores)

            def _pct(p: float) -> Optional[float]:

                if not scores_sorted:

                    return None

                idx = int(round((len(scores_sorted) - 1) * p))

                idx = max(0, min(len(scores_sorted) - 1, idx))

                return float(scores_sorted[idx])



            self._last_rag_debug = {

                "triggered": True,

                "raw": len(raw_results),

                "kept": len(kept),

                "queries": (queries or [])[:3],

                "query_rewrites": (query_rewrites or [])[:3],

                "retrieve_ms": retrieve_ms,

                "scores": {

                    "min": min(scores) if scores else None,

                    "p50": _pct(0.50),

                    "p90": _pct(0.90),

                    "max": max(scores) if scores else None,

                },

                "sources": [

                    {

                        "source": (getattr(doc, "metadata", {}) or {}).get("source", "unknown"),

                        "score": score,

                    }

                    for doc, score in kept[:5]

                ],

            }

        except Exception:

            self._last_rag_debug = None



        max_total_chars = 2400

        max_snippet_chars = 260

        lines = [

            "RAGå‘½ä¸­ç‰‡æ®µï¼ˆæ¥è‡ªçŸ¥è¯†åº“æ£€ç´¢ç»“æœï¼‰ï¼šå¦‚æœä¸‹é¢æœ‰ç‰‡æ®µï¼Œä½ å¿…é¡»ä¼˜å…ˆåŸºäºç‰‡æ®µå›ç­”ã€‚"
            "é»˜è®¤ä¸è¦åœ¨æ­£æ–‡/å†™å›å†…å®¹ä¸­å±•ç¤º source/URLï¼ˆé¿å…å½±å“æ’ç‰ˆï¼‰ï¼›ä»…å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚â€œå‡ºå¤„/æ¥æº/å¼•ç”¨/é“¾æ¥â€æ—¶ï¼Œ"
            "æ‰åœ¨å›ç­”æœ«å°¾ç”¨â€œæ¥æºï¼šâ€åˆ—å‡ºæœ€å°‘å¿…è¦çš„ source/URLã€‚"
            "ä¸è¦å£°ç§°â€œçŸ¥è¯†åº“/å·²æ‰“å¼€æ–‡æ¡£å‡æœªåŒ…å«ç›¸å…³ä¿¡æ¯â€ã€‚"

        ]

        used_chars = len(lines[0])

        truncated = False



        for idx, (doc, score) in enumerate(kept[:8], 1):

            meta = getattr(doc, "metadata", {}) or {}

            source = meta.get("source", "unknown")

            method = meta.get("import_method")

            snippet = (getattr(doc, "page_content", "") or "").strip().replace("\r", " ").replace("\n", " ")

            if len(snippet) > max_snippet_chars:

                snippet = snippet[:max_snippet_chars] + "..."

            score_text = f"{score:.4f}" if isinstance(score, (int, float)) else "n/a"



            header = f"[{idx}] source={source}"

            if method:

                header += f" method={method}"

            header += f" score={score_text}"

            entry = f"{header}\n{snippet}"



            if used_chars + len(entry) + 2 > max_total_chars:

                truncated = True

                break

            lines.append(entry)

            used_chars += len(entry) + 2



        out = "\n\n".join(lines)

        try:

            if isinstance(self._last_rag_debug, dict):

                self._last_rag_debug["injected_chars"] = len(out)

                self._last_rag_debug["truncated"] = bool(truncated)

        except Exception as e:
            # Best-effort: never crash the main flow on debug-metadata updates, but keep it observable.
            logger.debug("[rag] update debug stats failed (ignored): %s", e, exc_info=True)

        return out



    def _is_writeback_intent(self, user_message: str, *, context: str = "") -> bool:
        """Heuristic: user wants the result written into the host document (not just chatted)."""

        t = (user_message or "").strip()
        if not t:
            return False

        # NOTE: Keep this generic (no business/scenario keywords). We only detect *writeback intent*.
        # Use \u-escapes to avoid editor/console encoding issues on Windows.
        write_verbs_strong = (
            "\u5199\u5165",  # ??
            "\u5199\u5230",  # ??
            "\u5199\u8fdb",  # ??
            "\u5199\u56de",  # ??
            "\u63d2\u5165",  # ??
            "\u8ffd\u52a0",  # ??
            "\u66ff\u6362",  # ??
            "\u66f4\u65b0",  # ??
            "\u7c98\u8d34",  # ??
            "\u5e94\u7528\u5199\u56de",  # ????
            "\u5e94\u7528",  # ???????/????????
        )
        write_verbs_soft = (
            "\u751f\u6210",  # ??
            "\u521b\u5efa",  # ??
            "\u5236\u4f5c",  # ??
            "\u8f93\u51fa",  # è¾“å‡º
            "\u505a\u4e00\u4e2a",  # ???
            "\u505a\u4e2a",  # ??
            "\u753b",  # ?
            "\u7ed8\u5236",  # ??
            "\u6574\u7406",  # ??
            "\u6392\u7248",  # ??
            "\u6da6\u8272",  # ??
            "\u6539\u5199",  # ??
        )
        doc_targets = (
            "\u6587\u6863",  # ??
            "WPS",
            "\u6b63\u6587",  # ??
            "\u6807\u9898",  # ??
            "\u76ee\u5f55",  # ??
            "\u7ae0\u8282",  # ??
            "\u5149\u6807",  # ??
            "\u5f53\u524d\u4f4d\u7f6e",  # ????
            "\u672b\u5c3e",  # ??
            "\u6587\u672b",  # ??
            "\u8868\u683c",  # ??
            "\u8868",  # ?
            "Sheet",
            "Excel",
            "PPT",
            "Slide",
            "\u5b57\u4f53",  # ??
            "\u989c\u8272",  # ??
            "\u683c\u5f0f",  # ??
            "\u6837\u5f0f",  # ??
            "\u6bb5\u843d",  # ??
        )

        chart_targets = (
            "\u56fe\u8868",  # å›¾è¡¨
            "\u67f1\u5f62\u56fe",  # æŸ±å½¢å›¾
            "\u6761\u5f62\u56fe",  # æ¡å½¢å›¾
            "\u6298\u7ebf\u56fe",  # æŠ˜çº¿å›¾
            "\u997c\u56fe",  # é¥¼å›¾
            "\u6563\u70b9\u56fe",  # æ•£ç‚¹å›¾
            "\u9762\u79ef\u56fe",  # é¢ç§¯å›¾
            "chart",
        )

        # Explicit: strong write verbs + an explicit target/location => writeback.
        if any(v in t for v in write_verbs_strong) and any(x in t for x in doc_targets):
            return True

        # Chart intents in sheets are almost always writeback operations.
        # Accept both explicit write verbs and common soft verbs like "è¾“å‡º/ç”»/ç”Ÿæˆ".
        if any(k in t for k in chart_targets) and (
            any(v in t for v in write_verbs_strong) or any(v in t for v in write_verbs_soft)
        ):
            return True

        # Continuation/confirmation should inherit writeback mode when we were just doing writeback.
        try:
            c = str(context or "")
            has_prior_writeback = (
                (PLAN_SCHEMA_ID in c)
                or ("schema_version" in c and PLAN_SCHEMA_ID in c)
                or ("upsert_block" in c)
                or ("```javascript" in c or "```js" in c)
                or ("BID.upsertBlock" in c)
                or ("[[AH32:" in c)
            )

            cont_prefixes = (
                "\u7ee7\u7eed",  # ??
                "\u63a5\u7740",  # ??
                "\u518d\u6765",  # ??
                "\u518d\u505a",  # ??
                "\u4e0b\u4e00\u6b65",  # ???
                "\u7ee7\u7eed\u6267\u884c",  # ????
                "\u63a5\u7740\u6267\u884c",  # ????
                "\u6309\u4e0a\u6b21",  # ???
                "\u540c\u4e0a",  # ??
            )
            if has_prior_writeback and any(t.startswith(p) for p in cont_prefixes):
                return True

            confirm_signals = (
                "\u6211\u786e\u8ba4",  # æˆ‘ç¡®è®¤
                "\u5c31\u662f\u8fd9\u4e2a\u610f\u601d",  # å°±æ˜¯è¿™ä¸ªæ„æ€
                "\u5bf9",  # å¯¹
                "\u662f\u7684",  # æ˜¯çš„
                "\u6ca1\u9519",  # æ²¡é”™
                "\u5c31\u6309\u8fd9\u4e2a",  # å°±æŒ‰è¿™ä¸ª
                "\u6309\u8fd9\u4e2a\u6267\u884c",  # æŒ‰è¿™ä¸ªæ‰§è¡Œ
                "\u76f4\u63a5\u6267\u884c",  # ç›´æ¥æ‰§è¡Œ
            )
            is_short_confirmation = len(t) <= 24 and any(s in t for s in confirm_signals)
            if has_prior_writeback and is_short_confirmation:
                return True
        except Exception as e:
            logger.debug("[writeback] continuation heuristic failed (ignored): %s", e, exc_info=True)

        # Generic fallback: unambiguous write verbs imply writeback even if the user omitted explicit targets.
        # Keep it conservative: require a strong verb OR (soft verb + doc-ish noun).
        if any(v in t for v in write_verbs_strong):
            return True
        if any(v in t for v in write_verbs_soft) and any(x in t for x in doc_targets):
            return True

        # Host-aware fallback: in ET context, short chart requests should default to writeback.
        try:
            c = str(context or "")
            in_et_host = ('"host_app": "et"' in c) or ("'host_app': 'et'" in c)
            has_chart_word = any(k in t for k in chart_targets)
            if in_et_host and has_chart_word and len(t) <= 24:
                return True
        except Exception as e:
            logger.debug("[writeback] host-aware chart heuristic failed (ignored): %s", e, exc_info=True)

        return False

    def _is_chat_only_intent(self, user_message: str) -> bool:
        """Heuristic: this turn is pure chat/QA and should not auto-writeback.

        Strategy (default-writeback mode):
        - In Office hosts, most actionable turns should write back.
        - Only suppress writeback for clearly conversational/explanatory turns.
        """

        t = (user_message or "").strip()
        if not t:
            return True

        tl = t.lower()

        explicit_write_request = False
        try:
            verbs = (
                "å†™åˆ°",
                "å†™å›",
                "æ’å…¥",
                "è¿½åŠ ",
                "è¾“å‡ºåˆ°",
                "ç”Ÿæˆåˆ°",
                "å†™å…¥åˆ°",
                "å†™è¿›",
                "å¡«åˆ°",
            )
            targets = (
                "æ–‡æ¡£",
                "æ­£æ–‡",
                "å…‰æ ‡",
                "å½“å‰ä½ç½®",
                "æ­¤å¤„",
                "è¿™é‡Œ",
                "è¡¨æ ¼",
                "å•å…ƒæ ¼",
                "å·¥ä½œè¡¨",
                "å·¥ä½œç°¿",
                "å¹»ç¯ç‰‡",
                "ppt",
                "slide",
            )
            explicit_write_request = any(v in tl for v in verbs) and any(x in tl for x in targets)
        except Exception as e:
            logger.debug("[writeback] explicit write request detect failed (ignored): %s", e, exc_info=True)
            explicit_write_request = False

        # Explicitly non-writeback instructions.
        explicit_no_write = (
            "ä¸è¦å†™å›",
            "ä¸ç”¨å†™å›",
            "ä¸å†™å›",
            "ä»…å›ç­”",
            "åªå›ç­”",
            "å…ˆåˆ«æ”¹",
            "ä¸è¦æ”¹æ–‡æ¡£",
            "ä¸ç”¨æ”¹æ–‡æ¡£",
            "åªèŠå¤©",
            "å…ˆè§£é‡Š",
        )
        if any(k in t for k in explicit_no_write):
            return True

        # "Text-only / do not create" instructions (common in PPT outline requests).
        # Treat them as chat-only unless the user also explicitly asked to write into the document.
        if not explicit_write_request:
            try:
                no_create_ppt = re.search(
                    r"(ä¸éœ€è¦|ä¸å¿…|æ— éœ€|ä¸è¦|ä¸ç”¨).{0,10}(ç›´æ¥)?(åˆ›å»º|ç”Ÿæˆ|åˆ¶ä½œ|åšæˆ).{0,10}(ppt|å¹»ç¯ç‰‡|æ¼”ç¤º|slide)",
                    tl,
                    flags=re.IGNORECASE,
                )
                no_plan = re.search(r"(ä¸éœ€è¦|ä¸å¿…|æ— éœ€|ä¸è¦|ä¸ç”¨).{0,10}(å¯æ‰§è¡Œ)?\\s*(plan|json)", tl, flags=re.IGNORECASE)
                if no_create_ppt or no_plan:
                    return True
            except Exception as e:
                logger.debug("[writeback] chat-only no-create detect failed (ignored): %s", e, exc_info=True)

        # Tiny small-talk/ack turns should stay in chat mode.
        # Normalize by stripping spaces/punctuation/emojis so "å¾ˆå¥½ğŸ‘" also matches.
        compact = ""
        try:
            compact = re.sub(r"[^0-9a-zA-Z\u4e00-\u9fff]+", "", tl)
        except Exception as e:
            logger.debug("[writeback] normalize chat-only ack failed (ignored): %s", e, exc_info=True)
            compact = tl

        tiny_chat = {
            "ä½ å¥½",
            "æ‚¨å¥½",
            "åœ¨å—",
            "è°¢è°¢",
            "è¾›è‹¦äº†",
            "æ”¶åˆ°",
            "æ˜ç™½äº†",
            "æ‡‚äº†",
            "ok",
            "okay",
            "å¥½",
            "å¥½çš„",
            "å¯ä»¥",
            "è¡Œ",
            "å¾ˆå¥½",
            "ä¸é”™",
            "æ£’",
            "èµ",
            "å®Œç¾",
            "æ²¡é—®é¢˜",
            "æ²¡äº‹",
            "hi",
            "hello",
            "nice",
            "great",
            "thanks",
            "thankyou",
        }
        if len(compact) <= 12 and compact in tiny_chat:
            return True
        try:
            # Multi-ack combos like "å¥½çš„è°¢è°¢" / "okthanks".
            ack_tokens = sorted([re.escape(x) for x in tiny_chat], key=len, reverse=True)
            ack_re = r"^(?:%s)+$" % "|".join(ack_tokens)
            if len(compact) <= 12 and re.fullmatch(ack_re, compact):
                return True
        except Exception as e:
            logger.debug("[writeback] chat-only ack regex failed (ignored): %s", e, exc_info=True)

        # Explanatory/QA turns should stay in chat mode by default (even if "æ–‡æ¡£/è¡¨æ ¼" is mentioned),
        # unless the user explicitly asks to apply changes into the document.
        has_question = ("?" in t) or ("ï¼Ÿ" in t)

        explain_signals = (
            "æ˜¯ä»€ä¹ˆ",
            "ä»€ä¹ˆæ„æ€",
            "æ€ä¹ˆç†è§£",
            "ä¸ºä»€ä¹ˆ",
            "ä»‹ç»",
            "è§£é‡Š",
            "æ€»ç»“",
            "åŒºåˆ«",
            "ä¼˜ç¼ºç‚¹",
            "åŸç†",
            "æ€ä¹ˆå›äº‹",
            "æ˜¯å¦",
        )
        howto_signals = (
            "å¦‚ä½•",
            "æ€ä¹ˆ",
            "æ€æ ·",
            "æ€ä¹ˆåš",
        )

        if any(k in t for k in explain_signals):
            return True
        if has_question or any(k in t for k in howto_signals):
            return False if explicit_write_request else True

        # If user clearly asks for document actions, do not classify as chat-only.
        write_or_doc_signals = (
            "å†™å…¥",
            "å†™åˆ°",
            "å†™å›",
            "æ’å…¥",
            "è¿½åŠ ",
            "æ›¿æ¢",
            "æ›´æ–°",
            "åº”ç”¨",
            "ç”Ÿæˆ",
            "åˆ›å»º",
            "è¾“å‡º",
            "æ–‡æ¡£",
            "æ­£æ–‡",
            "æ ‡é¢˜",
            "ç›®å½•",
            "æ®µè½",
            "è¡¨æ ¼",
            "å›¾è¡¨",
            "å•å…ƒæ ¼",
            "å·¥ä½œè¡¨",
            "å·¥ä½œç°¿",
            "wps",
            "excel",
            "sheet",
            "ppt",
            "slide",
        )
        if any(k in tl for k in write_or_doc_signals) or any(k in t for k in write_or_doc_signals):
            return False

        return False

    def _force_chat_only_by_skill_tool_hints(self, message: str, selected_skills: list) -> bool:
        """Force chat-only mode for background tasks declared by skill tool hints.

        This keeps business "what/when to trigger" inside skill manifests, while the core
        only implements a generic orchestration rule: background jobs should not go through
        the writeback fast-path.
        """
        msg = (message or "").strip()
        if not msg:
            return False

        # If user explicitly wants to write into the office document, do not force chat-only.
        # Examples: "æŠŠç»“æœå†™åˆ°æ–‡æ¡£/å…‰æ ‡å¤„/æ­£æ–‡é‡Œ"ã€‚
        try:
            verbs = (
                "å†™åˆ°",
                "å†™å›",
                "æ’å…¥",
                "è¿½åŠ ",
                "è¾“å‡ºåˆ°",
                "ç”Ÿæˆåˆ°",
                "å†™å…¥åˆ°",
                "å†™è¿›",
                "å¡«åˆ°",
            )
            targets = (
                "æ–‡æ¡£",
                "æ­£æ–‡",
                "å…‰æ ‡",
                "å½“å‰ä½ç½®",
                "æ­¤å¤„",
                "è¿™é‡Œ",
                "è¡¨æ ¼",
                "æ ‡é¢˜",
                "ç›®å½•",
                "æ®µè½",
            )
            if any(v in msg for v in verbs) and any(t in msg for t in targets):
                return False
        except Exception:
            logger.debug("[writeback] explicit write intent detect failed (ignored)", exc_info=True)

        for s in (selected_skills or []):
            if not s:
                continue
            tools = ()
            try:
                tools = getattr(s, "tools", None) or ()
            except Exception:
                tools = ()
            for tool in tools or ():
                hints = {}
                try:
                    hints = getattr(tool, "hints", None) or {}
                except Exception:
                    hints = {}
                if not isinstance(hints, dict) or not hints:
                    continue
                mode = str(hints.get("turn_mode") or hints.get("mode") or "").strip().lower()
                if mode not in ("background", "no_writeback", "chat_only"):
                    continue
                triggers = hints.get("triggers") or hints.get("trigger") or []
                if isinstance(triggers, str):
                    triggers = [triggers]
                if not isinstance(triggers, list):
                    triggers = []
                for k in triggers:
                    try:
                        kk = str(k or "").strip()
                    except Exception:
                        kk = ""
                    if not kk:
                        continue
                    if kk in msg:
                        return True
        return False

    def _writeback_anchor(self, user_message: str) -> str:

        """Return 'end' when user explicitly requests writing at document end/summary; otherwise 'cursor'."""

        t = (user_message or "").strip()

        if re.search(r"(æ–‡æœ«|æœ«å°¾|æœ€å|æ±‡æ€»|é™„å½•|èµ„æ–™å¼•ç”¨|å‚è€ƒèµ„æ–™|æ¥æº|å‡ºå¤„)", t):

            return "end"

        return "cursor"



    def _is_compare_table_delivery_intent(self, user_message: str) -> bool:

        """Heuristic: user is asking for review/rewrite suggestions (default deliverable: compare table).



        Keep it conservative: do not force compare-table delivery for simple formatting/template tasks.

        """

        t = (user_message or "").strip()

        if not t:

            return False

        return bool(

            re.search(

                r"(å®¡é˜…|é£é™©|æ”¹å†™|æ›¿æ¢|ä¿®æ”¹å»ºè®®|å»ºè®®ä¿®æ”¹|å»ºè®®æ”¹å†™|å¯¹ç…§è¡¨|ä¿®è®¢|åˆè§„|æ¡æ¬¾|é£é™©ç‚¹)",

                t,

            )

        )



    def _build_writeback_directive(self, user_message: str) -> str:
        """Instruction appended to the LLM context when we want to write back into the office document."""
        anchor = self._writeback_anchor(user_message)
        anchor_hint = (
            "é»˜è®¤å†™å…¥å½“å‰å…‰æ ‡å¤„ï¼ˆupsert_block.anchor='cursor'ï¼‰ã€‚"
            if anchor != "end"
            else "ç”¨æˆ·è¦æ±‚å†™åˆ°æ–‡æœ«/æ±‡æ€»åŒºï¼šè¯·å°† upsert_block.anchor è®¾ç½®ä¸º \"end\"ï¼ˆæ‰§è¡Œå™¨ä¼šè‡ªåŠ¨é”šå®šåˆ°æ–‡æœ«ï¼Œæ— éœ€ EndKey/GoToï¼‰ã€‚"
        )
        return "\n".join(
            [
                "ã€å†™å›æ–‡æ¡£æŒ‡ä»¤ã€‘ç”¨æˆ·æ„å›¾æ˜¯å°†ç»“æœå†™å…¥å½“å‰ WPS æ–‡æ¡£/è¡¨æ ¼/æ¼”ç¤ºã€‚",
                "è¯·è¾“å‡ºä¸€ä¸ªå¯æ‰§è¡Œçš„ ```json``` Plan ä»£ç å—ï¼ˆä»…åŒ…å« JSONï¼Œä¸è¦è¾“å‡º HTML/è§£é‡Šæ–‡æœ¬ï¼‰ã€‚",
                f"schema_version å¿…é¡»æ˜¯ {PLAN_SCHEMA_ID}ï¼Œhost_app å¿…é¡»ä¸å½“å‰å®¿ä¸»ä¸€è‡´ã€‚",
                anchor_hint,
                "ä»…å†™å…¥ç”¨æˆ·æ˜ç¡®è¦æ±‚çš„æ–‡æ¡£å†…å®¹ï¼›ä¸è¦æŠŠå®¢å¥—è¯/å®Œæˆæç¤ºå†™å…¥æ­£æ–‡ï¼ˆä¾‹å¦‚ï¼šä¸å®¢æ°”ã€å·²å®Œæˆã€éšæ—¶å‘Šè¯‰æˆ‘ï¼‰ï¼Œé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚å†™å…¥ã€‚",
                "è¦æ±‚ï¼šä¼˜å…ˆä½¿ç”¨ op=upsert_block ä¿è¯å¹‚ç­‰ï¼Œå¹¶è®¾ç½®ç¨³å®š block_idï¼›å¤šæ®µè½ç”¨ \\nã€‚",
            ]
        )


    def _selected_default_writeback_delivery(self, selected_skills: list) -> str:

        """Pick the default writeback delivery mode from selected skills (highest priority wins)."""

        best_mode = ""

        best_pri = -10_000

        for s in (selected_skills or []):

            if not s:

                continue

            try:

                mode = str(getattr(s, "default_writeback", "") or "").strip().lower()

            except Exception:

                mode = ""

            if not mode:

                continue

            try:

                pri = int(getattr(s, "priority", 0) or 0)

            except Exception:

                pri = 0

            if pri >= best_pri:

                best_pri = pri

                best_mode = mode

        return best_mode



    def _build_compare_table_writeback_directive(self, user_message: str) -> str:
        """Special writeback directive for review-like scenarios (default: deliver compare table)."""
        anchor = self._writeback_anchor(user_message)
        anchor_hint = (
            "é»˜è®¤å†™åˆ°æ–‡æœ«/æ±‡æ€»åŒºï¼šè¯·å°† upsert_block.anchor è®¾ç½®ä¸º \"end\"ï¼ˆæ‰§è¡Œå™¨ä¼šè‡ªåŠ¨é”šå®šåˆ°æ–‡æœ«ï¼‰ã€‚"
            if anchor == "end"
            else "é»˜è®¤å†™å…¥å½“å‰å…‰æ ‡å¤„ï¼ˆupsert_block.anchor='cursor'ï¼‰ã€‚"
        )
        return "\n".join(
            [
                "ã€å®¡é˜…äº¤ä»˜æŒ‡ä»¤ã€‘è¿™æ˜¯â€œåˆåŒ/åˆ¶åº¦â€ç±»å®¡é˜…äº¤ä»˜ã€‚é»˜è®¤ä¸è¦ç›´æ¥æ”¹æ­£æ–‡ï¼ˆé¿å…è¯¯æ”¹ï¼‰ï¼Œè€Œæ˜¯å…ˆäº¤ä»˜â€œå¯¹ç…§è¡¨â€ã€‚",
                "è¯·è¾“å‡ºä¸€ä¸ªå¯æ‰§è¡Œçš„ ```json``` Plan ä»£ç å—ï¼ˆä»…åŒ…å« JSONï¼Œä¸è¦è¾“å‡º HTML/è§£é‡Šæ–‡æœ¬ï¼‰ï¼šæ’å…¥/åˆ·æ–°â€œå¯¹ç…§è¡¨äº¤ä»˜â€è¡¨æ ¼ã€‚",
                f"schema_version å¿…é¡»æ˜¯ {PLAN_SCHEMA_ID}ï¼Œhost_app å¿…é¡»ä¸å½“å‰å®¿ä¸»ä¸€è‡´ã€‚",
                "å¯¹ç…§è¡¨åˆ—å¿…é¡»åŒ…å«ï¼šåŸæ–‡è¦ç‚¹ / å»ºè®®æ”¹å†™ / ç†ç”± / é£é™©ç­‰çº§ / æ˜¯å¦åº”ç”¨ã€‚è‡³å°‘å†™ 3 è¡Œç¤ºä¾‹ï¼›â€œæ˜¯å¦åº”ç”¨â€é»˜è®¤å¡«â€œå¦â€ã€‚",
                "æ³¨æ„ï¼šç³»ç»Ÿä¼šåœ¨å®å¡ç‰‡ä¸Šæä¾›â€œä¸€é”®åº”ç”¨ï¼ˆç”Ÿæˆä¿®è®¢ç¨¿ï¼‰â€æŒ‰é’®ï¼ˆè¯»å–å¯¹ç…§è¡¨ä¸­â€œæ˜¯å¦åº”ç”¨=æ˜¯â€çš„è¡Œç”Ÿæˆæ±‡æ€»ï¼Œä¸æ”¹åŸæ­£æ–‡ï¼‰ã€‚ä½ æ— éœ€åœ¨æœ¬å®é‡Œè‡ªè¡Œå®ç°â€œåº”ç”¨â€ã€‚",
                anchor_hint,
                "è¦æ±‚ï¼šä¼˜å…ˆä½¿ç”¨ op=upsert_block ä¿è¯å¹‚ç­‰ï¼Œå¹¶è®¾ç½®ç¨³å®š block_idï¼›å¤šæ®µè½ç”¨ \\nã€‚",
            ]
        )




    def _response_has_strict_plan_block(self, text: str, host_app: str) -> bool:
        """Return True if text contains a valid Plan JSON fenced block."""
        if not text:
            return False
        plan, _err = self._extract_plan_from_text(text, host_app)
        return plan is not None

    def _extract_plan_from_text(self, text: str, host_app: str) -> tuple[Optional[dict], str]:
        payload = _extract_json_payload(text or "")
        if not payload:
            return None, "empty_plan_payload"
        host = (host_app or "wps").strip().lower()
        if host not in ("wps", "et", "wpp"):
            host = "wps"
        try:
            from pydantic import ValidationError
            from ah32.plan.schema import Plan
            from ah32.plan.normalize import normalize_plan_payload

            obj = json.loads(payload)
            plan = Plan.model_validate(normalize_plan_payload(obj, host_app=host))
        except ValidationError as e:
            return None, f"invalid_plan:{e.errors(include_url=False)}"
        except Exception as e:
            return None, f"invalid_plan:{e}"
        if plan.host_app != host:
            return None, f"host_app mismatch: request={host!r} plan={plan.host_app!r}"
        return plan.model_dump(mode="json"), ""

    def _apply_writeback_plan_overrides(self, plan: dict, *, block_id: str, anchor: str) -> dict:
        if not isinstance(plan, dict):
            return plan
        target_anchor = anchor if anchor in ("cursor", "end") else ""

        def _walk(actions: list) -> bool:
            for action in actions:
                if not isinstance(action, dict):
                    continue
                if action.get("op") == "answer_mode_apply":
                    if block_id:
                        action["block_id"] = block_id
                    return True
                if action.get("op") == "upsert_block":
                    if block_id:
                        action["block_id"] = block_id
                    if target_anchor:
                        action["anchor"] = target_anchor
                    return True
                nested = action.get("actions")
                if isinstance(nested, list) and _walk(nested):
                    return True
            return False

        try:
            actions = plan.get("actions")
            if isinstance(actions, list):
                _walk(actions)
        except Exception:
            return plan
        return plan

    async def _auto_repair_missing_writeback_plan(
        self,
        *,
        user_query: str,
        rag_context: str,
        session_id: str,
        anchor: str,
        delivery: str,
        original_plan: Optional[dict] = None,
        error_type: str = "missing_plan",
        error_message: str = "",
    ) -> Optional[dict]:
        """Retry once to obtain a Plan when writeback intent is detected but the model didn't output one."""
        if not self.llm:
            return None

        host_app = ""
        try:
            host_app = str(getattr(getattr(self, "_turn_run_context", None), "host_app", "") or "").strip().lower()
        except Exception:
            host_app = ""
        host = host_app if host_app in ("wps", "et", "wpp") else "wps"

        try:
            from ah32.services.plan_prompts import get_plan_repair_prompt

            sys_prompt = get_plan_repair_prompt(host)
        except Exception as e:
            logger.error(f"[writeback] build plan repair prompt failed: {e}", exc_info=True)
            return None

        try:
            block_id = f"wb_{self._get_message_hash((session_id or '') + '|' + (user_query or ''))}"
        except Exception:
            block_id = "ah32_writeback"

        delivery_hint = "compare_table" if str(delivery or "").strip().lower() == "compare_table" else ""
        original_text = json.dumps(original_plan or {}, ensure_ascii=False, default=str)

        user_prompt = (
            f"User request:\n{(user_query or '').strip()}\n\n"
            f"{(rag_context or '').strip()}\n\n"
            f"Writeback anchor: {anchor}\n"
            f"Delivery: {delivery_hint}\n"
            f"Suggested block_id: {block_id}\n\n"
            "Original plan:\n"
            f"{original_text}\n\n"
            f"Error: {error_type}\n"
            f"Error message: {error_message}\n"
        ).strip()

        resp = await self.llm.ainvoke([("system", sys_prompt), ("user", user_prompt)])
        raw = getattr(resp, "content", "") or str(resp)
        plan, _err = self._extract_plan_from_text(raw, host)
        if not plan:
            try:
                logger.debug(
                    "[writeback] plan fast path invalid plan: %s (raw_len=%s)",
                    _err,
                    len(raw or ""),
                )
            except Exception as e:
                logger.debug("[writeback] plan fast path debug log failed (ignored): %s", e, exc_info=True)
            return None
        plan = self._apply_writeback_plan_overrides(plan, block_id=block_id, anchor=anchor)
        return plan


    async def _generate_writeback_plan_fast_path(
        self,
        *,
        user_query: str,
        context: str,
        session_id: str,
        document_name: Optional[str],
        frontend_context: Optional[dict],
        anchor: str,
        delivery: str = "",
    ) -> Optional[dict]:
        """Generate a Plan with a dedicated prompt + (usually) a faster model.

        This is used for document writeback intents to improve the 0-repair success rate and reduce latency,
        while keeping the normal ReAct/tool loop for analysis-only turns.
        """
        if not self.llm:
            return None

        host_app = ""
        try:
            host_app = str(getattr(getattr(self, "_turn_run_context", None), "host_app", "") or "").strip().lower()
        except Exception:
            host_app = ""
        host = host_app if host_app in ("wps", "et", "wpp") else "wps"

        llm = self.llm

        # Capabilities (best-effort): small, host-filtered, deterministic JSON for prompt guidance.
        caps_text = ""
        try:
            caps = {}
            if isinstance(frontend_context, dict):
                caps = frontend_context.get("capabilities") or {}
                if not isinstance(caps, dict):
                    caps = {}
            if caps:
                caps_text = json.dumps(caps, ensure_ascii=False, indent=2, default=str)
                if len(caps_text) > 1800:
                    caps_text = caps_text[:1800] + "\n... (truncated)"
        except Exception as e:
            logger.debug(f"[writeback] normalize capabilities failed: {e}", exc_info=True)
            caps_text = ""

        try:
            block_id = f"wb_{self._get_message_hash((session_id or '') + '|' + (user_query or ''))}"
        except Exception:
            block_id = "ah32_writeback"

        delivery_hint = "compare_table" if str(delivery or "").strip().lower() == "compare_table" else ""

        try:
            from ah32.services.plan_prompts import get_plan_generation_prompt

            system = get_plan_generation_prompt(host)
        except Exception as e:
            logger.error(f"[writeback] build plan prompt failed: {e}", exc_info=True)
            return None

        user = (
            f"session_id: {session_id}\n"
            f"document_name: {document_name or ''}\n"
            f"host_app: {host}\n"
            f"capabilities: {caps_text}\n"
            f"writeback_anchor: {anchor}\n"
            f"delivery: {delivery_hint}\n"
            f"suggested_block_id: {block_id}\n\n"
            f"User request:\n{user_query}\n"
        )

        import asyncio as _asyncio

        t0 = time.perf_counter()
        resp = await _asyncio.shield(llm.ainvoke([("system", system), ("system", context), ("user", user)]))
        self._accumulate_token_usage(resp)
        ms = int((time.perf_counter() - t0) * 1000)
        try:
            self._turn_llm_total_ms = int(getattr(self, "_turn_llm_total_ms", 0) or 0) + ms
            if not getattr(self, "_turn_first_llm_ms", None):
                self._turn_first_llm_ms = ms
            tsvc = get_telemetry()
            if tsvc:
                tsvc.emit(
                    "chat.llm_call",
                    {"iteration": 1, "llm_ms": ms, "kind": "plan_fast_path"},
                    ctx=getattr(self, "_turn_run_context", None),
                )
        except Exception as e:
            logger.debug(f"[telemetry] emit chat.llm_call (plan_fast_path) failed: {e}", exc_info=True)

        raw = getattr(resp, "content", "") or str(resp)
        plan, _err = self._extract_plan_from_text(raw, host)
        if not plan:
            return None

        plan = self._apply_writeback_plan_overrides(plan, block_id=block_id, anchor=anchor)
        return plan


    def _create_react_prompt(self) -> str:

        """åˆ›å»ºReActç³»ç»Ÿæç¤º"""

        # è·å–åŸºç¡€ReActæç¤ºè¯

        base_prompt = get_react_system_prompt()



        # è·å–ç”¨æˆ·ä¿¡æ¯æŸ¥è¯¢çš„ç‰¹æ®Šå¤„ç†æç¤º

        from ah32.core.prompts import get_prompt



        user_info_prompt = get_prompt("user_info_context")



        # æ·»åŠ å·¥å…·æè¿°

        tools_desc = []

        for name, tool in self.tools.items():

            desc = getattr(tool, "description", f"Tool: {name}")

            tools_desc.append(f"- {name}: {desc}")



        # æ·»åŠ æ–‡æ¡£è¯»å–çš„ç‰¹æ®Šè¯´æ˜ï¼ˆç²¾ç®€ç‰ˆï¼Œé™ä½ tokensï¼‰

        file_path_note = (

            "æ–‡æ¡£è¯»å–ï¼šread_document è¾“å…¥æ”¯æŒ æ–‡ä»¶è·¯å¾„ / å·²æ‰“å¼€æ–‡æ¡£å / å·²æ‰“å¼€æ–‡æ¡£åºå·(å¦‚\"1\"). "

            "ä¼˜å…ˆæŒ‰ç”¨æˆ·ç»™çš„è·¯å¾„è¯»å–ï¼›æ”¯æŒ docx/doc/wps/txt/md/pdf/rtfã€‚"

        )



        # å°†å·¥å…·æè¿°æ›¿æ¢åˆ°åŸºç¡€æç¤ºè¯çš„{tools}å ä½ç¬¦ä¸­

        tools_section = f"{chr(10).join(tools_desc)}\n{file_path_note}"

        prompt = base_prompt.replace("{tools}", tools_section) + user_info_prompt



        return prompt



    def _get_message_hash(self, message: str) -> str:

        """ç”Ÿæˆæ¶ˆæ¯çš„å”¯ä¸€å“ˆå¸Œå€¼ï¼Œç”¨äºç¼“å­˜"""

        import hashlib



        # ä½¿ç”¨MD5å“ˆå¸Œï¼Œæˆªå–å‰16ä½ä½œä¸ºç¼“å­˜é”®

        return hashlib.md5(message.encode("utf-8")).hexdigest()[:16]



    def get_tool_calls_summary(self) -> List[Dict[str, Any]]:

        """è·å–å·¥å…·è°ƒç”¨æ±‡æ€»"""

        return [call.to_dict() for call in self.tool_calls]



    def clear_tool_calls(self):

        """æ¸…ç©ºå·¥å…·è°ƒç”¨è®°å½•"""

        self.tool_calls.clear()

        self._tool_cache.clear()

        self._tool_call_counts.clear()



    def get_classification_cache_stats(self) -> Dict[str, Any]:

        """è·å–LLMåˆ†ç±»ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""

        total_requests = self._classification_cache_hits + self._classification_cache_misses

        hit_rate = (

            (self._classification_cache_hits / total_requests * 100) if total_requests > 0 else 0

        )



        return {

            "hits": self._classification_cache_hits,

            "misses": self._classification_cache_misses,

            "total_requests": total_requests,

            "hit_rate": f"{hit_rate:.2f}%",

            "cache_size": len(self._classification_cache),

        }



    def clear_classification_cache(self):

        """æ¸…ç©ºLLMåˆ†ç±»ç¼“å­˜"""

        self._classification_cache.clear()

        self._classification_cache_hits = 0

        self._classification_cache_misses = 0

        logger.debug("[ç¼“å­˜] å·²æ¸…ç©ºLLMåˆ†ç±»ç¼“å­˜")



    async def _extract_and_store_user_info(

        self, message: str, session_id: str, classification_result=None

    ):

        """æå–å’Œå­˜å‚¨ç”¨æˆ·ä¿¡æ¯"""

        try:

            if classification_result and hasattr(classification_result, "identity_info"):

                user_info = {

                    "identity_info": classification_result.identity_info,

                    "confidence": classification_result.confidence,

                    "timestamp": datetime.now().isoformat(),

                }

            else:

                user_info = {"message": message, "timestamp": datetime.now().isoformat()}



            # å­˜å‚¨åˆ°å…¨å±€è®°å¿†

            await self.memory_system.update_global_user_info(

                session_id=session_id, user_info=user_info

            )

            logger.debug("[è®°å¿†] å·²æ›´æ–°ç”¨æˆ·ä¿¡æ¯")

        except Exception as e:

            logger.error(f"[è®°å¿†] ç”¨æˆ·ä¿¡æ¯æå–å¤±è´¥: {e}")



    async def _extract_and_store_user_preferences(

        self, message: str, session_id: str, classification_result=None

    ):

        """æå–å’Œå­˜å‚¨ç”¨æˆ·åå¥½"""

        try:

            if classification_result and hasattr(classification_result, "preferences"):

                user_prefs = {

                    "preferences": classification_result.preferences,

                    "confidence": classification_result.confidence,

                    "timestamp": datetime.now().isoformat(),

                }

            else:

                user_prefs = {"message": message, "timestamp": datetime.now().isoformat()}



            if user_prefs.get("preferences"):

                prefs_dict = user_prefs["preferences"]

                if isinstance(prefs_dict, dict) and prefs_dict:

                    first_key = list(prefs_dict.keys())[0]

                    first_value = prefs_dict[first_key]

                    await self.memory_system.learn_user_preference(

                        session_id=session_id,

                        preference_type=first_key,

                        preference_value=str(first_value),

                        feedback=message[:100],

                    )

            logger.debug("[è®°å¿†] å·²æ›´æ–°ç”¨æˆ·åå¥½")

        except Exception as e:

            logger.error(f"[è®°å¿†] ç”¨æˆ·åå¥½æå–å¤±è´¥: {e}")



    def _parse_tool_call(self, llm_response: str) -> Optional[Dict[str, Any]]:

        """è§£æLLMè¿”å›çš„å·¥å…·è°ƒç”¨ï¼ˆå¤„ç†åŒèŠ±æ‹¬å·æ ¼å¼ï¼‰.

        å…¼å®¹ä¸¤ç±»æ ¼å¼ï¼š
        1) æ—§æ ¼å¼: {"action": "tool_name", "input": ...}
        2) æ–°æ ¼å¼: {"name": "tool_name", "arguments": {...}}

        å®‰å…¨ç­–ç•¥ï¼š
        - ä¼˜å…ˆä»…åœ¨â€œæ•´ä¸ªå“åº”å°±æ˜¯ JSON å¯¹è±¡â€æ—¶è§£æï¼ˆé¿å…æŠŠå®ä»£ç é‡Œçš„ `{...}` è¯¯åˆ¤ä¸ºå·¥å…· JSONï¼‰ã€‚
        - è‹¥ä¸¥æ ¼è§£æå¤±è´¥ï¼Œå†å°è¯•ä»â€œå“åº”æœ«å°¾â€æå–æœ€åä¸€ä¸ªå·¥å…· JSONï¼ˆå¸¸è§äºæ¨¡å‹å…ˆè¯´ä¸€å¥è¯å†ç»™ JSONï¼‰ã€‚
        """

        try:

            # å¤„ç†åŒèŠ±æ‹¬å·æ ¼å¼ {{...}}

            cleaned = str(llm_response or "").strip()

            # å»æ‰å¤–å±‚èŠ±æ‹¬å·

            if cleaned.startswith("{{") and cleaned.endswith("}}"):

                cleaned = cleaned[1:-1].strip()



            def _is_tool_call_obj(obj: Any) -> bool:
                if not isinstance(obj, dict):
                    return False
                if "action" in obj and isinstance(obj.get("action"), str):
                    return True
                if "name" in obj and isinstance(obj.get("name"), str):
                    return True
                return False

            def _get_tool_name(obj: Dict[str, Any]) -> str:
                if "name" in obj and isinstance(obj.get("name"), str):
                    return str(obj.get("name") or "").strip()
                return str(obj.get("action") or "").strip()

            def _is_known_tool(name: str) -> bool:
                if not name:
                    return False

                # Built-in tools
                try:
                    tools = getattr(self, "tools", None)
                    if isinstance(tools, dict) and name in tools:
                        return True
                except Exception:
                    logger.debug("[tools] check built-in tools failed (ignored)", exc_info=True)

                # Skill tools (declared by selected skills)
                try:
                    reg = getattr(self, "skills_registry", None)
                    selected = getattr(self, "_selected_skills", None) or []
                    if reg is not None and selected:
                        for skill in selected:
                            try:
                                if reg.find_tool(skill.skill_id, name) is not None:
                                    return True
                            except Exception:
                                continue
                except Exception:
                    logger.debug("[tools] check skill tools failed (ignored)", exc_info=True)
                return False



            # 1) ä¸¥æ ¼è§£æï¼šä»…å½“æ•´ä¸ªå“åº”å°±æ˜¯ JSON å¯¹è±¡æ—¶æ‰å°è¯•è§£æå·¥å…·è°ƒç”¨ã€‚

            # å¦åˆ™åƒ JS å®ä»£ç é‡Œçš„ `{ ... }`ï¼ˆfor/try/catch å—ï¼‰ä¼šè¢«è¯¯è¯†åˆ«ä¸ºå·¥å…· JSONã€‚

            if cleaned.startswith("{"):

                try:
                    tool_call = json.loads(cleaned)
                except Exception as e:
                    logger.debug(f"è§£æå·¥å…·è°ƒç”¨å¤±è´¥: {e}")
                    tool_call = None

                if _is_tool_call_obj(tool_call):
                    tool_name = _get_tool_name(tool_call)
                    if _is_known_tool(tool_name):
                        return tool_call



            # 2) å®½æ¾è§£æï¼šä»å“åº”æœ«å°¾æå–æœ€åä¸€ä¸ªå·¥å…· JSONï¼ˆå¸¸è§äºâ€œå…ˆè¯´ä¸€å¥å†ç»™ JSONâ€ï¼‰ã€‚

            # ä»…æ¥å—ï¼šè§£æå‡ºçš„ JSON å¿…é¡»æ˜¯å·¥å…·è°ƒç”¨å¯¹è±¡ï¼Œä¸” JSON ä¹‹ååªå…è®¸ç©ºç™½å­—ç¬¦ã€‚

            tail = cleaned[-10000:] if len(cleaned) > 10000 else cleaned
            if "```" in tail:
                # é¿å…ä» fenced code block ä¸­è¯¯æå–
                tail = self._strip_fenced_code_blocks(tail)

            tool_start_re = re.compile(r'\{\s*"(?:name|action)"\s*:')
            matches = list(tool_start_re.finditer(tail))
            if not matches:
                return None

            def _is_ignorable_trailing_text(s: str) -> bool:
                """Allow minor punctuation after a tool-call JSON.

                Some models emit: `{...}ã€‚` or `{...}.` which is still clearly a tool call, but would
                fail the strict "JSON must be the last token" rule.
                """
                try:
                    rest = str(s or "")
                except Exception:
                    rest = ""
                if not rest.strip():
                    return True
                # Only allow whitespace and common punctuation (including Chinese punctuation).
                return bool(re.fullmatch(r"[\s\.,!\?;:\-â€“â€”ã€ï¼Œã€‚ï¼›ï¼šâ€¦ï¼ˆï¼‰()\[\]ã€ã€‘]*", rest))

            decoder = json.JSONDecoder()
            for m in reversed(matches):
                start = m.start()
                try:
                    obj, end = decoder.raw_decode(tail[start:])
                except json.JSONDecodeError:
                    continue
                except Exception:
                    logger.debug("è§£æå·¥å…·è°ƒç”¨å¤±è´¥ï¼ˆå®½æ¾è§£æå¼‚å¸¸ï¼‰", exc_info=True)
                    continue

                if not _is_tool_call_obj(obj):
                    continue

                # Ensure the extracted JSON is the last meaningful content.
                # (Allow trailing punctuation like "ã€‚"/".", etc.)
                if not _is_ignorable_trailing_text(tail[start + end :]):
                    continue

                tool_name = _get_tool_name(obj)
                if _is_known_tool(tool_name):
                    logger.debug("[tools] parsed tool call from tail: %s", tool_name)
                    return obj



        except Exception as e:

            logger.debug(f"è§£æå·¥å…·è°ƒç”¨å¤±è´¥: {e}")



        return None


    def _looks_truncated_text(self, text: str) -> bool:
        """Heuristic: detect frontend/backend capped document text markers."""
        try:
            s = str(text or "")
            if not s:
                return False
            if "truncated_by_max_chars" in s:
                return True
            if "...(truncated)" in s:
                return True
            if s.rstrip().endswith("...(truncated)"):
                return True
            return False
        except Exception:
            logger.debug("[doc_text] truncated marker check failed (ignored)", exc_info=True)
            return False



    async def _execute_tool(self, tool_call: Dict[str, Any]) -> str:

        """æ‰§è¡Œå·¥å…·è°ƒç”¨ - å¢åŠ ä¼šè¯çº§ç¼“å­˜ä»¥é¿å…é‡å¤å¤–éƒ¨è°ƒç”¨"""

        is_skill_format = ("name" in tool_call)
        tool_name = str(tool_call.get("name") if is_skill_format else tool_call.get("action") or "").strip()
        tool_input = tool_call.get("arguments") if is_skill_format else tool_call.get("input", "")



        # æ„é€ ç¼“å­˜é”®ï¼Œå°½é‡è§„èŒƒåŒ–è¾“å…¥ä»¥ä¾¿æ¯”è¾ƒ

        try:

            import json as _json



            key_input = _json.dumps(tool_input, sort_keys=True, ensure_ascii=False)

        except Exception:

            key_input = str(tool_input)

        cache_key = (tool_name, key_input)



        # increment call counter for observability

        self._tool_call_counts[cache_key] = self._tool_call_counts.get(cache_key, 0) + 1



        # è¿”å›ç¼“å­˜çš„ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰

        if cache_key in self._tool_cache:

            logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„å·¥å…·ç»“æœ: {tool_name} (input fingerprint)")

            return self._tool_cache[cache_key]

        # Remote-friendly: `read_document` can fall back to frontend-provided active doc text.
        # Why: on remote backends, synced document paths often point to client-local Windows paths
        # (e.g. C:\\Users\\...), which do not exist on the server.
        #
        # Privacy policy for chat:
        # - Prefer *ephemeral* per-request active doc payload (active_doc_text_full/active_doc_text)
        # - Do NOT fetch the active document content from RAG as an implicit "backup"
        try:
            if tool_name == "read_document":
                requested = ""
                if isinstance(tool_input, dict):
                    requested = str(
                        tool_input.get("file_path")
                        or tool_input.get("path")
                        or tool_input.get("source_path")
                        or tool_input.get("query")
                        or ""
                    ).strip()
                else:
                    requested = str(tool_input or "").strip()

                ctx = getattr(self, "_turn_frontend_context", None)
                if requested and isinstance(ctx, dict):
                    active = ctx.get("activeDocument") or ctx.get("document") or {}
                    active_id = str(active.get("id") or active.get("docId") or active.get("doc_id") or "").strip()
                    active_name = str(active.get("name") or "").strip()
                    active_path = str(active.get("fullPath") or active.get("path") or "").strip()

                    rc = ctx.get("run_context") or ctx.get("runContext") or {}
                    doc_key = str(rc.get("doc_key") or rc.get("docKey") or "").strip()

                    direct_text = (
                        ctx.get("active_doc_text")
                        or ctx.get("activeDocText")
                        or ctx.get("active_document_text")
                        or ctx.get("activeDocumentText")
                    )
                    direct_full = (
                        ctx.get("active_doc_text_full")
                        or ctx.get("activeDocTextFull")
                        or ctx.get("active_document_text_full")
                        or ctx.get("activeDocumentTextFull")
                    )

                    same_doc = requested in (active_id, active_name, active_path, doc_key)
                    if not same_doc and doc_key:
                        # Common case: requested="doc_xxx", doc_key="wps:doc_xxx"
                        same_doc = doc_key.endswith(requested) or requested.endswith(doc_key)

                    if same_doc:
                        # Prefer full active doc payload when available (ephemeral per request).
                        if isinstance(direct_full, str) and direct_full.strip():
                            name = active_name or requested
                            content_full = str(direct_full or "")
                            # Cap tool output to avoid blowing up the LLM context.
                            content = content_full
                            if len(content) > 60_000:
                                content = content[: 60_000 - 20] + "\n...(truncated)"
                            char_count = len(content_full)
                            line_count = content_full.count("\n") + 1 if content_full else 0

                            rendered = "\n".join(
                                [
                                    f"=== æ–‡æ¡£å†…å®¹(å‰ç«¯æå–): {name} ===\n",
                                    f"ã€ç»Ÿè®¡ã€‘å­—ç¬¦: {char_count}, è¡Œæ•°: {line_count}",
                                    "æ¥æº: frontend_context.active_doc_text_full\n",
                                    "=" * 50,
                                    "",
                                    content,
                                    "",
                                    "=" * 50,
                                    f"è¯»å–å®Œæˆï¼š{name}",
                                ]
                            )
                            self._tool_cache[cache_key] = rendered
                            return rendered

                        # Fall back to frontend-provided text preview (bounded).
                        if isinstance(direct_text, str) and direct_text.strip():
                            doc_text = maybe_extract_active_docx(ctx, max_chars=60_000)
                            if doc_text:
                                name = active_name or requested
                                content = str(doc_text or "")
                                char_count = len(content)
                                line_count = content.count("\n") + 1 if content else 0

                                rendered = "\n".join(
                                    [
                                        f"=== æ–‡æ¡£å†…å®¹(å‰ç«¯æå–): {name} ===\n",
                                        f"ã€ç»Ÿè®¡ã€‘å­—ç¬¦: {char_count}, è¡Œæ•°: {line_count}",
                                        "æ¥æº: frontend_context.active_doc_text\n",
                                        "=" * 50,
                                        "",
                                        content,
                                        "",
                                        "=" * 50,
                                        f"è¯»å–å®Œæˆï¼š{name}",
                                    ]
                                )
                                self._tool_cache[cache_key] = rendered
                                return rendered
        except Exception as e:
            logger.debug("[read_document] active_doc_text fast path failed (ignored): %s", e, exc_info=True)



        try:

            # First try built-in ReAct tools
            tool = self.tools.get(tool_name)

            # Then try skill-declared tools (name/arguments)
            if tool is None and self._skill_tool_executor is not None and self.skills_registry is not None:
                selected = tuple(self._selected_skills or [])
                matches = []
                for skill in selected:
                    skill_tool = self.skills_registry.find_tool(skill.skill_id, tool_name)
                    if skill_tool is not None:
                        matches.append((skill, skill_tool))

                if len(matches) > 1:
                    rendered = (
                        "é”™è¯¯ï¼šæŠ€èƒ½å·¥å…·åå†²çªï¼ˆå¤šä¸ªå·²é€‰æŠ€èƒ½æä¾›åŒåå·¥å…·ï¼‰ã€‚"
                        f"tool={tool_name} candidates={','.join([s.skill_id for s, _ in matches])}"
                    )
                    self._tool_cache[cache_key] = rendered
                    return rendered

                if len(matches) == 1:
                    _, skill_tool = matches[0]
                    try:
                        if not isinstance(tool_input, dict):
                            return f"é”™è¯¯ï¼šå·¥å…·å‚æ•°å¿…é¡»ä¸ºå¯¹è±¡ï¼ˆdictï¼‰ã€‚tool={tool_name}"

                        tool_args = dict(tool_input)
                        try:
                            props = (
                                skill_tool.parameters.get("properties")
                                if isinstance(getattr(skill_tool, "parameters", None), dict)
                                else {}
                            )
                            if isinstance(props, dict) and "doc_text" in props:
                                if not str(tool_args.get("doc_text") or "").strip():
                                    active_doc_text = str(getattr(self, "_turn_active_doc_text", "") or "")
                                    # Ensure the tool call always includes doc_text when the schema declares it.
                                    # Some skill tool scripts define `doc_text` as a positional arg without default;
                                    # passing an empty string keeps execution deterministic and avoids TypeError.
                                    use_text = active_doc_text if active_doc_text else ""

                                    # If preview is truncated/missing, try to use the full active doc payload
                                    # from frontend_context (ephemeral per request; not persisted).
                                    if (not str(use_text or "").strip()) or self._looks_truncated_text(use_text):
                                        try:
                                            ctx = getattr(self, "_turn_frontend_context", None)
                                            full_text = maybe_extract_active_doc_text_full(ctx)
                                            if full_text and str(full_text).strip():
                                                use_text = str(full_text or "")
                                        except Exception as e:
                                            logger.debug("[tools] inject doc_text from frontend_context failed (ignored): %s", e, exc_info=True)

                                    tool_args["doc_text"] = use_text if use_text else ""

                            # Inject active document path when the tool schema declares it.
                            # This enables project-scoped ingestion APIs (derive project from contextDocumentPath).
                            if isinstance(props, dict) and (
                                ("context_document_path" in props) or ("contextDocumentPath" in props)
                            ):
                                snake_key = "context_document_path"
                                camel_key = "contextDocumentPath"
                                needs_snake = ("context_document_path" in props) and (not str(tool_args.get(snake_key) or "").strip())
                                needs_camel = ("contextDocumentPath" in props) and (not str(tool_args.get(camel_key) or "").strip())
                                if needs_snake or needs_camel:
                                    try:
                                        ctx = getattr(self, "_turn_frontend_context", None) or {}
                                        active = (ctx or {}).get("activeDocument") or {}
                                        p = (active.get("fullPath") or active.get("path") or "").strip()
                                        if p:
                                            if needs_snake:
                                                tool_args[snake_key] = p
                                            if needs_camel:
                                                tool_args[camel_key] = p
                                    except Exception as e:
                                        logger.debug(
                                            "[tools] inject context_document_path from frontend_context failed (ignored): %s",
                                            e,
                                            exc_info=True,
                                        )
                        except Exception as e:
                            logger.debug(f"[tools] inject active doc text failed (ignored): {e}", exc_info=True)

                        ok, err = self._skill_tool_executor.validate_tool_arguments(skill_tool, tool_args)
                        if not ok:
                            return f"é”™è¯¯ï¼šå·¥å…·å‚æ•°æ ¡éªŒå¤±è´¥ã€‚tool={tool_name} err={err}"
                        # Run skill tools in a dedicated thread to avoid blocking the asyncio loop.
                        # This is required for integrations like Playwright Sync API.
                        from ah32.skills.tool_executor import get_skill_tool_thread_pool

                        loop = asyncio.get_running_loop()
                        exec_result = await loop.run_in_executor(
                            get_skill_tool_thread_pool(),
                            self._skill_tool_executor.execute_tool,
                            skill_tool,
                            tool_args,
                        )
                        rendered = self._skill_tool_executor.render_tool_result_for_llm(skill_tool, exec_result)
                        self._tool_cache[cache_key] = rendered
                        return rendered
                    except Exception as e:
                        logger.error(f"æ‰§è¡ŒæŠ€èƒ½å·¥å…· {tool_name} å¤±è´¥: {e}", exc_info=True)
                        return f"é”™è¯¯ï¼šå·¥å…·æ‰§è¡Œå¤±è´¥ã€‚tool={tool_name} err={str(e)}"

            if not tool:

                return f"é”™è¯¯ï¼šæœªæ‰¾åˆ°å·¥å…· '{tool_name}'"

            tool_input_for_builtin = tool_input
            if not isinstance(tool_input_for_builtin, str):
                try:
                    tool_input_for_builtin = json.dumps(tool_input_for_builtin, ensure_ascii=False)
                except Exception:
                    tool_input_for_builtin = str(tool_input_for_builtin)



            # æ‰§è¡Œå·¥å…·ï¼ˆä¼ é€’LLMå®ä¾‹ç»™éœ€è¦çš„å·¥å…·ï¼‰

            if tool_name in [

                "analyze_chapter",

                "extract_requirements",

                "extract_responses",

                "match_requirements",

                "assess_quality",

                "assess_risks",

                "answer_question",


            ]:

                # ä¼ é€’LLMå®ä¾‹è¿›è¡Œç»“æ„åŒ–åˆ†æ

                if hasattr(tool, "arun"):

                    result = await tool.arun(tool_input_for_builtin, llm=self.llm)

                else:

                    result = tool.run(tool_input_for_builtin, llm=self.llm)

            else:

                # å…¶ä»–å·¥å…·ä¸ä¼ é€’LLM

                if hasattr(tool, "arun"):

                    result = await tool.arun(tool_input_for_builtin)

                else:

                    result = tool.run(tool_input_for_builtin)



            result_str = str(result)



            # å°†ç»“æœå†™å…¥ç¼“å­˜å¹¶è¿”å›

            try:

                self._tool_cache[cache_key] = result_str

            except Exception:

                logger.debug("æ— æ³•ç¼“å­˜å·¥å…·ç»“æœï¼ˆå¯èƒ½ä¸å¯åºåˆ—åŒ–ï¼‰")



            return result_str



        except Exception as e:

            logger.error(f"æ‰§è¡Œå·¥å…· {tool_name} å¤±è´¥: {e}")

            return f"é”™è¯¯ï¼šæ‰§è¡Œå·¥å…· '{tool_name}' å¤±è´¥ - {str(e)}"



    def _build_context(

        self,

        memory: Dict[str, Any],

        session_id: Optional[str],

        query_message: str = "",

        classification_result=None,

    ) -> str:

        """æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨ç­–ç•¥æ¨¡å¼ï¼‰



        Args:

            memory: æ£€ç´¢åˆ°çš„è®°å¿†

            session_id: ä¼šè¯ID

            query_message: ç”¨æˆ·æŸ¥è¯¢æ¶ˆæ¯ï¼ˆç”¨äºåŠ¨æ€åˆ¤æ–­æŸ¥è¯¢ç±»å‹ï¼‰

            classification_result: LLMåˆ†ç±»ç»“æœï¼ˆå¯é€‰ï¼‰

        """

        context_parts = []



        # ä½¿ç”¨LLMåˆ†ç±»ç»“æœåŠ¨æ€æ„å»ºä¼˜å…ˆçº§

        priority_order = None

        query_type = "regular"



        # åŠ¨æ€åˆ¤æ–­æ˜¯å¦æ˜¯ç”¨æˆ·ä¿¡æ¯æŸ¥è¯¢ï¼ˆæ ¹æ®æ¶ˆæ¯å†…å®¹ï¼‰

        user_info_keywords = ["æˆ‘æ˜¯", "æˆ‘å«", "æˆ‘çš„åå­—", "æˆ‘æ˜¯è°", "è®°ä½", "ä¸ªäººä¿¡æ¯", "ç”¨æˆ·ä¿¡æ¯"]

        user_info_query = any(kw in query_message for kw in user_info_keywords)



        try:

            from ah32.strategies.context_strategy import InfoPriority, context_strategy



            # å¦‚æœæœ‰åˆ†ç±»ç»“æœï¼Œä½¿ç”¨ç®€åŒ–åˆ†ç±»æ¥æ¨æ–­æŸ¥è¯¢ç±»å‹

            if classification_result:

                try:

                    from ah32.strategies.llm_driven_strategy import StorageLevel



                    # åŸºäºåˆ†ç±»ç»“æœæ¨æ–­æŸ¥è¯¢ç±»å‹

                    if classification_result.category.value == "identity":

                        query_type = "user_info"

                    elif classification_result.category.value == "preference":

                        query_type = "user_info"

                    elif classification_result.category.value in ["technical", "commercial", "project", "timeline"]:

                        query_type = "analysis"



                    logger.debug(f"[ä¸Šä¸‹æ–‡æ„å»º] ä½¿ç”¨ç®€åŒ–å…³é”®è¯åˆ†ç±»ç»“æœæ„å»ºä¸Šä¸‹æ–‡")

                    logger.debug(f"[ä¸Šä¸‹æ–‡æ„å»º] åˆ†ç±»ç±»åˆ«: {classification_result.category.value}")

                    logger.debug(f"[ä¸Šä¸‹æ–‡æ„å»º] å­˜å‚¨çº§åˆ«: {classification_result.storage_level.value}")

                    logger.debug(f"[ä¸Šä¸‹æ–‡æ„å»º] æŸ¥è¯¢ç±»å‹: {query_type}")



                    # æ ¹æ®å­˜å‚¨çº§åˆ«ç¡®å®šä¼˜å…ˆçº§ï¼ˆå§‹ç»ˆåŒ…å«@å¼•ç”¨å¤„ç†ï¼‰

                    if classification_result.storage_level == StorageLevel.P0_GLOBAL:

                        priority_order = [InfoPriority.P0_AT_REFERENCE, InfoPriority.P0_IDENTITY, InfoPriority.P2_HISTORY, InfoPriority.P1_SESSION]

                    elif classification_result.storage_level == StorageLevel.P2_CROSS_SESSION:

                        priority_order = [InfoPriority.P0_AT_REFERENCE, InfoPriority.P2_HISTORY, InfoPriority.P0_IDENTITY, InfoPriority.P1_SESSION]

                    else:

                        priority_order = [InfoPriority.P0_AT_REFERENCE, InfoPriority.P1_SESSION, InfoPriority.P2_HISTORY, InfoPriority.P0_IDENTITY]



                except Exception as e:

                    logger.warning(f"[ä¸Šä¸‹æ–‡æ„å»º] ä½¿ç”¨ç®€åŒ–åˆ†ç±»ç»“æœæ„å»ºä¼˜å…ˆçº§å¤±è´¥: {e}")

                    priority_order = None



            # å¦‚æœæ²¡æœ‰LLMåˆ†ç±»ç»“æœï¼Œä½¿ç”¨åŸæœ‰ç­–ç•¥æ¨¡å¼ï¼ˆå§‹ç»ˆåŒ…å«@å¼•ç”¨å¤„ç†ï¼‰

            if not priority_order:

                # åˆ¤æ–­æŸ¥è¯¢ç±»å‹

                query_type = "user_info" if user_info_query else "regular"

                if "åˆ†æ" in query_message or "è¯„ä¼°" in query_message:

                    query_type = "analysis"



                # ä½¿ç”¨ context_strategy çš„ get_info_source_for_query æ–¹æ³•

                base_priority_order = context_strategy.get_info_source_for_query(

                    query_message, query_type

                )



                # ç¡®ä¿@å¼•ç”¨å¤„ç†å§‹ç»ˆåœ¨ä¼˜å…ˆçº§åˆ—è¡¨ä¸­

                if InfoPriority.P0_AT_REFERENCE not in base_priority_order:

                    # åœ¨æœ€å‰é¢æ’å…¥@å¼•ç”¨å¤„ç†

                    priority_order = [InfoPriority.P0_AT_REFERENCE] + list(base_priority_order)

                else:

                    priority_order = list(base_priority_order)



                logger.debug(f"[ä¸Šä¸‹æ–‡æ„å»º] ä½¿ç”¨å…³é”®è¯ç­–ç•¥æ„å»ºä¸Šä¸‹æ–‡ï¼ŒæŸ¥è¯¢ç±»å‹: {query_type}")



            # æ ¹æ®ä¼˜å…ˆçº§æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯

            for info_type in priority_order:

                if info_type == InfoPriority.P0_IDENTITY:

                    # æ·»åŠ å…¨å±€è®°å¿†ä¿¡æ¯ï¼ˆç”¨æˆ·èº«ä»½ã€å…¬å¸ã€åå¥½ç­‰ï¼‰

                    if "global_memory" in memory and memory["global_memory"]:

                        global_mem = memory["global_memory"]

                        if isinstance(global_mem, dict):

                            if global_mem.get("user_profile"):

                                context_parts.append(f"ç”¨æˆ·èº«ä»½ä¿¡æ¯:\n{global_mem['user_profile']}")

                            if global_mem.get("user_preferences"):

                                context_parts.append(

                                    f"ç”¨æˆ·åå¥½ä¿¡æ¯:\n{global_mem['user_preferences']}"

                                )

                        else:

                            context_parts.append(f"å…¨å±€è®°å¿†ä¿¡æ¯:\n{global_mem}")



                elif info_type == InfoPriority.P1_SESSION:

                    # æ·»åŠ å½“å‰ä¼šè¯è®°å¿†ï¼ˆå¯¹è¯å†å²ï¼‰

                    if "session_memory" in memory and memory["session_memory"]:

                        session_mem = memory["session_memory"]

                        if isinstance(session_mem, list) and session_mem:

                            # æ ¼å¼åŒ–å¯¹è¯å†å²ï¼Œä½¿ç”¨æ›´æ¸…æ™°çš„æ ¼å¼è®©LLMæ›´å®¹æ˜“è¯†åˆ«

                            history_text = "å¯¹è¯å†å²:\n\n"

                            def _sanitize_history_text(s: str) -> str:

                                try:

                                    text = str(s or "")

                                    # Avoid "old signature" pollution: older sessions may include deprecated API calls.

                                    if "upsertBlock({" in text or "BID.upsertBlock({" in text:

                                        text = text.replace("BID.upsertBlock({", "BID.upsertBlock(")

                                        text = text.replace("upsertBlock({", "upsertBlock(")

                                        text += (

                                            "\n\n[ç³»ç»Ÿæç¤º] æ³¨æ„ï¼šå†å²é‡Œå¯èƒ½å‡ºç°è¿‡æœŸç­¾åã€‚å½“å‰ç‰ˆæœ¬æ­£ç¡®ç­¾åæ˜¯ "

                                            "BID.upsertBlock(blockId, function(){...}, opts?)ï¼Œä¸è¦ä½¿ç”¨ upsertBlock({id, content})ã€‚"

                                        )

                                    return text

                                except Exception:

                                    return str(s or "")

                            for msg in session_mem[-10:]:  # åªå–æœ€è¿‘10æ¡

                                role = msg.get("role", "unknown")

                                message = _sanitize_history_text(msg.get("message", ""))

                                # ä½¿ç”¨æ›´æ¸…æ™°çš„å¯¹è¯æ ¼å¼ï¼Œè®©LLMæ›´å®¹æ˜“è¯†åˆ«

                                history_text += f"**{role}**: {message}\n\n"

                            context_parts.append(history_text)

                            logger.debug(f"[ä¸Šä¸‹æ–‡æ„å»º] åŒ…å« {len(session_mem)} æ¡å¯¹è¯å†å²")

                        elif isinstance(session_mem, str):

                            context_parts.append(f"ä¼šè¯è®°å¿†:\n{session_mem}")



                elif info_type == InfoPriority.P2_HISTORY:

                    # æ·»åŠ è·¨ä¼šè¯è®°å¿†ï¼ˆå†å²æ£€ç´¢ç»“æœï¼‰

                    if "cross_session_memory" in memory and memory["cross_session_memory"]:

                        cross_mem = memory["cross_session_memory"]

                        if isinstance(cross_mem, list) and cross_mem:

                            history_text = "ç›¸å…³å†å²å¯¹è¯:\n"

                            for item in cross_mem[:3]:  # åªå–å‰3æ¡

                                content = item.get("content", "")

                                history_text += f"- {content[:200]}\n"

                            context_parts.append(history_text)

                        elif isinstance(cross_mem, str):

                            context_parts.append(f"å†å²è®°å¿†:\n{cross_mem}")



                elif info_type == InfoPriority.P3_TOOLS:

                    # å·¥å…·æ‰§è¡Œå†å²å·²é€šè¿‡ self.tool_calls åœ¨å¾ªç¯ä¸­ä¼ é€’

                    pass



                elif info_type == InfoPriority.P0_AT_REFERENCE:

                    # ğŸ”§ æ–°å¢ï¼šæ·»åŠ @å¼•ç”¨å¤„ç†ç»“æœ

                    if "at_reference_context" in memory and memory["at_reference_context"]:

                        at_context = memory["at_reference_context"]

                        if isinstance(at_context, dict):

                            # æ·»åŠ @å¼•ç”¨å¤„ç†çŠ¶æ€

                            processed_count = len(at_context.get("at_references_processed", []))

                            errors_count = len(at_context.get("at_references_errors", []))

                            paths_count = len(at_context.get("at_references_paths", []))



                            if processed_count > 0:

                                context_parts.append(f"âœ… @å¼•ç”¨æ–‡æ¡£å¤„ç†çŠ¶æ€:\n- æˆåŠŸå¤„ç†: {processed_count} ä¸ªæ–‡æ¡£\n- æ–‡ä»¶è·¯å¾„: {paths_count} ä¸ª\n- é”™è¯¯æ•°é‡: {errors_count}")



                                # å¦‚æœæœ‰å¤„ç†æˆåŠŸçš„æ–‡æ¡£ï¼Œåˆ—å‡ºå®ƒä»¬

                                if at_context.get("at_references_processed"):

                                    processed_files = at_context["at_references_processed"]

                                    context_parts.append("âœ… å·²å¤„ç†çš„@å¼•ç”¨æ–‡ä»¶:")

                                    for i, file_info in enumerate(processed_files[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª

                                        if isinstance(file_info, dict):

                                            filename = file_info.get("filename", "æœªçŸ¥æ–‡ä»¶")

                                            context_parts.append(f"  {i}. {filename}")

                                        else:

                                            context_parts.append(f"  {i}. {str(file_info)}")



                                # å¦‚æœæœ‰é”™è¯¯ï¼Œåˆ—å‡ºå®ƒä»¬

                                if at_context.get("at_references_errors"):

                                    errors = at_context["at_references_errors"]

                                    context_parts.append(f"âš ï¸ @å¼•ç”¨å¤„ç†é”™è¯¯ ({len(errors)} ä¸ª):")

                                    for i, error in enumerate(errors[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª

                                        context_parts.append(f"  {i}. {str(error)[:100]}...")



                            elif paths_count > 0:

                                # æœ‰è·¯å¾„ä½†å¤„ç†å¤±è´¥

                                context_parts.append(f"âš ï¸ @å¼•ç”¨æ–‡æ¡£å¤„ç†çŠ¶æ€:\n- æ£€æµ‹åˆ°: {paths_count} ä¸ªå¼•ç”¨æ–‡ä»¶\n- å¤„ç†å¤±è´¥: {errors_count} ä¸ª\n- é”™è¯¯ä¿¡æ¯:")

                                if at_context.get("at_references_errors"):

                                    errors = at_context["at_references_errors"]

                                    for i, error in enumerate(errors[:2], 1):  # åªæ˜¾ç¤ºå‰2ä¸ª

                                        context_parts.append(f"  {i}. {str(error)[:80]}...")

                            else:

                                # å®Œå…¨æ²¡æœ‰ä»»ä½•@å¼•ç”¨

                                context_parts.append("â„¹ï¸ @å¼•ç”¨æ–‡æ¡£å¤„ç†çŠ¶æ€: æœªæ£€æµ‹åˆ°@å¼•ç”¨æ–‡ä»¶")



                        logger.debug(f"[ä¸Šä¸‹æ–‡æ„å»º] @å¼•ç”¨ä¸Šä¸‹æ–‡: {at_context}")



                else:

                    # å…¶ä»–ä¿¡æ¯ç±»å‹

                    logger.debug(f"[ä¸Šä¸‹æ–‡æ„å»º] æœªçŸ¥ä¿¡æ¯ç±»å‹: {info_type}")

                    pass



            # æ·»åŠ å½“å‰ä¼šè¯ä¿¡æ¯

            context_parts.append(f"å½“å‰ä¼šè¯ID: {session_id}")



            # æ·»åŠ æŸ¥è¯¢ç±»å‹ä¿¡æ¯

            context_parts.append(f"æŸ¥è¯¢ç±»å‹: {query_type}")



            # æ‹¼æ¥æ‰€æœ‰ä¸Šä¸‹æ–‡éƒ¨åˆ†

            context = "\n\n".join(context_parts)



            logger.debug(f"[ä¸Šä¸‹æ–‡æ„å»º] ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")

            return context



        except Exception as e:

            logger.error(f"[ä¸Šä¸‹æ–‡æ„å»º] æ„å»ºä¸Šä¸‹æ–‡å¤±è´¥: {e}")

            # è¿”å›åŸºæœ¬ä¸Šä¸‹æ–‡

            return f"å½“å‰ä¼šè¯ID: {session_id}"



    async def stream_chat(

        self,

        message: str,

        session_id: str = None,

        document_name: str = None,

        frontend_context: dict = None,

        rule_files: Optional[List[str]] = None,

        show_rag_hits: bool = False,

    ):

        """æµå¼èŠå¤© - å®æ—¶è¿”å›å“åº”å†…å®¹



        Args:

            message: ç”¨æˆ·æ¶ˆæ¯

            session_id: ä¼šè¯IDï¼ˆå¿…é¡»ç”±å‰ç«¯æä¾›ï¼‰

            document_name: å½“å‰æ‰“å¼€çš„æ–‡æ¡£åï¼ˆç”¨äºé”™è¯¯å¤„ç†ï¼‰

            frontend_context: åŠ¨æ€æ„ŸçŸ¥æ•°æ®ï¼ˆå…‰æ ‡ã€é€‰åŒºã€æ ¼å¼ç­‰ï¼Œæ¥è‡ªå‰ç«¯ï¼‰



        Yields:

            dict: åŒ…å«typeå’Œcontentçš„äº‹ä»¶

                - type: "thinking" | "tool_call" | "tool_result" | "content" | "done"

                - content: å¯¹åº”çš„å†…å®¹

        """

        self.tool_calls = []

        # Best-effort token usage accounting (for MacroBench maxCost and observability).

        # Not all providers populate this; missing data is acceptable.

        self._token_usage_acc = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}



        # Token usage accumulation helper: use self._accumulate_token_usage(obj)



        # Some legacy clients append a "åŠ¨æ€æ„ŸçŸ¥" block into the user message.

        # That harms RAG retrieval and bloats tokens; strip it and rely on structured frontend_context instead.

        normalized_message = (message or "").strip()

        if "ã€åŠ¨æ€æ„ŸçŸ¥ã€‘" in normalized_message:

            normalized_message = normalized_message.split("ã€åŠ¨æ€æ„ŸçŸ¥ã€‘", 1)[0].strip()

        # Also strip common separators.

        if "\n---" in normalized_message:

            normalized_message = normalized_message.split("\n---", 1)[0].strip()

        # Per-turn writeback bookkeeping (used by output guard / repair).
        # Keep these best-effort and never crash the main flow.
        try:
            self._turn_user_query = normalized_message
        except Exception:
            pass
        try:
            self._turn_rag_context = ""
        except Exception:
            pass
        try:
            self._turn_writeback_anchor = "cursor"
        except Exception:
            pass
        try:
            self._turn_writeback_delivery = ""
        except Exception:
            pass
        try:
            self._turn_writeback_repair_attempted = False
        except Exception:
            pass



        # é‡ç½®æ¨ç†å†…å®¹è·Ÿè¸ªå™¨

        self._last_reasoning_content = None



        # å¼ºåˆ¶è¦æ±‚æä¾›æœ‰æ•ˆçš„session_idï¼Œä¸å†å†…éƒ¨ç”Ÿæˆ

        if not session_id:

            raise ValueError(

                f"Session ID is required and must be provided by the frontend. Document name: {document_name}"

            )



        # åŠ¨æ€æ„ŸçŸ¥ï¼šè®°å½•å¹¶æ³¨å…¥å‰ç«¯ä¸Šä¸‹æ–‡ï¼ˆç»“æ„åŒ–æ•°æ®ï¼Œä¸æ±¡æŸ“ user messageï¼‰

        bench_mode = False

        if frontend_context:

            # Keep raw frontend context for tool execution fallbacks (e.g. remote read_document).
            # This is per-request; the agent instance is created per request in agentic_chat_api.
            try:
                self._turn_frontend_context = frontend_context if isinstance(frontend_context, dict) else None
            except Exception:
                self._turn_frontend_context = None

            try:

                logger.debug(

                    f"[åŠ¨æ€æ„ŸçŸ¥] æ”¶åˆ°å‰ç«¯ä¸Šä¸‹æ–‡: {frontend_context.get('activeDocument', {}) or frontend_context.get('document', {})}"

                )

            except Exception as e:

                logger.debug(f"[åŠ¨æ€æ„ŸçŸ¥] log frontend context failed: {e}", exc_info=True)

            try:

                bench_mode = bool(frontend_context.get("bench"))

            except Exception:

                bench_mode = False



        # Telemetry: per-turn run context + timers (best-effort; must not break chat).

        self._turn_started_at_perf = time.perf_counter()

        self._turn_llm_total_ms = 0

        self._turn_first_llm_ms = None

        try:

            self._turn_run_context = _extract_run_context(frontend_context, session_id, mode="chat")

        except Exception:

            self._turn_run_context = RunContext(session_id=str(session_id or "").strip(), mode="chat")

        try:

            tsvc = get_telemetry()

            if tsvc:

                tsvc.emit(

                    "chat.turn_start",

                    {

                        "message_len": len(normalized_message or ""),

                        "bench": bool(bench_mode),

                        "has_frontend_context": bool(frontend_context),

                        "has_rule_files": bool(rule_files),

                    },

                    ctx=getattr(self, "_turn_run_context", None),

                )

        except Exception as e:

            logger.debug(f"[telemetry] emit chat.turn_start failed: {e}", exc_info=True)



        # Project-scoped RAG: compute allowed sources for this turn (project + global).

        # This is a read-only filter; importing/linking is handled by the RAG ingestion API.

        self._rag_source_filter = None

        try:

            if self.vector_store and frontend_context:

                active = frontend_context.get("activeDocument") or {}

                doc_path = (active.get("fullPath") or active.get("path") or "").strip()

                ctx = derive_project_context(doc_path) if doc_path else None

                if ctx:

                    persist_dir = getattr(self.vector_store, "persist_path", None)

                    if persist_dir:

                        rag_index = get_project_rag_index(persist_dir)

                        # First-run bootstrap: keep legacy behavior by treating all existing sources as global.

                        try:

                            if not rag_index.index_path.exists() and hasattr(self.vector_store, "get_all_metadatas"):

                                metas = self.vector_store.get_all_metadatas() or []

                                sources = sorted({(m or {}).get("source") for m in metas if (m or {}).get("source")})

                                if sources:

                                    rag_index.bootstrap_all_as_global(sources)

                                    rag_index.save()

                        except Exception as e:

                            logger.debug(f"[RAG] project index bootstrap failed: {e}", exc_info=True)

                        # If the index hasn't been initialized yet, don't enforce a filter (keeps legacy behavior).

                        allowed = rag_index.get_allowed_sources(ctx.project_id, include_global=True)

                        if allowed:

                            self._rag_source_filter = {"source": {"$in": allowed[:5000]}}

        except Exception:

            self._rag_source_filter = None



        # 1. æ£€ç´¢ç›¸å…³è®°å¿†

        relevant_memory = await self.memory_system.get_comprehensive_context(normalized_message, session_id)



        # 1.5. ä½¿ç”¨LLMé©±åŠ¨ç­–ç•¥æ™ºèƒ½åˆ†ç±»ä¼šè¯å†…å®¹

        classification_result = None

        try:

            from ah32.strategies.llm_driven_strategy import classify_conversation



            # ç”Ÿæˆæ¶ˆæ¯å“ˆå¸Œå¹¶æ£€æŸ¥ç¼“å­˜

            message_hash = self._get_message_hash(normalized_message)



            if message_hash in self._classification_cache:

                # ç¼“å­˜å‘½ä¸­

                classification_result = self._classification_cache[message_hash]

                logger.debug(f"[LLMåˆ†ç±»] æµå¼èŠå¤©ç¼“å­˜å‘½ä¸­ï¼Œä½¿ç”¨ä¹‹å‰çš„åˆ†ç±»ç»“æœ")

            else:

                # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨ç®€åŒ–å…³é”®è¯åˆ†ç±»

                classification_result = classify_conversation(normalized_message)

                self._classification_cache[message_hash] = classification_result

                logger.info(

                    f"[LLMåˆ†ç±»] æµå¼èŠå¤©åˆ†ç±»å®Œæˆï¼Œç±»åˆ«: {classification_result.category.value}, ç½®ä¿¡åº¦: {classification_result.confidence.value}"

                )

        except Exception as e:

            logger.warning(f"[LLMåˆ†ç±»] æµå¼èŠå¤©åˆ†ç±»å¤±è´¥: {e}")



        # 2. æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡

        context = self._build_context(relevant_memory, session_id, normalized_message, classification_result)



        # Inject structured frontend_context for this turn.

        if frontend_context:

            try:

                import json as _json

                # Privacy: never dump active document text into the prompt.
                safe_frontend_context = frontend_context
                try:
                    if isinstance(frontend_context, dict):
                        safe_frontend_context = dict(frontend_context)
                        for k in (
                            "active_doc_text",
                            "activeDocText",
                            "active_document_text",
                            "activeDocumentText",
                            "active_doc_text_full",
                            "activeDocTextFull",
                            "active_document_text_full",
                            "activeDocumentTextFull",
                            "active_et_meta",
                            "activeEtMeta",
                            "active_et_header_map",
                            "activeEtHeaderMap",
                        ):
                            safe_frontend_context.pop(k, None)
                except Exception:
                    safe_frontend_context = frontend_context

                compact = _json.dumps(safe_frontend_context, ensure_ascii=False)

                if len(compact) > 1500:

                    compact = compact[:1500] + "...(truncated)"

                context = f"{context}\n\nã€åŠ¨æ€æ„ŸçŸ¥(å‰ç«¯)ã€‘\n{compact}"

            except Exception as e:

                logger.debug(f"[åŠ¨æ€æ„ŸçŸ¥] inject compact frontend context failed: {e}", exc_info=True)



        skills_used_payload = None

        selected_skills = []

        self._selected_skills = []

        skills_injected_chars = 0

        skills_truncated = False

        lazy_stats_before = None

        lazy_stats_after = None

        lazy_calls_delta = 0

        lazy_cache_hits_delta = 0

        lazy_total_ms_delta = 0.0

        lazy_activated_skills: List[str] = []



        # Inject hot-loaded skills (prompt-only) for this turn.

        try:

            from ah32.config import settings



            if settings.skills_enabled and settings.enable_dynamic_skills and self.skills_registry is not None:

                # Skill routing should be stable across follow-up turns that omit domain keywords

                # (e.g. "å†ç»†ä¸€ç‚¹"). Use lightweight hints from the active document + recent context

                # ONLY for routing, not for LLM prompting.

                host = None

                try:

                    if isinstance(frontend_context, dict):

                        host = (

                            frontend_context.get("host_app")

                            or frontend_context.get("hostApp")

                            or (frontend_context.get("activeDocument") or {}).get("hostApp")

                        )

                except Exception:

                    host = None

                # Auto-select relevant skills for this turn to keep prompts small and fast.

                try:

                    from ah32.knowledge.embeddings import resolve_embedding

                    from ah32.skills.router import SkillRouter, SkillRoutingHints



                    embedder = resolve_embedding(settings)

                    router = SkillRouter(self.skills_registry)



                    routing_doc_name = ""

                    routing_extra: list[str] = []

                    routing_recent_user: tuple[str, ...] = ()

                    try:

                        if document_name:

                            routing_doc_name = str(document_name).strip()

                    except Exception:

                        routing_doc_name = ""



                    try:

                        if isinstance(frontend_context, dict):

                            active = frontend_context.get("activeDocument") or frontend_context.get("document") or {}

                            if isinstance(active, dict):

                                dn = str(active.get("name") or active.get("document_name") or "").strip()

                                dp = str(active.get("fullPath") or active.get("path") or "").strip()

                                if dn and (not routing_doc_name or dn != routing_doc_name):

                                    routing_extra.append(f"active_doc_name: {dn}")

                                if dp:

                                    # Path is used only as a lexical hint (it often contains business keywords).

                                    routing_extra.append(f"active_doc_path: {dp}")

                    except Exception:

                        routing_extra = []



                    # If the user message is short/generic, append a tiny slice of session memory for routing.

                    # This keeps skill selection "sticky" to the current topic without hard-coded fallbacks.

                    try:

                        msg0 = (normalized_message or "").strip()

                        if len(msg0) < 24 and isinstance(relevant_memory, dict):

                            mems = relevant_memory.get("session_memory")

                            if isinstance(mems, list) and mems:

                                recent: list[str] = []

                                for m in mems[:6]:

                                    if not isinstance(m, dict):

                                        continue

                                    u = (

                                        m.get("user_message")

                                        or m.get("user")

                                        or m.get("query")

                                        or m.get("content")

                                        or ""

                                    )

                                    u = str(u or "").strip()

                                    if not u:

                                        continue

                                    recent.append(u)

                                    if len(recent) >= 2:

                                        break

                                if recent:

                                    routing_recent_user = tuple(recent)

                    except Exception:

                        routing_recent_user = ()



                    ranked = router.select_for_message(

                        normalized_message,

                        embedder=embedder,

                        hints=SkillRoutingHints(

                            host=str(host).strip() if host else None,

                            document_name=routing_doc_name,

                            recent_user_messages=routing_recent_user,

                            extra=tuple(routing_extra),

                        ),

                        host=host,

                        top_k=settings.skills_top_k,

                        min_score=settings.skills_min_score,

                    )

                    selected_skills = [x.get("skill") for x in ranked if x.get("skill")]

                    skills_used_payload = [

                        {

                            "id": x["skill"].skill_id,

                            "name": x["skill"].name,

                            "version": x["skill"].version,

                            "priority": x["skill"].priority,

                            "score": round(float(x.get("score") or 0.0), 3),

                        }

                        for x in ranked

                        if x.get("skill") is not None

                    ]

                except Exception:

                    # Embeddings may be unavailable (offline bootstrap). Fall back to lexical routing.

                    from ah32.skills.router import SkillRouter, SkillRoutingHints



                    router = SkillRouter(self.skills_registry)

                    ranked = router.select_for_message(

                        normalized_message,

                        embedder=None,

                        hints=SkillRoutingHints(host=str(host).strip() if host else None),

                        host=host,

                        top_k=settings.skills_top_k,

                        min_score=settings.skills_min_score,

                    )

                    selected_skills = [x.get("skill") for x in ranked if x.get("skill")]

                    skills_used_payload = [

                        {

                            "id": x["skill"].skill_id,

                            "name": x["skill"].name,

                            "version": x["skill"].version,

                            "priority": x["skill"].priority,

                            "score": round(float(x.get("score") or 0.0), 3),

                        }

                        for x in ranked

                        if x.get("skill") is not None

                    ]



                # Skill-driven attachment: some skills require reading the active document text.

                # Best-effort and bounded; if extraction fails we just proceed without it.

                try:

                    # Cache active doc text for skill tools (auto-injected).
                    #
                    # IMPORTANT (privacy + token budget):
                    # - Tools may need the full doc text to parse questions reliably.
                    # - The LLM prompt should only receive a bounded preview.
                    #
                    # `self._turn_active_doc_text` is kept per-request (agent is per request in agentic_chat_api).
                    self._turn_active_doc_text = ""

                    need_doc = [

                        s for s in (selected_skills or []) if getattr(s, "needs_active_doc_text", False)

                    ]

                    if frontend_context and need_doc:

                        preview_max_chars = 40_000

                        try:

                            cap_max = max(int(getattr(s, "active_doc_max_chars", 0) or 0) for s in need_doc)

                            if cap_max > 0:

                                preview_max_chars = min(200_000, cap_max)

                        except Exception as e:
                            logger.debug(
                                "[docx] compute active_doc_max_chars failed (ignored): %s",
                                e,
                                exc_info=True,
                            )

                        # Full text for tools (ephemeral; do NOT persist).
                        full_text = maybe_extract_active_doc_text_full(frontend_context)
                        if full_text:
                            self._turn_active_doc_text = str(full_text or "")

                        # Bounded preview for the LLM prompt.
                        preview_text = maybe_extract_active_docx(frontend_context, max_chars=preview_max_chars)
                        if not preview_text and full_text:
                            preview_text = str(full_text)[:preview_max_chars]
                            if len(str(full_text)) > preview_max_chars:
                                preview_text = preview_text + "\n...(truncated)"

                        if preview_text:
                            context = f"{context}\n\nã€å½“å‰æ–‡æ¡£å†…å®¹ã€‘\n{preview_text}"

                except Exception as e:

                    logger.debug(f"[docx] attach active doc text failed (ignored): {e}", exc_info=True)



                skills_text = self.skills_registry.render_for_prompt(selected=selected_skills)

                if skills_text:

                    skills_injected_chars = len(skills_text)

                    skills_truncated = "...truncated..." in skills_text or "[...truncated...]" in skills_text

                    context = f"{context}\n\n{skills_text}"

                # Render tools from selected skills (if any have tools)
                tools_text = ""
                if self.skills_registry is not None and hasattr(self.skills_registry, "get_lazy_activation_stats"):
                    try:
                        lazy_stats_before = self.skills_registry.get_lazy_activation_stats()
                    except Exception as e:
                        logger.debug(f"[skills] lazy stats(before) failed (ignored): {e}", exc_info=True)
                if selected_skills:
                    # Convert to tuple for the render method
                    selected_tuple = tuple(selected_skills) if not isinstance(selected_skills, tuple) else selected_skills
                    tools_text = self.skills_registry.render_tools_for_prompt(selected_tuple)
                if self.skills_registry is not None and hasattr(self.skills_registry, "get_lazy_activation_stats"):
                    try:
                        lazy_stats_after = self.skills_registry.get_lazy_activation_stats()
                    except Exception as e:
                        logger.debug(f"[skills] lazy stats(after) failed (ignored): {e}", exc_info=True)

                try:
                    if isinstance(lazy_stats_before, dict) and isinstance(lazy_stats_after, dict):
                        lazy_calls_delta = int(lazy_stats_after.get("calls", 0)) - int(lazy_stats_before.get("calls", 0))
                        lazy_cache_hits_delta = int(lazy_stats_after.get("cache_hits", 0)) - int(
                            lazy_stats_before.get("cache_hits", 0)
                        )
                        lazy_total_ms_delta = float(lazy_stats_after.get("total_ms", 0.0)) - float(
                            lazy_stats_before.get("total_ms", 0.0)
                        )
                        per_before = lazy_stats_before.get("per_skill") if isinstance(lazy_stats_before.get("per_skill"), dict) else {}
                        per_after = lazy_stats_after.get("per_skill") if isinstance(lazy_stats_after.get("per_skill"), dict) else {}
                        skill_ids = set(per_before.keys()) | set(per_after.keys())
                        activated: List[str] = []
                        for sid in sorted(skill_ids):
                            cb = int((per_before.get(sid) or {}).get("calls", 0))
                            ca = int((per_after.get(sid) or {}).get("calls", 0))
                            if ca > cb:
                                activated.append(sid)
                        lazy_activated_skills = activated
                except Exception as e:
                    logger.debug(f"[skills] lazy stats delta calc failed (ignored): {e}", exc_info=True)

                if tools_text:
                    context = f"{context}\n\n{tools_text}"

                if skills_used_payload is None:

                    skills_used_payload = []

                self._selected_skills = list(selected_skills or [])

        except Exception as e:

            logger.debug(f"[skills] inject skills failed (ignored): {e}", exc_info=True)



        # Telemetry: record routing decision (even if none matched).

        try:

            enabled_dynamic = True

            try:

                from ah32.config import settings as _settings



                enabled_dynamic = bool(getattr(_settings, "enable_dynamic_skills", True))

            except Exception:

                enabled_dynamic = True

            tsvc = get_telemetry()

            if tsvc:

                tsvc.emit(

                    "skills.selected",

                    {

                        "skills": list(skills_used_payload or []),

                        "injected_chars": int(skills_injected_chars or 0),

                        "truncated": bool(skills_truncated),

                        "enabled_dynamic": bool(enabled_dynamic),

                        "lazy_activation_calls": int(max(0, lazy_calls_delta)),

                        "lazy_activation_cache_hits": int(max(0, lazy_cache_hits_delta)),

                        "lazy_activation_ms": float(max(0.0, lazy_total_ms_delta)),

                        "lazy_activated_skills": list(lazy_activated_skills),

                    },

                    ctx=getattr(self, "_turn_run_context", None),

                )

        except Exception as e:

            logger.debug(f"[telemetry] emit skills.selected failed: {e}", exc_info=True)



        # Inject user-provided rule files (Claude.md-like), loaded each turn.

        # Rules should override Skills when conflicts arise, so inject them after Skills.

        try:

            from ah32.config import settings

            from ah32.services.conversation_rules import load_rule_files, render_rules_for_prompt

            from pathlib import Path



            rule_paths = (

                [Path(p) for p in (rule_files or []) if p]

                if rule_files

                else settings.get_conversation_rule_file_paths()

            )

            rules = load_rule_files(rule_paths, max_total_chars=settings.conversation_rule_files_max_chars)

            rules_text = render_rules_for_prompt(rules)

            if rules_text:

                context = f"{context}\n\n{rules_text}"

        except Exception as e:

            logger.debug(f"[rules] inject rule files failed (ignored): {e}", exc_info=True)



        # Inject RAG snippets (knowledge base) into the context for this turn.

        at_paths = []

        try:

            at_paths = (relevant_memory.get("at_reference_context") or {}).get("at_references_paths") or []

        except Exception:

            at_paths = []

        rag_context = self._build_rag_context(

            normalized_message, at_paths if isinstance(at_paths, list) else None

        )

        if rag_context:

            context = f"{context}\n\n{rag_context}"

        try:
            self._turn_rag_context = str(rag_context or "")
        except Exception:
            pass

        try:

            tsvc = get_telemetry()

            if tsvc:

                dbg = self._last_rag_debug

                if not isinstance(dbg, dict):

                    dbg = {

                        "triggered": bool(self.vector_store),

                        "raw": 0,

                        "kept": 0,

                        "injected_chars": len(rag_context or ""),

                        "truncated": False,

                    }

                tsvc.emit("rag.retrieve", dbg, ctx=getattr(self, "_turn_run_context", None))

        except Exception as e:

            logger.debug(f"[telemetry] emit rag.retrieve failed: {e}", exc_info=True)



        want_writeback = False

        writeback_anchor = "cursor"

        writeback_delivery = ""



        # If user intent is to write the result into the WPS document, instruct the LLM

        # to include an executable Plan JSON (the frontend will run it automatically).

        #

        # IMPORTANT: do NOT gate this on RAG. Many writeback tasks (tables/formatting/cover pages)

        # do not require retrieval, but still must produce runnable plans (MacroBench relies on this).

        host_app = ""

        try:

            host_app = str(getattr(getattr(self, "_turn_run_context", None), "host_app", "") or "").strip().lower()

        except Exception:

            host_app = ""

        is_office_host = host_app in ("wps", "writer", "et", "wpp")
        chat_only_turn = False
        try:
            chat_only_turn = self._is_chat_only_intent(normalized_message)
        except Exception as e:
            logger.debug(f"[writeback] chat-only detect failed (ignored): {e}", exc_info=True)
            chat_only_turn = False
        # Skill tools may declare "background" tasks (e.g. crawling/ingestion). When the user intent
        # matches those triggers, we should not go through the writeback Plan fast-path.
        try:
            if (not chat_only_turn) and is_office_host:
                if self._force_chat_only_by_skill_tool_hints(normalized_message, selected_skills or []):
                    chat_only_turn = True
        except Exception as e:
            logger.debug(f"[writeback] skill tool chat-only override failed (ignored): {e}", exc_info=True)

        explicit_writeback_intent = False
        try:
            explicit_writeback_intent = self._is_writeback_intent(normalized_message, context=context)
        except Exception as e:
            logger.debug("[writeback] explicit intent detect failed (ignored): %s", e, exc_info=True)
            explicit_writeback_intent = False

        skill_default_writeback = ""
        try:
            skill_default_writeback = str(self._selected_default_writeback_delivery(selected_skills or []) or "").strip()
        except Exception as e:
            logger.debug("[writeback] skill default delivery detect failed (ignored): %s", e, exc_info=True)
            skill_default_writeback = ""

        # Routing rule (project invariant):
        # - "Writeback" means: frontend JS will execute a Plan JSON to mutate the WPS document.
        # - Default is CHAT (no writeback). Only write back when:
        #   1) user explicitly asks to write into the document, OR
        #   2) the selected skill declares a default writeback delivery.
        should_writeback = False
        if is_office_host:
            should_writeback = (not chat_only_turn) and (explicit_writeback_intent or bool(skill_default_writeback))
        else:
            # Non-office hosts: never default-writeback; require explicit intent.
            should_writeback = (not chat_only_turn) and bool(explicit_writeback_intent)

        if should_writeback:

            try:

                want_writeback = True

                writeback_anchor = self._writeback_anchor(normalized_message)



                # Skill-driven delivery mode: some scenarios prefer "compare table first" instead of direct rewrite.

                delivery = ""

                try:

                    delivery = skill_default_writeback or self._selected_default_writeback_delivery(selected_skills or [])

                except Exception:

                    delivery = ""



                # Safety default: review compare tables should go to document end unless user explicitly says "å…‰æ ‡/å½“å‰ä½ç½®".

                if delivery == "compare_table" and self._is_compare_table_delivery_intent(normalized_message):

                    writeback_delivery = "compare_table"

                    try:

                        if not re.search(r"(å…‰æ ‡|å½“å‰ä½ç½®|å½“å‰å…‰æ ‡|æ­¤å¤„|è¿™é‡Œ)", normalized_message):

                            writeback_anchor = "end"

                    except Exception:

                        writeback_anchor = "end"

                    context = f"{context}\n\n{self._build_compare_table_writeback_directive(normalized_message)}"

                else:

                    writeback_delivery = ""

                    context = f"{context}\n\n{self._build_writeback_directive(normalized_message)}"

            except Exception as e:

                logger.debug(f"[writeback] build writeback directive failed (ignored): {e}", exc_info=True)

        logger.debug(
            "[writeback] decision host_app=%r office=%s chat_only=%s explicit=%s skill_default=%s want=%s anchor=%s delivery=%s",
            host_app,
            is_office_host,
            chat_only_turn,
            explicit_writeback_intent,
            skill_default_writeback,
            want_writeback,
            writeback_anchor,
            writeback_delivery,
        )

        # Persist per-turn decision so downstream post-processing can reliably decide whether
        # to keep/strip executable fenced blocks from model output.
        self._turn_want_writeback = bool(want_writeback)
        try:
            self._turn_writeback_anchor = str(writeback_anchor or "cursor")
        except Exception:
            pass
        try:
            self._turn_writeback_delivery = str(writeback_delivery or "")
        except Exception:
            pass

        if show_rag_hits:

            try:

                dbg = self._last_rag_debug or {}

                kept = int(dbg.get("kept") or 0)

                sources = dbg.get("sources") or []

                preview = ", ".join([str(s.get("source")) for s in sources[:3] if isinstance(s, dict)])

                yield {

                    "type": "rag",

                    "content": f"RAGå‘½ä¸­: {kept}ï¼Œsources: {preview}" if preview else f"RAGå‘½ä¸­: {kept}",

                    "session_id": session_id,

                }

            except Exception as e:

                logger.debug(f"[RAG] emit rag debug event failed: {e}", exc_info=True)



        # Skill hint (early): show which skills were selected for prompt injection for this turn.

        # This makes the UI immediately reflect the routing decision, even before the model responds.

        if skills_used_payload is not None and self.skills_registry is not None:

            try:

                selected_payload = list(skills_used_payload or [])[:4]

                names = [s.get("name") for s in selected_payload if isinstance(s, dict) and s.get("name")]

                if names:

                    yield {

                        "type": "skills",

                        "content": "å·²é€‰æ‹©æŠ€èƒ½: " + "ã€".join(names[:4]),

                        "skills": selected_payload,

                        "skills_kind": "selected",

                        "skills_phase": "selected",

                        "skills_metrics": {
                            "lazy_activation_calls": int(max(0, lazy_calls_delta)),
                            "lazy_activation_cache_hits": int(max(0, lazy_cache_hits_delta)),
                            "lazy_activation_ms": float(max(0.0, lazy_total_ms_delta)),
                            "lazy_activated_skills": list(lazy_activated_skills),
                        },

                        "session_id": session_id,

                    }

                else:

                    yield {

                        "type": "skills",

                        "content": "æœªå‘½ä¸­åœºæ™¯æŠ€èƒ½",

                        "skills": [],

                        "skills_kind": "none",

                        "skills_phase": "selected",

                        "skills_metrics": {
                            "lazy_activation_calls": int(max(0, lazy_calls_delta)),
                            "lazy_activation_cache_hits": int(max(0, lazy_cache_hits_delta)),
                            "lazy_activation_ms": float(max(0.0, lazy_total_ms_delta)),
                            "lazy_activated_skills": list(lazy_activated_skills),
                        },

                        "session_id": session_id,

                    }

            except Exception as e:

                logger.debug(f"[skills] emit selected skills event failed: {e}", exc_info=True)



        # 3. å‘é€å¼€å§‹äº‹ä»¶

        yield {"type": "start", "content": "", "session_id": session_id}



        if want_writeback:
            # Fast path: writeback intents should produce runnable plans deterministically.
            # Falling back to the full ReAct loop is still allowed (for compatibility),
            # but we prefer a dedicated plan prompt + faster model to reduce repair loops.
            try:
                yield {"type": "thinking", "content": "æ­£åœ¨ç”Ÿæˆå¯æ‰§è¡Œ Plan...", "session_id": session_id}
                plan = await self._generate_writeback_plan_fast_path(
                    user_query=normalized_message,
                    context=context,
                    session_id=session_id,
                    document_name=document_name,
                    frontend_context=frontend_context,
                    anchor=writeback_anchor,
                    delivery=writeback_delivery,
                )
                if plan:
                    plan_text = json.dumps(plan, ensure_ascii=False, default=str)
                    resp_text = f"```json\n{plan_text}\n```"

                    processed = await self._send_response_with_code_check(resp_text, session_id)
                    final_to_store = (processed or resp_text).strip()
                    self._last_response = final_to_store
                    yield {"type": "content", "content": final_to_store, "session_id": session_id}
                else:
                    async for event in self._stream_react_loop(normalized_message, context, session_id):
                        yield event
            except Exception as e:
                logger.error(f"[writeback] plan fast path failed, fallback to ReAct loop: {e}", exc_info=True)
                async for event in self._stream_react_loop(normalized_message, context, session_id):
                    yield event
        else:

            # 6. æ‰§è¡ŒReActå¾ªç¯ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰

            async for event in self._stream_react_loop(normalized_message, context, session_id):

                yield event



        # 7. å­˜å‚¨å¯¹è¯åˆ°è®°å¿†

        final_response = getattr(self, "_last_response", "")

        

        # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºå®Œæ•´å“åº”å†…å®¹

        logger.debug(f"[è®°å¿†å­˜å‚¨] å¼€å§‹å­˜å‚¨å¯¹è¯")

        logger.debug(f"[è®°å¿†å­˜å‚¨] user_message: {message[:100]}...")

        logger.debug(f"[è®°å¿†å­˜å‚¨] final_response: '{final_response[:200] if final_response else '(empty)'}...'")

        logger.debug(f"[è®°å¿†å­˜å‚¨] _last_responseå±æ€§å€¼: '{getattr(self, '_last_response', 'NOT_SET')}'")

        logger.debug(f"[è®°å¿†å­˜å‚¨] session_id: {session_id}")



        # Writeback guard: if we instructed the model to output a Plan but it did not,
        # retry once and/or surface an explicit hint (do not pretend success).
        already_repaired = False
        try:
            already_repaired = bool(getattr(self, "_turn_writeback_repair_attempted", False))
        except Exception:
            already_repaired = False
        if want_writeback and (not already_repaired):
            try:
                host = host_app if host_app in ("wps", "et", "wpp") else "wps"
                plan_obj, plan_err = self._extract_plan_from_text(final_response or "", host)
                if (not final_response) or (not self._response_has_strict_plan_block(final_response, host)):
                    repaired = await self._auto_repair_missing_writeback_plan(
                        user_query=normalized_message,
                        rag_context=rag_context or "",
                        session_id=session_id,
                        anchor=writeback_anchor,
                        delivery=writeback_delivery,
                        original_plan=plan_obj,
                        error_type="missing_plan" if (not plan_err or plan_err == "empty_plan_payload") else "invalid_plan",
                        error_message=plan_err or "",
                    )
                    if repaired:
                        extra = f"\n\n```json\n{json.dumps(repaired, ensure_ascii=False, default=str)}\n```"
                        final_response = (final_response or "") + extra
                        self._last_response = final_response
                        yield {"type": "content", "content": extra, "session_id": session_id}
                    else:
                        # Do not pretend success; tell the user clearly what went wrong.
                        msg = (
                            "\n\nã€ç³»ç»Ÿæç¤ºã€‘æœ¬è½®éœ€è¦å†™å…¥æ–‡æ¡£ï¼Œä½†æœªç”Ÿæˆå¯æ‰§è¡Œçš„ Plan JSONï¼ˆæ¨¡å‹å¯èƒ½è¾“å‡ºäº† HTML/çº¯æ–‡æœ¬ï¼‰ã€‚"
                            "è¯·ç‚¹å‡»é‡è¯•ï¼Œæˆ–åœ¨æé—®ä¸­æ˜ç¡®â€œåªè¾“å‡º Plan JSON ä»£ç å—â€ã€‚"
                        )
                        final_response = (final_response or "") + msg
                        self._last_response = final_response
                        yield {"type": "content", "content": msg, "session_id": session_id}
            except Exception as e:
                logger.error(f"[writeback] auto-repair missing plan failed: {e}", exc_info=True)
        # NOTE: Persisting to vector DB can be slow (embeddings + Chroma upsert).

        # Don't block the SSE 'done' event on this, otherwise the frontend will

        # appear "stuck" even after content is already rendered.

        if final_response:

            import asyncio



            async def _persist():

                try:

                    meta = {

                        # NOTE: tool_calls can include verbose implementation details.

                        # For bench runs, keep them out of long-lived memories to avoid polluting future sessions.

                        "tool_calls": [] if bench_mode else [call.to_dict() for call in self.tool_calls],

                        "streaming": True,

                        "bench": bench_mode,

                    }

                    await self.memory_system.store_conversation(

                        session_id=session_id,

                        user_message=normalized_message,

                        assistant_response=final_response,

                        metadata=meta,

                    )

                except Exception as e:

                    logger.error(f"[è®°å¿†å­˜å‚¨] å¼‚æ­¥æŒä¹…åŒ–å¤±è´¥: {e}", exc_info=True)



            asyncio.create_task(_persist())

        else:

            logger.warning(f"[è®°å¿†å­˜å‚¨] æœªæ‰¾åˆ°æœ€ç»ˆå“åº”ï¼Œè·³è¿‡è®°å¿†å­˜å‚¨ï¼Œsession_id: {session_id}")



        # Post-hoc skill hint: show skills that appear to be applied in the final output (heuristic).

        # This makes the UI display more accurate than "selected for prompt injection".

        if skills_used_payload is not None and self.skills_registry is not None:

            try:

                applied = self.skills_registry.detect_applied_skills(

                    final_response,

                    selected=selected_skills or None,

                )

                applied_ids = {x["skill"].skill_id for x in (applied or []) if isinstance(x, dict) and x.get("skill")}

                applied_payload = [s for s in (skills_used_payload or []) if isinstance(s, dict) and s.get("id") in applied_ids]

                try:

                    tsvc = get_telemetry()

                    if tsvc:

                        tsvc.emit(

                            "skills.applied",

                            {

                                "applied_ids": sorted(list(applied_ids)),

                                "selected_ids": sorted(

                                    [str(s.get("id")) for s in (skills_used_payload or []) if isinstance(s, dict) and s.get("id")]

                                ),

                            },

                            ctx=getattr(self, "_turn_run_context", None),

                        )

                except Exception as e:

                    logger.debug(f"[telemetry] emit skills.applied failed: {e}", exc_info=True)



                # Skill hint (late): after the response is produced, report whether the output actually

                # applied a skill template/structure. If not, still echo the selected skills so the UI

                # doesn't look "wrong" for topic-focused chats.

                selected_payload = list(skills_used_payload or [])[:4]



                if applied_payload:

                    payload = applied_payload

                    names = [s.get("name") for s in payload if isinstance(s, dict) and s.get("name")]

                    msg = ("å‘½ä¸­æŠ€èƒ½: " + "ã€".join(names[:4])) if names else "æœªå‘½ä¸­åœºæ™¯æŠ€èƒ½"

                    kind = "applied"

                elif selected_payload:

                    # "å®äº‹æ±‚æ˜¯": do not pretend the skill was applied. Report selection in hint only.

                    names = [s.get("name") for s in selected_payload if isinstance(s, dict) and s.get("name")]

                    msg = ("è¾“å‡ºæœªæ˜¾å¼åº”ç”¨æŠ€èƒ½æ¨¡æ¿ï¼ˆå·²é€‰æ‹©: " + "ã€".join(names[:4]) + "ï¼‰") if names else "æœªå‘½ä¸­åœºæ™¯æŠ€èƒ½"

                    kind = "applied_none"

                    payload = []

                else:

                    payload = []

                    msg = "æœªå‘½ä¸­åœºæ™¯æŠ€èƒ½"

                    kind = "none"



                yield {

                    "type": "skills",

                    "content": msg,

                    "skills": payload,

                    "skills_kind": kind,

                    "skills_phase": "applied",

                    "session_id": session_id,

                }

            except Exception as e:

                logger.debug(f"[skills] emit applied skills event failed: {e}", exc_info=True)



        # æ¸…ç†æ¨ç†å†…å®¹è·Ÿè¸ªå™¨

        self._last_reasoning_content = None



        done_payload = {"type": "done", "content": "", "session_id": session_id}

        # Frontend may auto-enqueue writeback on `done`. Provide an explicit, authoritative
        # server-side decision to avoid spurious "å†™å›å¤±è´¥" on pure chat turns (e.g. "ä½ å¥½").
        try:
            done_payload["want_writeback"] = bool(getattr(self, "_turn_want_writeback", False))
        except Exception as e:
            logger.debug("[output-guard] attach want_writeback failed (ignored): %s", e, exc_info=True)

        try:

            acc = getattr(self, "_token_usage_acc", None)

            if isinstance(acc, dict):

                done_payload["token_usage"] = dict(acc)

        except Exception as e:

            logger.debug(f"[token] attach token_usage failed (ignored): {e}", exc_info=True)

        try:

            tsvc = get_telemetry()

            if tsvc:

                t0 = getattr(self, "_turn_started_at_perf", None)

                chat_ms = int((time.perf_counter() - float(t0)) * 1000) if t0 else 0

                tsvc.emit(

                    "chat.turn_done",

                    {

                        "ok": True,

                        "chat_ms": chat_ms,

                        "first_token_ms": getattr(self, "_turn_first_llm_ms", None),

                        "llm_total_ms": getattr(self, "_turn_llm_total_ms", 0),

                        "token_usage": done_payload.get("token_usage"),

                    },

                    ctx=getattr(self, "_turn_run_context", None),

                )

        except Exception as e:

            logger.debug(f"[telemetry] emit chat.turn_done failed: {e}", exc_info=True)

        yield done_payload



    async def _stream_react_loop(self, message: str, context: str, session_id: str):

        """æµå¼ReActå¾ªç¯



        Yields:

            dict: äº‹ä»¶æµ

        """

        max_iterations = 5

        iteration = 0



        # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯å†å²ï¼ŒåŒ…æ‹¬ä¹‹å‰çš„å¯¹è¯å†å²

        logger.debug(f"[ä¸Šä¸‹æ–‡å†…å®¹] ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")

        if "å¯¹è¯å†å²" in context:

            logger.debug(f"[ä¸Šä¸‹æ–‡å†…å®¹] åŒ…å«å¯¹è¯å†å²")

        logger.debug(f"[ä¸Šä¸‹æ–‡å†…å®¹] å‰200å­—ç¬¦: {context[:200]}...")



        # è·å–JSå®å†å²é”™è¯¯è­¦å‘Šï¼ˆç”¨äºç”Ÿæˆé˜¶æ®µï¼‰
        # TODO: æ¥å…¥æŒä¹…åŒ–çš„å®é”™è¯¯è®°å½•åå†å¡«å……è¿™é‡Œã€‚
        js_macro_history_warning = ""

        # æ„å»ºæœ€ç»ˆçš„ç³»ç»Ÿæç¤ºï¼ˆåŒ…å«å†å²é”™è¯¯è­¦å‘Šï¼‰
        final_system_prompt = self.system_prompt
        if js_macro_history_warning:
            insertion = "\n\n" + js_macro_history_warning.strip()
            # å¦‚æœç³»ç»Ÿæç¤ºä»åŒ…å« {tools} å ä½ç¬¦ï¼Œåˆ™æŠŠè­¦å‘Šæ‹¼æ¥åˆ°å·¥å…·åˆ—è¡¨åï¼Œ
            # ä»¥ä¾¿åç»­å¡«å……å·¥å…·åˆ—è¡¨æ—¶ä¸€èµ·ç”Ÿæ•ˆã€‚
            if "{tools}" in final_system_prompt:
                final_system_prompt = final_system_prompt.replace("{tools}", "{tools}" + insertion)
            else:
                final_system_prompt += insertion
        # If the user explicitly signals "go ahead", treat it as authorization to

        # proceed with the previously suggested next step, instead of asking again.

        # This reduces manual confirmation loops in a dev/testing environment.

        auto_exec_keywords = (

            "æ¥å—å»ºè®®",

            "ç»§ç»­",

            "ç»§ç»­æ‰§è¡Œ",

            "å¼€å§‹æ‰§è¡Œ",

            "æŒ‰ä½ æ¨è",

            "å°±æŒ‰ä½ è¯´çš„",

            "åŒæ„",

            "ç›´æ¥æ‰§è¡Œ",

            "ä¸ç”¨ç¡®è®¤",

            "æ‰§è¡Œå§",

            "å°±è¿™ä¹ˆåŠ",

            "OK",

            "ok",

        )

        auto_exec_numeric_choice = (message or "").strip() in ("1", "2", "3")

        if any(k in (message or "") for k in auto_exec_keywords) or auto_exec_numeric_choice:

            final_system_prompt += (

                "\n\nã€è‡ªåŠ¨æ‰§è¡Œæ¨¡å¼ã€‘ç”¨æˆ·å·²æ˜ç¡®æˆæƒä½ ç›´æ¥æ‰§è¡Œä½ å»ºè®®çš„ä¸‹ä¸€æ­¥ã€‚"

                "é™¤éä½ æ— æ³•ç¡®å®šç›®æ ‡æ–‡æ¡£/å†™å…¥ä½ç½®æˆ–å­˜åœ¨æ˜æ˜¾ç ´åæ€§æ“ä½œï¼Œå¦åˆ™ä¸è¦åé—®è®©ç”¨æˆ·äºŒé€‰ä¸€ï¼›"

                "è¯·ç›´æ¥äº§å‡ºå¯æ‰§è¡Œçš„äº¤ä»˜ç‰©å¹¶å®Œæˆè¯¥æ­¥ï¼šä¼˜å…ˆè¾“å‡ºå¯æ‰§è¡Œ Plan JSONï¼›ä»…å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚â€œJSå®/JavaScriptâ€æ—¶æ‰è¾“å‡º JS å®ã€‚"

            )


        # Per-turn output mode guard:
        # - When we do NOT intend to write back, explicitly forbid any fenced code blocks.
        #   This prevents Skills (or stale chat history) from accidentally emitting JS/Plan payloads,
        #   which would trigger frontend auto-writeback and surface "å†™å›å¤±è´¥" for harmless chat like "ä½ å¥½".
        # - When we DO intend to write back, require a single Plan JSON block (no JS macros).
        # NOTE: _stream_react_loop doesn't compute writeback intent itself; stream_chat sets
        # `self._turn_want_writeback` after routing/intent detection.
        want_writeback = bool(getattr(self, "_turn_want_writeback", False))

        turn_directive = (
            "ã€æœ¬è½®è¾“å‡ºæ¨¡å¼ã€‘æœ¬è½®ä¸éœ€è¦å†™å›æ–‡æ¡£ï¼šåªç”¨è‡ªç„¶è¯­è¨€å›ç­”ï¼ˆç®€æ´ä¼˜å…ˆï¼‰ï¼›ä¸è¦è¾“å‡ºä»»ä½• fenced ä»£ç å—ï¼ˆä¸è¦ ```json``` / ```javascript```ï¼‰ï¼›ä¸è¦ç²˜è´´/å¤è¿°æ–‡æ¡£åŸæ–‡ï¼ˆå¦‚éœ€å¼•ç”¨ï¼Œæœ€å¤š1å¥ä¸”<=50å­—ï¼‰ã€‚"
            if not want_writeback
            else f"ã€æœ¬è½®è¾“å‡ºæ¨¡å¼ã€‘æœ¬è½®éœ€è¦å†™å›æ–‡æ¡£ï¼šè¯·åªè¾“å‡º 1 ä¸ª ```json``` ä»£ç å—ï¼ˆä»… JSONï¼‰ï¼Œschema_version å¿…é¡»æ˜¯ {PLAN_SCHEMA_ID}ï¼›ä¸è¦è¾“å‡º JS å®ã€‚"
        )

        # IMPORTANT: Put dynamic context (skills/rules/RAG hints) *before* the core system prompt,
        # and put the turn directive last so it remains authoritative.
        messages = [("system", context), ("system", final_system_prompt), ("system", turn_directive), ("user", message)]



        while iteration < max_iterations:

            iteration += 1



            # å‘é€æ€è€ƒä¸­äº‹ä»¶

            yield {

                "type": "thinking",

                "content": f"æ­£åœ¨åˆ†æ... (æ­¥éª¤ {iteration})",

                "session_id": session_id,

            }



            # é‡ç½®æ¨ç†å†…å®¹è·Ÿè¸ªå™¨ï¼ˆæ¯ä¸ªè¿­ä»£æ­¥éª¤ç‹¬ç«‹ï¼‰

            self._last_reasoning_content = None



            # è°ƒç”¨LLMè·å–æ€è€ƒå’Œè¡ŒåŠ¨

            llm_start_time = time.time()

            logger.info(f"[LLMç›‘æ§] ç¬¬ {iteration} æ­¥LLMè°ƒç”¨å¼€å§‹æ—¶é—´: {datetime.fromtimestamp(llm_start_time).strftime('%Y-%m-%d %H:%M:%S')}")



            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ asyncio.shield() ä¿æŠ¤ LLM è°ƒç”¨ä¸è¢«å–æ¶ˆ

            import asyncio

            response = await asyncio.shield(self.llm.ainvoke(messages))

            self._accumulate_token_usage(response)



            llm_end_time = time.time()

            llm_duration = llm_end_time - llm_start_time

            logger.info(f"[LLMç›‘æ§] ç¬¬ {iteration} æ­¥LLMè°ƒç”¨ç»“æŸæ—¶é—´: {datetime.fromtimestamp(llm_end_time).strftime('%Y-%m-%d %H:%M:%S')}")

            logger.info(f"[LLMç›‘æ§] ç¬¬ {iteration} æ­¥LLMå“åº”æ—¶é—´: {llm_duration:.2f}ç§’")

            try:

                ms = int(max(0.0, float(llm_duration)) * 1000)

                self._turn_llm_total_ms = int(getattr(self, "_turn_llm_total_ms", 0) or 0) + ms

                if iteration == 1 and not getattr(self, "_turn_first_llm_ms", None):

                    self._turn_first_llm_ms = ms

                tsvc = get_telemetry()

                if tsvc:

                    tsvc.emit(

                        "chat.llm_call",

                        {"iteration": iteration, "llm_ms": ms},

                        ctx=getattr(self, "_turn_run_context", None),

                    )

            except Exception as e:

                logger.debug(f"[telemetry] emit chat.llm_call failed: {e}", exc_info=True)



            # å¤„ç†DeepSeekæ€è€ƒæ¨¡å¼çš„reasoning_content

            reasoning = None

            if hasattr(response, "additional_kwargs") and response.additional_kwargs:

                reasoning = response.additional_kwargs.get("reasoning_content")



            if reasoning:

                logger.debug(f"[æ¨ç†] æ­¥éª¤ {iteration} - reasoning_content: {reasoning[:200]}...")

                yield {"type": "reasoning", "content": reasoning, "session_id": session_id}

            else:

                logger.debug(f"[æ¨ç†] æ­¥éª¤ {iteration} - æœªè·å–åˆ° reasoning_content")



            tool_call = self._parse_tool_call(response.content)



            if tool_call:

                tool_name = str(tool_call.get("action") or tool_call.get("name") or "unknown")

                tool_input = tool_call.get("input") if "action" in tool_call else tool_call.get("arguments", {})



                # è®°å½•å·¥å…·è°ƒç”¨ï¼ˆä¸å‘é€ç»™ç”¨æˆ·ï¼‰

                logger.debug(f"=== å·¥å…·è°ƒç”¨ ===")

                logger.debug(f"å·¥å…·åç§°: {tool_name}")

                logger.debug(f"è¾“å…¥å‚æ•°: {tool_input}")

                # ä¸yield tool_calläº‹ä»¶ï¼Œé¿å…æš´éœ²å†…éƒ¨å®ç°



                # æ‰§è¡Œå·¥å…·

                result = await self._execute_tool(tool_call)



                # è®°å½•å·¥å…·è°ƒç”¨

                call = ToolCall(tool_name, tool_call, result)

                self.tool_calls.append(call)



                # å°†ç»“æœè½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼ˆä¸æš´éœ²å·¥å…·åç§°å’ŒJSONæ ¼å¼ï¼‰

                # è¿™é‡Œéœ€è¦æ ¹æ®å·¥å…·ç±»å‹è½¬æ¢

                if tool_name == "import_documents":

                    # å¯¼å…¥å·¥å…·çš„ç»“æœï¼Œç›´æ¥ä½œä¸ºç³»ç»Ÿæç¤º

                    pass  # å·¥å…·ç»“æœä¼šåœ¨æœ€ç»ˆå“åº”ä¸­å¤„ç†

                elif tool_name in ["list_open_documents", "read_document"]:

                    # æ–‡æ¡£æ“ä½œå·¥å…·ï¼Œæå–å…³é”®ä¿¡æ¯

                    pass  # å·¥å…·ç»“æœä¼šåœ¨æœ€ç»ˆå“åº”ä¸­å¤„ç†

                else:

                    # å…¶ä»–å·¥å…·ï¼Œç»“æœåœ¨æœ€ç»ˆå“åº”ä¸­å¤„ç†

                    pass  # å·¥å…·ç»“æœä¼šåœ¨æœ€ç»ˆå“åº”ä¸­å¤„ç†



                # ä¸å‘é€tool_resultäº‹ä»¶ï¼Œé¿å…æš´éœ²å†…éƒ¨å®ç°

                # å·¥å…·ç»“æœä¼šåœ¨æœ€ç»ˆå“åº”ä¸­ä»¥è‡ªç„¶è¯­è¨€å½¢å¼å‘ˆç°



                # æ·»åŠ åˆ°æ¶ˆæ¯å†å²ï¼ˆç”¨äºå†…éƒ¨å¤„ç†ï¼‰- ä¼ é€’å®é™…ç»“æœè®©LLMå†³å®šä¸‹ä¸€æ­¥

                messages.append(("assistant", response.content))

                messages.append(("user", f"TOOL_RESULT: {result}"))



            else:

                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç”Ÿæˆæœ€ç»ˆå“åº”

                logger.info(f"[ReAct] æ— å·¥å…·è°ƒç”¨ï¼Œtool_callsæ•°é‡: {len(self.tool_calls)}")

                if self.tool_calls:

                    # æœ‰å·¥å…·è°ƒç”¨ç»“æœï¼Œæµå¼ç”Ÿæˆæœ€ç»ˆå“åº”

                    async for event in self._stream_final_response(message, messages, session_id):

                        yield event

                else:

                    # æ— å·¥å…·è°ƒç”¨ï¼šé¿å…å†æ¬¡è°ƒç”¨LLMç”Ÿæˆâ€œæœ€ç»ˆç­”æ¡ˆâ€ã€‚

                    # è¿™é‡Œçš„ response.content å·²ç»æ˜¯å¸¦ä¸Šä¸‹æ–‡çš„æœ€ç»ˆå›å¤ï¼ˆä¸”å·²é€šè¿‡ tool_call è§£æç¡®è®¤ä¸æ˜¯å·¥å…·è°ƒç”¨ï¼‰ã€‚

                    full_response = getattr(response, "content", None)

                    if full_response is None:

                        full_response = str(response)



                    processed_response = await self._send_response_with_code_check(full_response, session_id)

                    if processed_response:

                        yield {"type": "content", "content": processed_response, "session_id": session_id}

                    else:

                        # Never end a turn without any visible output; otherwise the UI/bench sees "no assistant message".

                        logger.warning("[ReAct] processed_responseä¸ºç©ºï¼Œå‘é€å¯è§é”™è¯¯æç¤ºï¼ˆé¿å…é™é»˜å¤±è´¥ï¼‰")

                        hint = "ã€ç³»ç»Ÿæç¤ºã€‘æœ¬è½®æ¨¡å‹æ²¡æœ‰è¿”å›å¯æ˜¾ç¤ºå†…å®¹ï¼ˆå¯èƒ½æ˜¯æ¨¡å‹/ç½‘ç»œå¼‚å¸¸ï¼‰ã€‚è¯·é‡è¯•ã€‚"

                        yield {"type": "content", "content": hint, "session_id": session_id}

                        processed_response = hint



                    # ä¿å­˜æœ€ç»ˆå“åº”ä»¥ä¾›åç»­è®°å¿†å­˜å‚¨ï¼ˆä¼˜å…ˆä¿å­˜ç”¨æˆ·å®é™…çœ‹åˆ°çš„å†…å®¹ï¼‰

                    self._last_response = processed_response or (full_response or "")

                return



        # è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°

        yield {

            "type": "content",

            "content": f"å·²æ‰§è¡Œ{len(self.tool_calls)}ä¸ªå·¥å…·ï¼Œåˆ†æå®Œæˆã€‚",

            "session_id": session_id,

        }



    async def _stream_final_response(self, message: str, messages: List, session_id: str):

        """æµå¼ç”Ÿæˆæœ€ç»ˆå“åº”



        Yields:

            dict: contentäº‹ä»¶

        """

        # æ„å»ºåˆ†æç»“æœï¼ˆè½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼‰

        analysis_result = ""

        for call in self.tool_calls:

            if call.result:

                # è½¬æ¢å·¥å…·ç»“æœä¸ºè‡ªç„¶è¯­è¨€æè¿°

                result = _format_result_for_user(call.tool_name, call.result)

                analysis_result += result + "\n\n"



        # è·å–æ™ºèƒ½å»ºè®®

        final_info = analysis_result.strip()



        # åœ¨ messages åŸºç¡€ä¸Šæ·»åŠ æœ€ç»ˆå“åº”æç¤º

        messages.append(

            (

                "system",

                "\nè¯·æ ¹æ®ä»¥ä¸Šå·¥å…·æ‰§è¡Œç»“æœç›´æ¥å›å¤ç”¨æˆ·ã€‚\n\nè¦æ±‚ï¼š\n1. ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜\n2. è¯­è¨€è‡ªç„¶ã€ç®€æ´\n3. ä»¥ä¸“ä¸šé¡¾é—®çš„å£å»\n4. ä¸è¦æåŠå·¥å…·è°ƒç”¨è¿‡ç¨‹",

            )

        )



        # ä½¿ç”¨åŒ…å«å·¥å…·ç»“æœçš„ messages ç”Ÿæˆæœ€ç»ˆå“åº”



        # ä½¿ç”¨æµå¼å“åº”

        full_response = ""



        # å…¼å®¹æ€§æ£€æŸ¥ï¼šç¡®ä¿ astream è¿”å›å¼‚æ­¥ç”Ÿæˆå™¨

        astream_result = self.llm.astream(messages)



        # æ£€æŸ¥æ˜¯å¦æ˜¯å¼‚æ­¥ç”Ÿæˆå™¨

        import inspect

        if inspect.isasyncgen(astream_result):

            # æ ‡å‡†å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œä½¿ç”¨ async for è¿­ä»£

            async for chunk in astream_result:

                # å¤„ç†DeepSeekæ€è€ƒæ¨¡å¼ï¼ˆç´¯ç§¯æ˜¾ç¤ºï¼‰

                reasoning = None

                if hasattr(chunk, "additional_kwargs") and chunk.additional_kwargs:

                    reasoning = chunk.additional_kwargs.get("reasoning_content")



                if reasoning:

                    # æ¯æ¬¡éƒ½å‘é€å®Œæ•´çš„ reasoning_contentï¼ˆå‰ç«¯ç´¯ç§¯æ˜¾ç¤ºï¼‰

                    logger.debug(f"[æ¨ç†] æµå¼å“åº” - reasoning_content: {reasoning[:200]}...")

                    yield {"type": "reasoning", "content": reasoning, "session_id": session_id}

                else:

                    logger.debug(f"[æ¨ç†] æµå¼å“åº” - æœªè·å–åˆ° reasoning_content")



                # æ”¶é›†å“åº”å†…å®¹ï¼Œä½†å…ˆä¸å‘é€ç»™å‰ç«¯

                if hasattr(chunk, "content") and chunk.content:

                    content = chunk.content

                    full_response += content

        else:

            # å¦‚æœä¸æ˜¯å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œè¯´æ˜æ˜¯æ™®é€šå“åº”å¯¹è±¡ï¼Œç›´æ¥è·å– content

            if hasattr(astream_result, "content"):

                full_response = astream_result.content

            else:

                full_response = str(astream_result)



        # ç»Ÿä¸€å¤„ç†å“åº”å‘é€ï¼ˆå«ä»£ç è´¨é‡æ£€æŸ¥ï¼‰

        logger.debug(f"[ReAct] å¼€å§‹å‘é€æœ€ç»ˆå“åº”ï¼ŒåŸå§‹å“åº”é•¿åº¦: {len(full_response)}")

        processed_response = await self._send_response_with_code_check(full_response, session_id)

        logger.debug(f"[ReAct] ä»£ç æ£€æŸ¥å®Œæˆï¼Œå¤„ç†åå“åº”é•¿åº¦: {len(processed_response) if processed_response else 0}")



        # å‘é€å¤„ç†åçš„å“åº”

        if processed_response:

            logger.debug(f"[ReAct] å‘é€contentäº‹ä»¶ï¼Œå“åº”é•¿åº¦: {len(processed_response)}")

            yield {"type": "content", "content": processed_response, "session_id": session_id}

        else:

            # Never end a turn without any visible output; otherwise the UI/bench sees "no assistant message".

            logger.warning("[ReAct] processed_responseä¸ºç©ºï¼Œå‘é€å¯è§é”™è¯¯æç¤ºï¼ˆé¿å…é™é»˜å¤±è´¥ï¼‰")

            hint = "ã€ç³»ç»Ÿæç¤ºã€‘æœ¬è½®æ¨¡å‹æ²¡æœ‰è¿”å›å¯æ˜¾ç¤ºå†…å®¹ï¼ˆå¯èƒ½æ˜¯æ¨¡å‹/ç½‘ç»œå¼‚å¸¸ï¼‰ã€‚è¯·é‡è¯•ã€‚"

            yield {"type": "content", "content": hint, "session_id": session_id}

            processed_response = hint



        # ä¿å­˜æœ€ç»ˆå“åº”ä»¥ä¾›åç»­ä½¿ç”¨

        # ç¡®ä¿å³ä½¿ç©ºå“åº”ä¹Ÿè¢«ä¿å­˜ï¼›ä¼˜å…ˆä¿å­˜ç”¨æˆ·å®é™…çœ‹åˆ°çš„ï¼ˆå¯èƒ½å·²è‡ªåŠ¨æ”¹å†™ä»£ç ï¼‰ã€‚

        final_to_store = processed_response or full_response or ""

        self._last_response = final_to_store

        logger.debug(f"[è®°å¿†å­˜å‚¨] _last_response å·²è®¾ç½®ä¸º: '{final_to_store[:100] if final_to_store else '(empty)'}...'")



    async def _stream_direct_response(self, message: str, context: str, session_id: str):

        """æµå¼ç›´æ¥å“åº”ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰



        Args:

            message: ç”¨æˆ·æ¶ˆæ¯

            context: å¯¹è¯ä¸Šä¸‹æ–‡

            session_id: ä¼šè¯ID



        Yields:

            dict: contentäº‹ä»¶

        """

        # æ„å»ºç›´æ¥å“åº”æ¶ˆæ¯ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡

        # Same ordering rationale as the main loop: keep dynamic context first and the core
        # system prompt last so it remains authoritative.
        #
        # Also apply a per-turn output mode guard. We don't have `want_writeback` here, so infer it
        # from the injected context directives.
        inferred_writeback = False
        try:
            c = str(context or "")
            inferred_writeback = ("ã€å†™å›æ–‡æ¡£æŒ‡ä»¤ã€‘" in c) or ("schema_version" in c and PLAN_SCHEMA_ID in c)
        except Exception as e:
            logger.debug("[output-guard] infer writeback failed (ignored): %s", e, exc_info=True)
            inferred_writeback = False
        turn_directive = (
            "ã€æœ¬è½®è¾“å‡ºæ¨¡å¼ã€‘æœ¬è½®ä¸éœ€è¦å†™å›æ–‡æ¡£ï¼šåªç”¨è‡ªç„¶è¯­è¨€å›ç­”ï¼›ä¸è¦è¾“å‡ºä»»ä½• fenced ä»£ç å—ï¼ˆä¸è¦ ```json``` / ```javascript```ï¼‰ï¼›ä¸è¦ç²˜è´´/å¤è¿°æ–‡æ¡£åŸæ–‡ï¼ˆå¦‚éœ€å¼•ç”¨ï¼Œæœ€å¤š1å¥ä¸”<=50å­—ï¼‰ã€‚"
            if not inferred_writeback
            else f"ã€æœ¬è½®è¾“å‡ºæ¨¡å¼ã€‘æœ¬è½®éœ€è¦å†™å›æ–‡æ¡£ï¼šè¯·åªè¾“å‡º 1 ä¸ª ```json``` ä»£ç å—ï¼ˆä»… JSONï¼‰ï¼Œschema_version å¿…é¡»æ˜¯ {PLAN_SCHEMA_ID}ï¼›ä¸è¦è¾“å‡º JS å®ã€‚"
        )
        messages = [
            ("system", context),  # ä½¿ç”¨ä¼ å…¥çš„ä¸Šä¸‹æ–‡
            ("system", self.system_prompt),
            ("system", turn_directive),
            ("user", message),
        ]



        # ä½¿ç”¨æµå¼å“åº”

        full_response = ""

        reasoning_content = ""



        # å…¼å®¹æ€§æ£€æŸ¥ï¼šç¡®ä¿ astream è¿”å›å¼‚æ­¥ç”Ÿæˆå™¨

        astream_result = self.llm.astream(messages)



        import inspect

        if inspect.isasyncgen(astream_result):

            # æ ‡å‡†å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œä½¿ç”¨ async for è¿­ä»£

            async for chunk in astream_result:

                self._accumulate_token_usage(chunk)

                # å¤„ç†DeepSeekæ€è€ƒæ¨¡å¼

                if hasattr(chunk, "additional_kwargs") and chunk.additional_kwargs:

                    reasoning = chunk.additional_kwargs.get("reasoning_content")

                    if reasoning:

                        reasoning_content += reasoning



                # æ”¶é›†å“åº”å†…å®¹ï¼Œä½†å…ˆä¸å‘é€ç»™å‰ç«¯

                if hasattr(chunk, "content") and chunk.content:

                    content = chunk.content

                    full_response += content

        else:

            # å¦‚æœä¸æ˜¯å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œè¯´æ˜æ˜¯æ™®é€šå“åº”å¯¹è±¡ï¼Œç›´æ¥è·å– content

            if hasattr(astream_result, "content"):

                full_response = astream_result.content

            else:

                full_response = str(astream_result)



        # ğŸ” å‘é€æ€è€ƒè¿‡ç¨‹ï¼ˆå¦‚æœæœ‰ï¼‰

        if reasoning_content:

            yield {"type": "reasoning", "content": reasoning_content, "session_id": session_id}



        # ç»Ÿä¸€å¤„ç†å“åº”å‘é€ï¼ˆå«ä»£ç è´¨é‡æ£€æŸ¥ï¼‰

        logger.debug(f"[ReAct] å¼€å§‹å‘é€æœ€ç»ˆå“åº”ï¼ŒåŸå§‹å“åº”é•¿åº¦: {len(full_response)}")

        processed_response = await self._send_response_with_code_check(full_response, session_id)

        logger.debug(f"[ReAct] ä»£ç æ£€æŸ¥å®Œæˆï¼Œå¤„ç†åå“åº”é•¿åº¦: {len(processed_response) if processed_response else 0}")



        # å‘é€å¤„ç†åçš„å“åº”

        if processed_response:

            logger.debug(f"[ReAct] å‘é€contentäº‹ä»¶ï¼Œå“åº”é•¿åº¦: {len(processed_response)}")

            yield {"type": "content", "content": processed_response, "session_id": session_id}

        else:

            logger.warning(f"[ReAct] processed_responseä¸ºç©ºï¼Œä¸å‘é€contentäº‹ä»¶")



        # ä¿å­˜æœ€ç»ˆå“åº”ä»¥ä¾›åç»­ä½¿ç”¨

        # ç¡®ä¿å³ä½¿ç©ºå“åº”ä¹Ÿè¢«ä¿å­˜ï¼›ä¼˜å…ˆä¿å­˜ç”¨æˆ·å®é™…çœ‹åˆ°çš„ï¼ˆå¯èƒ½å·²è‡ªåŠ¨æ”¹å†™ä»£ç ï¼‰ã€‚

        final_to_store = processed_response or full_response or ""

        self._last_response = final_to_store

        logger.debug(f"[è®°å¿†å­˜å‚¨] _last_response å·²è®¾ç½®ä¸º: '{final_to_store[:100] if final_to_store else '(empty)'}...'")



    @staticmethod
    def _strip_fenced_code_blocks(text: str) -> str:
        """Remove triple-backtick fenced blocks from a model response (best-effort).

        Frontend auto-detects fenced blocks and may enqueue writeback execution. For turns that
        are not intended to write back, we keep outputs strictly non-executable.
        """
        s = str(text or "")
        if "```" not in s:
            return s
        try:
            # Remove any fenced blocks (```...```), including language tags.
            s2 = re.sub(r"```[\w.-]*\s*[\s\S]*?```", "", s)
            # If the model leaked stray fences, strip them too.
            s2 = s2.replace("```", "")
            # Normalize whitespace so the UI doesn't show large gaps.
            s2 = re.sub(r"\n{3,}", "\n\n", s2).strip()
            return s2
        except Exception as e:
            logger.debug("[output-guard] strip fenced blocks failed (ignored): %s", e, exc_info=True)
            return s

    @staticmethod
    def _keep_single_plan_fence(text: str) -> str:
        """Keep only one executable Plan JSON fence in a response (best-effort).

        Some models may emit multiple ```json``` fences. Frontend will detect and execute each fenced plan,
        which can accidentally pollute the document (e.g., a second "reply user" plan inserting chatty text).
        """
        s = str(text or "")
        if "```" not in s:
            return s

        try:
            plan_re = re.compile(r"```(?:json|plan|ah32[-_.]?plan(?:\.v1)?)\s*([\s\S]*?)```", re.IGNORECASE)
            matches = list(plan_re.finditer(s))
            if len(matches) <= 1:
                return s

            def _is_plan_candidate(body: str) -> bool:
                try:
                    t = str(body or "").strip()
                    if not t:
                        return False

                    # Handle malformed captures like "json\\n{...}" or accidental wrappers.
                    nl = t.find("\n")
                    if 0 < nl <= 20:
                        first = t[:nl].strip().lower()
                        rest = t[nl + 1 :].strip()
                        if (
                            first == "json" or first == "plan" or first.startswith("ah32")
                        ) and rest.startswith("{"):
                            t = rest

                    try:
                        obj = json.loads(t)
                    except json.JSONDecodeError:
                        obj = None
                    except Exception as e:
                        logger.debug(
                            "[output-guard] json.loads(t) failed unexpectedly (ignored): %s",
                            e,
                            exc_info=True,
                        )
                        obj = None
                    if isinstance(obj, dict) and obj.get("schema_version") == PLAN_SCHEMA_ID:
                        return True

                    fb = t.find("{")
                    lb = t.rfind("}")
                    if fb < 0 or lb <= fb:
                        return False
                    try:
                        obj = json.loads(t[fb : lb + 1])
                        return isinstance(obj, dict) and obj.get("schema_version") == PLAN_SCHEMA_ID
                    except json.JSONDecodeError:
                        return False
                    except Exception as e:
                        logger.debug(
                            "[output-guard] json.loads(slice) failed unexpectedly (ignored): %s",
                            e,
                            exc_info=True,
                        )
                        return False
                except Exception as e:
                    logger.debug(
                        "[output-guard] plan candidate scan failed (ignored): %s",
                        e,
                        exc_info=True,
                    )
                    return False

            keep_idx = None
            for idx, m in enumerate(matches):
                if _is_plan_candidate(m.group(1)):
                    keep_idx = idx
                    break
            if keep_idx is None:
                keep_idx = 0

            out_parts = []
            cursor = 0
            for idx, m in enumerate(matches):
                out_parts.append(s[cursor : m.start()])
                if idx == keep_idx:
                    out_parts.append(s[m.start() : m.end()])
                cursor = m.end()
            out_parts.append(s[cursor:])
            return "".join(out_parts).strip()
        except Exception as e:
            logger.debug("[output-guard] keep single plan fence failed (ignored): %s", e, exc_info=True)
            return s

    def _is_known_tool_name(self, name: str) -> bool:
        """Best-effort: check if a tool name is known (built-in or selected-skill tool)."""
        tool = str(name or "").strip()
        if not tool:
            return False
        try:
            tools = getattr(self, "tools", None)
            if isinstance(tools, dict) and tool in tools:
                return True
        except Exception:
            logger.debug("[tools] check built-in tools failed (ignored)", exc_info=True)

        try:
            reg = getattr(self, "skills_registry", None)
            selected = getattr(self, "_selected_skills", None) or []
            if reg is not None and selected:
                for skill in selected:
                    try:
                        if reg.find_tool(skill.skill_id, tool) is not None:
                            return True
                    except Exception:
                        continue
        except Exception:
            logger.debug("[tools] check skill tools failed (ignored)", exc_info=True)
        return False

    def _strip_internal_json_objects(self, text: str) -> str:
        """Strip internal JSON payloads from user-visible output (best-effort).

        We must never leak:
        - Tool-call JSON objects (e.g. {"name": "read_document", ...})
        - Plan JSON objects (schema_version=ah32.plan.v1) on non-writeback turns

        This is a safety net; the primary mechanism is routing + tool parsing.
        """
        s = str(text or "")
        if "{" not in s:
            return s
        try:
            start_re = re.compile(r'\{\s*"(?:schema_version|name|action|error)"\s*:')
            matches = list(start_re.finditer(s))
            if not matches:
                return s

            decoder = json.JSONDecoder()
            spans: list[tuple[int, int]] = []
            for m in matches:
                start = m.start()
                try:
                    obj, end = decoder.raw_decode(s[start:])
                except json.JSONDecodeError:
                    continue
                except Exception as e:
                    logger.debug("[output-guard] raw_decode internal json failed (ignored): %s", e, exc_info=True)
                    continue

                if not isinstance(obj, dict):
                    continue

                is_plan = obj.get("schema_version") == PLAN_SCHEMA_ID
                is_tool_call = False
                is_error = False
                try:
                    if "name" in obj and isinstance(obj.get("name"), str):
                        is_tool_call = self._is_known_tool_name(obj.get("name"))
                    elif "action" in obj and isinstance(obj.get("action"), str):
                        is_tool_call = self._is_known_tool_name(obj.get("action"))
                    elif "error" in obj:
                        is_error = True
                except Exception as e:
                    logger.debug("[output-guard] check internal tool-call failed (ignored): %s", e, exc_info=True)
                    is_tool_call = False

                if is_plan or is_tool_call or is_error:
                    spans.append((start, start + int(end)))

            if not spans:
                return s

            # Merge overlapping spans (defensive).
            spans.sort()
            merged: list[tuple[int, int]] = []
            for a, b in spans:
                if not merged or a > merged[-1][1]:
                    merged.append((a, b))
                else:
                    merged[-1] = (merged[-1][0], max(merged[-1][1], b))

            out = s
            for a, b in reversed(merged):
                out = out[:a] + out[b:]
            out = re.sub(r"\n{3,}", "\n\n", out).strip()
            return out
        except Exception as e:
            logger.debug("[output-guard] strip internal json objects failed (ignored): %s", e, exc_info=True)
            return s


    async def _send_response_with_code_check(self, full_response: str, session_id: str):
        """Send response without JS macro review (plan-only).

        Also strip any fenced blocks on non-writeback turns to prevent accidental auto-writeback.
        """
        try:
            if re.search(r"```(?:javascript|js)\s*\n", full_response or "", re.IGNORECASE):
                logger.warning("[plan-only] JS macro block detected in response; skipping macro review.")
        except Exception as e:
            logger.debug("[plan-only] macro block scan failed (ignored): %s", e, exc_info=True)

        want_writeback = False
        try:
            want_writeback = bool(getattr(self, "_turn_want_writeback", False))
        except Exception as e:
            logger.debug("[output-guard] read _turn_want_writeback failed (ignored): %s", e, exc_info=True)
            want_writeback = False

        if not want_writeback:
            stripped = self._strip_fenced_code_blocks(full_response)
            stripped2 = self._strip_internal_json_objects(stripped)
            if stripped2 != (stripped or ""):
                logger.info(
                    "[output-guard] stripped internal json (non-writeback) session_id=%s len_before=%s len_after=%s",
                    session_id,
                    len(stripped or ""),
                    len(stripped2 or ""),
                )
            if stripped != (full_response or ""):
                logger.info(
                    "[output-guard] stripped fenced blocks (non-writeback) session_id=%s len_before=%s len_after=%s",
                    session_id,
                    len(full_response or ""),
                    len(stripped or ""),
                )
            return stripped2

        host_app = ""
        try:
            host_app = str(getattr(getattr(self, "_turn_run_context", None), "host_app", "") or "").strip().lower()
        except Exception:
            host_app = ""
        host = host_app if host_app in ("wps", "et", "wpp") else "wps"

        # Writeback invariant: return a single executable Plan JSON fence and nothing else.
        plan = None
        plan_err = ""
        try:
            plan, plan_err = self._extract_plan_from_text(full_response or "", host)
            if plan is not None:
                return f"```json\n{json.dumps(plan, ensure_ascii=False, default=str)}\n```"
        except Exception as e:
            logger.debug("[output-guard] extract plan for canonicalization failed (ignored): %s", e, exc_info=True)
            plan = None
            plan_err = f"invalid_plan:{e}"

        # If we couldn't extract a valid plan, attempt one repair *before* streaming content.
        # This prevents emitting an invalid Plan block and then appending a second repaired Plan later.
        attempted = False
        try:
            attempted = bool(getattr(self, "_turn_writeback_repair_attempted", False))
        except Exception:
            attempted = False

        if not attempted:
            try:
                self._turn_writeback_repair_attempted = True
            except Exception:
                pass
            try:
                user_query = str(getattr(self, "_turn_user_query", "") or "").strip()
                rag_ctx = str(getattr(self, "_turn_rag_context", "") or "")
                anchor = str(getattr(self, "_turn_writeback_anchor", "") or "cursor").strip() or "cursor"
                delivery = str(getattr(self, "_turn_writeback_delivery", "") or "").strip()

                repaired = await self._auto_repair_missing_writeback_plan(
                    user_query=user_query,
                    rag_context=rag_ctx,
                    session_id=session_id,
                    anchor=anchor,
                    delivery=delivery,
                    original_plan=plan,
                    error_type="missing_plan" if (not plan_err or plan_err == "empty_plan_payload") else "invalid_plan",
                    error_message=plan_err or "",
                )
                if repaired is not None:
                    return f"```json\n{json.dumps(repaired, ensure_ascii=False, default=str)}\n```"
            except Exception as e:
                logger.error("[writeback] auto-repair (output-guard) failed: %s", e, exc_info=True)

        # If repair still didn't produce a valid plan, avoid leaking invalid JSON to the frontend executor.
        return "ã€ç³»ç»Ÿæç¤ºã€‘æœ¬è½®éœ€è¦å†™å…¥æ–‡æ¡£ï¼Œä½†æœªèƒ½ç”Ÿæˆå¯æ‰§è¡Œçš„ Plan JSONã€‚è¯·ç‚¹å‡»é‡è¯•ï¼Œæˆ–æ˜ç¡®â€œåªè¾“å‡º Plan JSONâ€ã€‚"

    def _extract_self_check_result(self, response: str) -> Optional[Dict[str, Any]]:

        """ä»LLMå“åº”ä¸­æå–è‡ªæ£€ç»“æœ



        Args:

            response: LLMå“åº”å†…å®¹



        Returns:

            è‡ªæ£€ç»“æœå­—å…¸ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None

        """

        try:

            # å°è¯•æå–JSONæ ¼å¼çš„è‡ªæ£€ç»“æœ

            json_match = re.search(r'```json\s*\n(.*?)\n```', response, re.DOTALL)

            if json_match:

                json_str = json_match.group(1).strip()

                try:

                    parsed = json.loads(json_str)

                    if "self_check" in parsed:

                        return parsed["self_check"]

                except json.JSONDecodeError:

                    pass



            # å°è¯•ä»æ–‡æœ¬ä¸­æå–è‡ªæ£€ä¿¡æ¯

            if '"self_check"' in response:

                # ç®€å•çš„æ–‡æœ¬åŒ¹é…ï¼Œæå–å…³é”®ä¿¡æ¯

                if '"result": "pass"' in response:

                    return {

                        "performed": True,

                        "result": "pass",

                        "issues": [],

                        "summary": "LLMè‡ªæ£€é€šè¿‡"

                    }

                elif '"result": "fail"' in response:

                    issues = []

                    # å°è¯•æå–é—®é¢˜åˆ—è¡¨

                    issues_match = re.search(r'"issues":\s*\[(.*?)\]', response, re.DOTALL)

                    if issues_match:

                        issues_str = issues_match.group(1)

                        issues = [issue.strip().strip('"') for issue in issues_str.split(',')]



                    return {

                        "performed": True,

                        "result": "fail",

                        "issues": issues,

                        "summary": "LLMè‡ªæ£€å‘ç°ä»£ç é—®é¢˜"

                    }

        except Exception as e:

            logger.debug(f"æå–è‡ªæ£€ç»“æœå¤±è´¥: {e}")



        return None



    def _quick_js_macro_review(self, code: str) -> Dict[str, Any]:

        """Fast, local-only JS macro review and a few safe rewrites.



        We keep this extremely cheap because it runs inline on the chat request path.

        """

        errors: List[str] = []

        warnings: List[str] = []

        fixed_code = code or ""



        # 1) VBA residue (common failure mode when the model drifts).

        vba_patterns = [

            (r"\bDim\s+\w+", "VBAè¯­æ³•æ®‹ç•™: Dim å˜é‡å£°æ˜"),

            (r"\bSet\s+\w+\s*=", "VBAè¯­æ³•æ®‹ç•™: Set èµ‹å€¼è¯­å¥"),

            (r"\bEnd\s+\w+\b", "VBAè¯­æ³•æ®‹ç•™: End è¯­å¥"),

            (r"\bNext\b", "VBAè¯­æ³•æ®‹ç•™: Next å¾ªç¯"),

            (r"\bIf\s+.*\s+Then\b", "VBAè¯­æ³•æ®‹ç•™: If ... Then è¯­å¥"),

        ]

        for pattern, desc in vba_patterns:

            if re.search(pattern, code or "", re.IGNORECASE):

                errors.append(desc)



        # 2) Known WPS JS pitfalls: some samples use convenience APIs that are not

        # always present. We can safely rewrite a subset to standard Word-like APIs.

        if re.search(r"\bselection\.StartOf\s*\(", code or ""):

            warnings.append("æ£€æµ‹åˆ° selection.StartOf()ï¼›å·²è‡ªåŠ¨æ”¹å†™ä¸º selection.HomeKey(6) ä»¥æå‡å…¼å®¹æ€§ã€‚")

            fixed_code = re.sub(

                r"\bselection\.StartOf\s*\(\s*\)\s*;?",

                "selection.HomeKey(6);",

                fixed_code,

            )

        if re.search(r"\bselection\.EndOf\s*\(", code or ""):

            warnings.append("æ£€æµ‹åˆ° selection.EndOf()ï¼›å·²è‡ªåŠ¨æ”¹å†™ä¸º selection.EndKey(6) ä»¥æå‡å…¼å®¹æ€§ã€‚")

            fixed_code = re.sub(

                r"\bselection\.EndOf\s*\(\s*\)\s*;?",

                "selection.EndKey(6);",

                fixed_code,

            )



        # 3) Soft checks.

        if (code or "").count("(") != (code or "").count(")"):

            errors.append("æ‹¬å·ä¸åŒ¹é…")



        # Many snippets in prompts use selection.GoTo(line). That API is not guaranteed

        # to exist in WPS JS; we prefer a runtime shim on the frontend.

        if re.search(r"\bselection\.GoTo\s*\(\s*\d+\s*\)", code or ""):

            warnings.append("æ£€æµ‹åˆ° selection.GoTo(n) ç®€å†™ï¼›ä¸åŒ WPS ç‰ˆæœ¬å¯èƒ½ä¸å…¼å®¹ï¼ˆå»ºè®®ä½¿ç”¨ GoTo å…¼å®¹å†™æ³•æˆ–è®©æ‰§è¡Œå™¨ shimï¼‰ã€‚")



        return {"errors": errors, "warnings": warnings, "fixed_code": fixed_code}



    async def _check_and_fix_code(self, response: str, session_id: str) -> str:

        """æ£€æµ‹å¹¶ä¿®æ­£ä»£ç è´¨é‡ - Vibe Codingå¯è§†åŒ–æµç¨‹



        Args:

            response: åŸå§‹å“åº”

            session_id: ä¼šè¯ID



        Returns:

            str: ä¿®æ­£åçš„å“åº”

        """

        # æå–ä»£ç å—

        code_blocks = re.findall(r'```(?:javascript|js)?\n(.*?)```', response, re.DOTALL)

        if not code_blocks:

            logger.info("ğŸ” æœªæ‰¾åˆ°ä»£ç å—ï¼Œè·³è¿‡æ£€æµ‹")

            return response



        # æå–åŸå§‹ä»£ç 

        original_code = code_blocks[0]

        logger.info(f"ğŸ” å¯åŠ¨JSå®å¿«é€Ÿè‡ªæ£€ï¼Œä»£ç é•¿åº¦: {len(original_code)}")



        review = self._quick_js_macro_review(original_code)

        fixed_code = review.get("fixed_code", original_code)

        errors = review.get("errors", []) or []

        warnings = review.get("warnings", []) or []



        if warnings:

            logger.info(f"ğŸ” JSå®è‡ªæ£€è­¦å‘Š: {warnings[:3]}{'...' if len(warnings) > 3 else ''}")



        # If there are no hard errors, keep it fast: return immediately (with safe rewrites if any).

        if not errors:

            if fixed_code != original_code:

                logger.info("âœ… å·²åº”ç”¨å¿«é€Ÿå…¼å®¹æ€§æ”¹å†™ï¼ˆæ— éœ€LLMäºŒæ¬¡æ£€æŸ¥ï¼‰")

                return re.sub(

                    r"```(?:javascript|js)?\n(.*?)```",

                    f"```javascript\n{fixed_code}\n```",

                    response,

                    count=1,

                    flags=re.DOTALL,

                )

            return response



        logger.warning(f"âš ï¸ JSå®è‡ªæ£€å‘ç°é—®é¢˜: {errors}")



        # Try Vibe Coding workflow (LangGraph) only when we have hard issues.

        # This can be expensive (LLM calls), so keep it conditional.

        if not self.llm:

            logger.warning("âš ï¸ LLMä¸å¯ç”¨ï¼Œæ— æ³•è‡ªåŠ¨ä¿®å¤JSå®ä»£ç ï¼Œè¿”å›åŸå§‹å“åº”")

            return response



        try:

            from .js_macro_workflow import JSMacroVibeWorkflow

        except Exception as e:

            logger.warning(f"æ— æ³•å¯¼å…¥Vibe Codingå·¥ä½œæµ: {e}")

            return await self._traditional_llm_check_and_fix(original_code, response, session_id)



        workflow = JSMacroVibeWorkflow(llm=self.llm)

        user_query = (

            "è¯·ä¿®å¤ä¸‹é¢çš„WPS JSå®ä»£ç ï¼Œä½¿å…¶å¯åœ¨WPSå…è´¹ç‰ˆä¸­æ‰§è¡Œã€‚\n"

            "è¦æ±‚ï¼š\n"

            "1) ä¸è¦ä½¿ç”¨VBAè¯­æ³•ï¼›ä½¿ç”¨ window.Application/app.Selection ç­‰å¯¹è±¡æ¨¡å‹ã€‚\n"

            "2) é¿å…ä½¿ç”¨ä¸ç¡®å®šå­˜åœ¨çš„APIï¼ˆå¦‚ StartOf/EndOf/GoTo çš„ç®€å†™ï¼‰ï¼Œå¿…è¦æ—¶æä¾›å…¼å®¹å†™æ³•ã€‚\n"

            "3) ä¸è¦è¾“å‡ºTypeScript/ESMè¯­æ³•ï¼ˆç±»å‹æ³¨è§£ã€interface/typeã€asæ–­è¨€ã€éç©ºæ–­è¨€!ï¼›ä¸è¦ç”¨ import/exportï¼‰ã€‚\n"

            "4) æ¨èä¼˜å…ˆç”¨ selection.Range.Text å†™å…¥æ–‡æœ¬ï¼ˆé¿å… TypeTextï¼‰ã€‚\n"

            "5) å¯é€‰ï¼šä½ å¯ä»¥ä½¿ç”¨ BID åŠ©æ‰‹å¯¹è±¡ï¼ˆè¿è¡Œæ—¶æ³¨å…¥ï¼‰ä»¥æ›´ç¨³å®šè°ƒç”¨ WPS å†…ç½®èƒ½åŠ›ï¼šBID.upsertBlock / BID.insertTable / BID.insertChartFromSelection / BID.insertWordArtã€‚\n"

            "6) ä¿ç•™åŸæ„å›¾ä¸è¡Œä¸ºã€‚\n\n"

            f"å·²æ£€æµ‹åˆ°çš„é—®é¢˜ï¼š\n- " + "\n- ".join(errors) + "\n\n"

            "åŸä»£ç ï¼š\n```javascript\n" + original_code.strip() + "\n```"

        )



        logger.info("ğŸš€ å¯åŠ¨Vibe Codingå·¥ä½œæµè¿›è¡Œä¿®å¤ï¼ˆä»…åœ¨æ£€æµ‹åˆ°é—®é¢˜æ—¶è§¦å‘ï¼‰...")

        final_code: Optional[str] = None

        final_success: Optional[bool] = None

        try:

            async for event in workflow.process_with_visualization(user_query, session_id):

                if event.get("type") == "final_result":

                    result = event.get("result") or {}

                    final_code = (result.get("code") or "").strip() or None

                    final_success = bool(result.get("success", False))

        except Exception as e:

            logger.error(f"Vibe Codingå·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")

            return await self._traditional_llm_check_and_fix(original_code, response, session_id)



        if final_code:

            logger.info(f"âœ… Vibe Codingä¿®å¤å®Œæˆ success={final_success}ï¼Œæ–°ä»£ç é•¿åº¦: {len(final_code)}")

            return re.sub(

                r"```(?:javascript|js)?\n(.*?)```",

                f"```javascript\n{final_code}\n```",

                response,

                count=1,

                flags=re.DOTALL,

            )



        logger.warning("âš ï¸ Vibe Codingæœªè¿”å›å¯ç”¨ä»£ç ï¼Œè¿”å›åŸå§‹å“åº”")

        return response



    async def _traditional_llm_check_and_fix(self, code: str, response: str, session_id: str) -> str:

        """ä¼ ç»ŸLLMæ£€æŸ¥å’Œä¿®å¤æµç¨‹ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""

        # Keep fallback local-only to avoid adding a second long LLM call on the request path.

        logger.info("ğŸ” å¯åŠ¨ä¼ ç»Ÿæœ¬åœ°æ£€æŸ¥æµç¨‹ï¼ˆæ— LLMå›é€€ï¼‰...")

        review = self._quick_js_macro_review(code)

        fixed_code = review.get("fixed_code", code)

        if fixed_code != code:

            logger.info("âœ… ä¼ ç»Ÿå›é€€æµç¨‹åº”ç”¨äº†å…¼å®¹æ€§æ”¹å†™")

            return re.sub(

                r"```(?:javascript|js)?\n(.*?)```",

                f"```javascript\n{fixed_code}\n```",

                response,

                count=1,

                flags=re.DOTALL,

            )

        logger.info("âœ… ä¼ ç»Ÿå›é€€æµç¨‹å®Œæˆï¼ˆæ— æ”¹åŠ¨ï¼‰")

        return response
