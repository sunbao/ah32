"""Ah32è®°å¿†ç®¡ç†å™¨ - ç®€åŒ–ç‰ˆè®°å¿†ç®¡ç†

åŸºäºä¸‰å±‚è®°å¿†æ¶æ„ï¼ˆä¼šè¯ã€è·¨ä¼šè¯ã€å…¨å±€ï¼‰ï¼Œä½¿ç”¨ç®€åŒ–å…³é”®è¯åˆ†ç±»
"""

from __future__ import annotations

import asyncio
import logging
import os
import threading
from datetime import datetime
from typing import Dict, List, Any, Optional

from ah32.config import settings

from ..services.memory import get_memory_manager, get_global_user_memory, get_task_memory

logger = logging.getLogger(__name__)


# å…¨å±€å‘é‡å­˜å‚¨ç¼“å­˜
_vector_store_cache = None
_vector_store_write_lock = threading.Lock()

# Default: do NOT block chat/macro responses on embedding/vector DB I/O.
_memory_vector_write_sync = os.getenv("AH32_MEMORY_VECTOR_WRITE_SYNC", "").lower() in ("1", "true", "yes")
# Default: only vectorize cross-session/global memory, not every session turn.
_memory_vectorize_session = os.getenv("AH32_MEMORY_VECTORIZE_SESSION", "").lower() in ("1", "true", "yes")
_memory_vector_max_chars = int(os.getenv("AH32_MEMORY_VECTOR_MAX_CHARS", "2000") or "2000")


def _truncate_for_embedding(text: str, max_chars: int) -> str:
    if not text:
        return ""
    if max_chars <= 0:
        return text
    if len(text) <= max_chars:
        return text
    return text[:max_chars] + "\n...[truncated]..."


def _log_task_exception(task: asyncio.Task, *, desc: str) -> None:
    try:
        exc = task.exception()
    except asyncio.CancelledError:
        return
    except Exception as e:
        logger.warning(f"[memory] background task '{desc}' status unknown: {e}")
        return
    if exc is not None:
        logger.warning(f"[memory] background task '{desc}' failed: {exc}")


def _get_vector_store():
    """è·å–å…¨å±€å‘é‡å­˜å‚¨ï¼ˆæ‡’åŠ è½½å•ä¾‹ï¼‰"""
    global _vector_store_cache
    if _vector_store_cache is None:
        try:
            from ..knowledge.store import ChromaDBStore
            from ..knowledge.chroma_utils import make_collection_name
            from ..knowledge.embeddings import resolve_embedding

            persist_path = _get_storage_path()
            embedding = resolve_embedding(settings)
            embedding_dim = None
            try:
                embedding_dim = len(embedding.embed_query("dimension_probe"))
            except Exception as e:
                logger.warning(f"[embedding] dimension probe failed for memory store: {e}")
            collection_name = make_collection_name(
                "ah32_memory",
                settings.embedding_model,
                embedding_dim=embedding_dim,
            )
            _vector_store_cache = ChromaDBStore(
                persist_path=persist_path,
                embedding=embedding,
                reset=False,
                config={
                    "collection_name": collection_name,
                    "collection_metadata": {
                        "embedding_model": settings.embedding_model,
                        "embedding_dim": embedding_dim,
                    },
                },
            )
            logger.info(f"å‘é‡å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ: {persist_path} (collection={collection_name})")
        except Exception as e:
            logger.error(f"å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return None
    return _vector_store_cache


def _get_storage_path():
    """è·å–å‘é‡å­˜å‚¨è·¯å¾„"""
    return settings.memory_root / "vector_store"


class Ah32MemorySystem:
    """Ah32è®°å¿†ç³»ç»Ÿ - åŸºäºä¸‰å±‚è®°å¿†æ¶æ„ï¼ˆä¼šè¯ã€è·¨ä¼šè¯ã€å…¨å±€ï¼‰"""

    def __init__(self, llm=None):
        """åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ

        Args:
            llm: LLMå®ä¾‹ï¼ˆä¸å†ä½¿ç”¨ï¼Œç®€åŒ–ä¸ºå…³é”®è¯åˆ†ç±»ï¼‰
        """
        # åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨
        self.memory_manager = get_memory_manager()

        # è®°å¿†ç´¢å¼•
        self.memory_index = {
            "session_memory": "session_memory",
            "cross_session_memory": "cross_session_memory",
            "global_memory": "global_memory"
        }

    async def store_conversation(self, session_id: str, user_message: str,
                                 assistant_response: str, metadata: Dict[str, Any] = None):
        """å­˜å‚¨å¯¹è¯åˆ°è®°å¿†ç³»ç»Ÿï¼ˆæ”¯æŒæ™ºèƒ½åˆ†ç±»ï¼‰

        Args:
            session_id: ä¼šè¯ID
            user_message: ç”¨æˆ·æ¶ˆæ¯
            assistant_response: åŠ©æ‰‹å“åº”
            metadata: å…ƒæ•°æ®
        """
        logger.debug(f"[è®°å¿†å­˜å‚¨] å¼€å§‹å­˜å‚¨å¯¹è¯ï¼Œsession_id: {session_id}")
        logger.debug(f"[è®°å¿†å­˜å‚¨] ç”¨æˆ·æ¶ˆæ¯: {user_message[:100]}...")
        logger.debug(f"[è®°å¿†å­˜å‚¨] åŠ©æ‰‹å“åº”: {assistant_response[:100]}...")
        # ç¡®ä¿session_idä¸ä¸ºç©º
        if not session_id:
            import uuid
            session_id = f"session_{uuid.uuid4().hex[:12]}"
            logger.warning(f"[è®°å¿†å­˜å‚¨] session_idä¸ºç©ºï¼Œè‡ªåŠ¨ç”Ÿæˆ: {session_id}")

        timestamp = datetime.now().isoformat()

        # 1. ä½¿ç”¨ç®€åŒ–çš„å…³é”®è¯åˆ†ç±»
        classification_result = None
        try:
            from ..strategies.llm_driven_strategy import classify_conversation
            classification_result = classify_conversation(user_message)
            logger.debug(f"ç®€åŒ–åˆ†ç±»ç»“æœ: {classification_result.to_dict()}")
        except Exception as e:
            logger.warning(f"ç®€åŒ–åˆ†ç±»å¤±è´¥: {e}")

        # 2. ç¡®å®šå­˜å‚¨ç±»å‹
        storage_type = "session_memory"  # é»˜è®¤å­˜å‚¨ç±»å‹
        classification_dict = None
        bench_mode = bool((metadata or {}).get("bench"))

        if classification_result and not bench_mode:
            # æ ¹æ®åˆ†ç±»ç»“æœç¡®å®šå­˜å‚¨ç±»å‹
            if classification_result.storage_level.value == "P0_å…¨å±€è®°å¿†":
                storage_type = "global_memory"
            elif classification_result.storage_level.value == "P2_è·¨ä¼šè¯è®°å¿†":
                storage_type = "cross_session_memory"
            else:
                storage_type = "session_memory"

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            classification_dict = classification_result.to_dict()
        elif bench_mode and classification_result:
            # Keep the classification for debugging, but never promote bench turns to cross-session/global.
            classification_dict = classification_result.to_dict()

        # 3. è·å–ä¼šè¯è®°å¿†å®ä¾‹
        session_memory = get_task_memory(session_id)

        # 4. æ·»åŠ å¯¹è¯åˆ°ä¼šè¯è®°å¿†ï¼ˆä¼ é€’å­˜å‚¨ç±»å‹å’Œåˆ†ç±»ç»“æœï¼‰
        session_memory.add_conversation(
            role="user",
            message=user_message,
            section_id=metadata.get("section_id") if metadata else None,
            storage_type=storage_type,
            classification_result=classification_dict
        )
        session_memory.add_conversation(
            role="assistant",
            message=assistant_response,
            section_id=metadata.get("section_id") if metadata else None,
            storage_type=storage_type,
            classification_result=classification_dict
        )

        # 5) Vector store (optional):
        # - Do not block interactive UX on embeddings/vector DB I/O by default.
        # - Do not vectorize every session turn by default (cross-session/global only).
        should_vectorize = (
            (not bench_mode)
            and (storage_type in ("cross_session_memory", "global_memory") or _memory_vectorize_session)
        )
        if should_vectorize:
            conversation_text = _truncate_for_embedding(
                f"ç”¨æˆ·: {user_message}\nåŠ©æ‰‹: {assistant_response}",
                _memory_vector_max_chars,
            )
            document_hash = ""
            try:
                from ..session.session_id_generator import SessionIdGenerator

                document_hash = SessionIdGenerator.get_file_hash(session_id) or ""
            except Exception:
                document_hash = ""

            vector_metadata = {
                "session_id": session_id,
                "document_hash": document_hash,
                "storage_type": storage_type,
                "timestamp": timestamp,
                "source": "conversation_memory",
            }
            if classification_dict:
                vector_metadata.update(classification_dict)

            if _memory_vector_write_sync:
                await self._store_to_vector_db(conversation_text, vector_metadata)
            else:
                task = asyncio.create_task(self._store_to_vector_db(conversation_text, vector_metadata))
                task.add_done_callback(lambda t: _log_task_exception(t, desc="memory_vector_write"))

        return classification_result

    async def _store_to_vector_db(self, content: str, metadata: Dict[str, Any]):
        """å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“"""
        try:
            vector_store = _get_vector_store()
            if vector_store is None:
                logger.warning("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“")
                return

            # ç”Ÿæˆå”¯ä¸€ID
            doc_id = f"mem_{datetime.now().isoformat()}"

            # æ‰‹åŠ¨è¿‡æ»¤å¤æ‚çš„metadataï¼ˆChromaDBä¸æ”¯æŒå¤æ‚çš„åµŒå¥—ç»“æ„ï¼‰
            # åªä¿ç•™åŸºæœ¬ç±»å‹ï¼šstr, bool, int, float
            allowed_types = (str, bool, int, float)
            filtered_metadata = {
                k: v for k, v in metadata.items()
                if isinstance(v, allowed_types)
            }
            # æ·»åŠ åŸºæœ¬å…ƒæ•°æ®
            filtered_metadata["id"] = doc_id
            filtered_metadata["created_at"] = datetime.now().isoformat()

            # åˆ›å»ºDocumentå¯¹è±¡
            from langchain_core.documents import Document
            doc = Document(
                page_content=content,
                metadata=filtered_metadata
            )

            # add_documents() is sync and can be slow (embeddings + disk I/O). Run it in a thread and
            # serialize writes to avoid SQLite/Chroma concurrency issues.
            def _write_one():
                with _vector_store_write_lock:
                    vector_store.add_documents([doc])

            await asyncio.to_thread(_write_one)

            logger.debug(f"å·²å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“: {doc_id}")

        except Exception as e:
            import traceback
            logger.error(f"å‘é‡æ•°æ®åº“å­˜å‚¨å¤±è´¥: {e}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

    async def retrieve_relevant_memory(self, query: str, session_id: Optional[str] = None,
                                      memory_types: List[str] = None,
                                      limit: int = 5, filter_by_document: bool = False) -> Dict[str, List[Dict]]:
        """æ£€ç´¢ç›¸å…³è®°å¿†

        Args:
            query: æŸ¥è¯¢å†…å®¹
            session_id: ä¼šè¯IDï¼ˆå¯é€‰ï¼Œç”¨äºè¿‡æ»¤ç‰¹å®šä¼šè¯çš„è®°å¿†ï¼‰
            memory_types: è®°å¿†ç±»å‹åˆ—è¡¨ (session_memory, cross_session_memory, global_memory)
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶

        Returns:
            åŒ…å«å„ç±»è®°å¿†çš„å­—å…¸
        """
        if memory_types is None:
            memory_types = ["session_memory", "cross_session_memory", "global_memory"]

        results = {}

        try:
            # æ„å»ºè¿‡æ»¤æ¡ä»¶
            filter_conditions = {}
            doc_hash = None
            if session_id:
                filter_conditions["session_id"] = session_id
                # æå–æ–‡æ¡£hashç”¨äºè¿‡æ»¤
                from ..session.session_id_generator import SessionIdGenerator
                doc_hash = SessionIdGenerator.get_file_hash(session_id)
                if doc_hash:
                    filter_conditions["document_hash"] = doc_hash

            # 1. æ£€ç´¢ä¼šè¯è®°å¿†ï¼ˆæŒ‰æ–‡æ¡£hashè¿‡æ»¤ï¼Œä¸“æ³¨å½“å‰æ ‡ä¹¦ï¼‰
            if "session_memory" in memory_types:
                if session_id:
                    session_memory = get_task_memory(session_id)
                    if session_memory:
                        # è·å–æœ€è¿‘çš„å¯¹è¯
                        recent_interactions = session_memory.get_conversation_history(limit)
                        # å°† ConversationMessage å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                        results["session_memory"] = [msg.model_dump() for msg in recent_interactions]

                        # å¦‚æœæœ‰æ–‡æ¡£hashï¼Œè®°å½•è¿‡æ»¤ä¿¡æ¯
                        if doc_hash:
                            logger.info(f"[è®°å¿†æ£€ç´¢] ä¼šè¯è®°å¿†æŒ‰æ–‡æ¡£hashè¿‡æ»¤: {doc_hash}, æ£€ç´¢åˆ° {len(results['session_memory'])} æ¡è®°å½•")
                    else:
                        results["session_memory"] = []
                else:
                    # session_id ä¸ºç©ºæ—¶ä»è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å… KeyError
                    results["session_memory"] = []

            # 2. æ£€ç´¢å…¨å±€è®°å¿†
            if "global_memory" in memory_types:
                global_memory = get_global_user_memory()
                results["global_memory"] = {
                    "user_profile": global_memory.get_user_profile().model_dump(),
                    "user_preferences": global_memory.get_user_preferences().model_dump()
                }

            # 3. æ£€ç´¢è·¨ä¼šè¯è®°å¿†ï¼ˆæŒ‰session_idéš”ç¦»ï¼Œå¯é€‰æŒ‰æ–‡æ¡£hashè¿‡æ»¤ï¼‰
            if "cross_session_memory" in memory_types:
                if session_id:
                    # Cross-session memory should survive multiple sessions for the same document/project.
                    # Use document_hash (derived from session_id) as the stable key.
                    cross_filter: Dict[str, Any] = {"storage_type": "cross_session_memory"}
                    if doc_hash:
                        cross_filter["document_hash"] = doc_hash
                        logger.info(f"[è®°å¿†æ£€ç´¢] è·¨ä¼šè¯è®°å¿†æŒ‰document_hashè¿‡æ»¤: {doc_hash}")
                    else:
                        # Fallback when session_id is not in expected format.
                        cross_filter["session_id"] = session_id
                        logger.info(f"[è®°å¿†æ£€ç´¢] è·¨ä¼šè¯è®°å¿†æŒ‰session_idè¿‡æ»¤(é™çº§): {session_id}")

                    try:
                        vector_store = _get_vector_store()
                        if vector_store is None:
                            logger.warning("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡è·¨ä¼šè¯è®°å¿†æ£€ç´¢")
                            results["cross_session_memory"] = []
                        else:
                            cross_docs_and_scores = vector_store.similarity_search(
                                query=query,
                                k=limit,
                                filter=cross_filter
                            )
                        results["cross_session_memory"] = [
                            {
                                "content": doc.page_content,
                                "metadata": doc.metadata,
                                "score": score
                            }
                            for doc, score in cross_docs_and_scores
                        ]
                        logger.info(f"[è®°å¿†æ£€ç´¢] æ£€ç´¢åˆ° {len(results['cross_session_memory'])} æ¡è·¨ä¼šè¯è®°å¿†")
                    except Exception as e:
                        logger.warning(f"æ£€ç´¢è·¨ä¼šè¯è®°å¿†å¤±è´¥: {e}")
                        results["cross_session_memory"] = []
                else:
                    # å¦‚æœæ²¡æœ‰session_idï¼Œä¸è¿”å›ä»»ä½•è·¨ä¼šè¯è®°å¿†
                    results["cross_session_memory"] = []
        except Exception as e:
            logger.error(f"æ£€ç´¢ç›¸å…³è®°å¿†å¤±è´¥: {e}")
            return {}
        return results

    async def get_comprehensive_context(self, query: str, session_id: Optional[str] = None) -> Dict[str, Any]:
        """è·å–ç»¼åˆä¸Šä¸‹æ–‡ä¿¡æ¯

        Args:
            query: æŸ¥è¯¢å†…å®¹
            session_id: ä¼šè¯IDï¼ˆå¯é€‰ï¼‰

        Returns:
            åŒ…å«ç»¼åˆä¸Šä¸‹æ–‡çš„å­—å…¸
        """
        try:
            # ğŸ”§ æ–°å¢ï¼šå¤„ç†@å¼•ç”¨åŠŸèƒ½
            at_reference_context = {}
            try:
                vector_store = _get_vector_store()
                if vector_store:
                    # ä½¿ç”¨@å¼•ç”¨å¤„ç†å™¨
                    from ..services.at_reference_handler import handle_at_references
                    at_result = await handle_at_references(query, vector_store)
                    at_reference_context = {
                        "at_references_processed": at_result.get("processed", []),
                        "at_references_errors": at_result.get("errors", []),
                        "at_references_paths": at_result.get("paths", [])
                    }
                    logger.debug(f"[@å¼•ç”¨] å¤„ç†ç»“æœ: {len(at_result.get('processed', []))} ä¸ªæ–‡ä»¶å·²å¤„ç†")
            except Exception as e:
                logger.warning(f"[@å¼•ç”¨] å¤„ç†å¤±è´¥: {e}")
                at_reference_context = {
                    "at_references_processed": [],
                    "at_references_errors": [str(e)],
                    "at_references_paths": []
                }

            # è·å–ç›¸å…³è®°å¿† - å¢åŠ limitåˆ°10ä»¥è·å–æ›´å¤šå¯¹è¯å†å²
            all_memories = await self.retrieve_relevant_memory(
                query=query,
                session_id=session_id,
                memory_types=["session_memory", "cross_session_memory", "global_memory"],
                limit=10
            )

            # è·å–ä¼šè¯è®°å¿†ï¼ˆåŒæ­¥æ–¹æ³•ï¼‰
            session_memory = None
            if session_id:
                session_memory = get_task_memory(session_id)

            # Pull structured per-document context from the task memory snapshot.
            project_context: Dict[str, Any] = {}
            cross_session_notes: List[Dict[str, Any]] = []
            try:
                if session_memory:
                    project_context = session_memory.get_project_context().model_dump()
                    doc_hash = ""
                    try:
                        from ..session.session_id_generator import SessionIdGenerator
                        doc_hash = SessionIdGenerator.get_file_hash(session_id) or ""
                    except Exception:
                        doc_hash = ""
                    if doc_hash:
                        record = session_memory.get_cross_session_memory(doc_hash, "notes")
                        if isinstance(record, dict) and isinstance(record.get("value"), list):
                            cross_session_notes = list(record.get("value") or [])[-20:]
            except Exception as e:
                logger.debug(f"[memory] load structured context failed (ignored): {e}")

            # æ„å»ºç»¼åˆä¸Šä¸‹æ–‡
            context = {
                "query": query,
                "timestamp": datetime.now().isoformat(),
                "session_memory": all_memories.get("session_memory", []),
                "cross_session_memory": all_memories.get("cross_session_memory", []),
                "global_memory": all_memories.get("global_memory", {}),
                "semantic_memory": all_memories.get("semantic_memory", []),
                "project_context": {
                    **(project_context or {}),
                    "notes": cross_session_notes,
                },
                "at_reference_context": at_reference_context,  # ğŸ”§ æ–°å¢ï¼š@å¼•ç”¨ä¸Šä¸‹æ–‡
                "memory_stats": {
                    "has_session_memory": bool(all_memories.get("session_memory")),
                    "has_global_memory": bool(all_memories.get("global_memory", {}).get("user_profile")),
                    "vector_store_path": str(_get_storage_path()),
                    "embedding_model": settings.embedding_model
                }
            }

            return context

        except Exception as e:
            logger.error(f"è·å–ç»¼åˆä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return {
                "query": query,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯

        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        try:
            # è·å–å…¨å±€è®°å¿†ç»Ÿè®¡
            global_memory = get_global_user_memory()
            global_stats = global_memory.get_summary()

            # è·å–æ‰€æœ‰ä¼šè¯è®°å¿†ç»Ÿè®¡
            all_memories = self.memory_manager.list_memories()

            return {
                "timestamp": datetime.now().isoformat(),
                "global_memory": {
                    "exists": bool(global_stats.get("has_user_profile")),
                    "user_name": global_stats.get("user_name"),
                    "user_company": global_stats.get("user_company"),
                },
                "session_memories": {
                    "total_count": len(all_memories),
                    "recent_memories": all_memories[:5]
                },
                "vector_store": {
                    "path": str(_get_storage_path()),
                    "embedding_model": settings.embedding_model
                }
            }

        except Exception as e:
            logger.error(f"è·å–è®°å¿†ç»Ÿè®¡å¤±è´¥: {e}")
            return {
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def update_global_user_info(self, session_id: str, user_info: Dict[str, Any]):
        """æ›´æ–°å…¨å±€ç”¨æˆ·ä¿¡æ¯

        Args:
            session_id: ä¼šè¯ID
            user_info: ç”¨æˆ·ä¿¡æ¯å­—å…¸
        """
        try:
            global_memory = get_global_user_memory()
            profile = global_memory.get_user_profile()

            # æ›´æ–°å­—æ®µ
            for key, value in user_info.items():
                if hasattr(profile, key):
                    setattr(profile, key, value)

            # ä¿å­˜
            global_memory.update_user_profile(profile)

            logger.info(f"å·²æ›´æ–°å…¨å±€ç”¨æˆ·ä¿¡æ¯: {user_info}")

        except Exception as e:
            logger.error(f"æ›´æ–°å…¨å±€ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {e}")

    async def get_session_context(self, session_id: str) -> Dict[str, Any]:
        """è·å–ä¼šè¯ä¸Šä¸‹æ–‡

        Args:
            session_id: ä¼šè¯ID

        Returns:
            åŒ…å«ä¼šè¯ä¸Šä¸‹æ–‡çš„å­—å…¸
        """
        try:
            # è·å–ä¼šè¯è®°å¿†
            session_memory = get_task_memory(session_id)
            project_context = session_memory.get_project_context()
            user_preferences = session_memory.get_user_preferences()
            conversation_history = session_memory.get_conversation_history(limit=10)

            # è·å–ç›¸å…³è®°å¿†
            relevant_memories = await self.retrieve_relevant_memory(
                query="",
                session_id=session_id,
                memory_types=["session_memory", "cross_session_memory"],
                limit=5
            )

            return {
                "session_id": session_id,
                "user_info": {
                    "name": None,
                    "company": None,
                },
                "project_context": project_context.model_dump(),
                "user_preferences": user_preferences.model_dump(),
                "conversation_history": [msg.model_dump() for msg in conversation_history],
                "relevant_memories": relevant_memories,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"è·å–ä¼šè¯ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return {
                "session_id": session_id,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }


# å‘åå…¼å®¹æ€§åˆ«å
